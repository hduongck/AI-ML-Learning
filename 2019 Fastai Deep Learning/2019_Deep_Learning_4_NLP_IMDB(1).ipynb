{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019_Deep_Learning_4_NLP_IMDB(1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/2019%20Fastai%20Deep%20Learning/2019_Deep_Learning_4_NLP_IMDB(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Al__snRp3oMU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Video](https://youtu.be/C9UdVPE3ynA) / [Lesson Forum](https://forums.fast.ai/t/lesson-4-official-resources-and-updates/30317)\n",
        "\n",
        "Welcome to Lesson 4! We are going to finish our journey through these key applications. We've already looked at a range of vision applications. We've looked a classification, localization, image regression. We briefly touched on NLP. We're going to do a deeper dive into NLP transfer learning today. We're going to then look at tabular data and collaborative filtering which are both super useful applications.\n",
        "\n",
        "Then we're going to take a complete u-turn. We're going to take that collaborative filtering example and dive deeply into it to understand exactly what's happening mathematically﹣exactly what's happening in the computer. And we're going to use that to gradually go back in reverse order through the applications again in order to understand exactly what's going on behind the scenes of all of those applications.\n",
        "\n",
        "**Correction on CamVid result**\n",
        "\n",
        "Before we do, somebody on the forum is kind enough to point out that when we compared ourselves to what we think might be the state of the art or was recently the state of the art for CamVid, there wasn't a fair comparison because the paper actually used a small subset of the classes, and we used all of the classes. So Jason in our study group was kind enough to rerun the experiments with the correct subset of classes from the paper, and our accuracy went up to 94% compared to 91.5% of the paper. So I think that's a really cool result. and a great example of how pretty much just using the defaults nowadays can get you far beyond what was the best of a year or two ago. It was certainly the best last year when we were doing this course because we started it quite intensely. So that's really exciting.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HQ61JCkyFzqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Natural Language Processing (NLP) [2:00](https://youtu.be/C9UdVPE3ynA?t=120)\n",
        "\n",
        "What I wanted to start with is going back over NLP a little bit to understand really what was going on there.\n",
        "\n",
        "**A quick review**\n",
        "\n",
        "So first of all, a quick review. Remember NLP is natural language processing. It's about taking text and doing something with it. Text classification is particularly useful﹣practically useful applications. It's what we're going to start off focusing on. Because classifying a text or classifying a document can be used for anything from:\n",
        "\n",
        "- Spam prevention\n",
        "- Identifying fake news\n",
        "- Finding a diagnosis from medical reports\n",
        "- Finding mentions of your product in Twitter\n",
        "\n",
        "So it's pretty interesting. And actually there was a great example during the week from one of our students @howkhang who is a lawyer and he mentioned on the forum that he had a really great results from classifying legal texts using this NLP approach. And I thought this was a great example. This is the post that they presented at an academic conference this week describing the approach:\n",
        "\n",
        "This series of three steps that you see here (and I'm sure you recognize this classification matrix) is what we're going to start by digging into.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson4/1.png?raw=true)\n",
        "\n",
        "We're going to start out with a movie review like this one and decide whether it's positive or negative sentiment about the movie. That is the problem. We have, in the training set, 25,000 movie reviews and for each one we have like one bit of information: they liked it, or they didn't like it. That's what we're going to look into a lot more detail today and in the current lessons. Our neural networks (remember, they're just a bunch of matrix multiplies and simple nonlinearities﹣particularly replacing negatives with zeros), those weight matrices start out random. So if you start out with with some random parameters and try to train those parameters to learn how to recognize positive vs. negative movie reviews, you literally have 25,000 ones and zeros to actually tell you I like this one I don't like that one. That's clearly not enough information to learn, basically, how to speak English﹣how to speak English well enough to recognize they liked this or they didn't like this. Sometimes that can be pretty nuanced. Particularly with movie reviews because these are like online movie reviews on IMDB, people can often use sarcasm. It could be really quite tricky.\n",
        "\n",
        "Until very recently, in fact, this year, neural nets didn't do a good job at all of this kind of classification problem. And that was why﹣there's not enough information available. So the trick, hopefully you can all guess, is to use transfer learning. It's always the trick.\n",
        "\n",
        "Last year in this course I tried something crazy which was I thought what if I try transform learning to demonstrate that it can work for NLP as well. I tried it out and it worked extraordinarily well. So here we are, a year later, and transfer learning in NLP is absolutely the hit thing. And I'm going to describe to you what happens.\n"
      ]
    },
    {
      "metadata": {
        "id": "UvzHVi6bKCbr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Transfer learning in NLP [6:04](https://youtu.be/C9UdVPE3ynA?t=364)\n",
        "\n",
        "The key thing is we're going to start with the same kind of thing that we used for computer vision﹣a pre-trained model that's been trained to do something different to what we're doing with it. For ImageNet, that was originally built as a model to predict which of a thousand categories each photo falls into. And people then fine-tune that for all kinds of different things as you've seen. So we're going to start with a pre-trained model that's going to do something else. Not movie review classification. We're going to start with a pre-trained model which is called a language model. **(my note)---> Image classification can use pre-trained model to predict new dataset but with text classification, we have to go with a language model first then do classification.**\n",
        "\n",
        "A language model has a very specific meaning in NLP and it's this. A language model is a model that learns to predict the next word of a sentence. To predict the next word of a sentence, you actually have to know quite a lot about English (assuming you're doing it in English) and quite a lot of world knowledge. By world knowledge, I'll give you an example.\n",
        "\n",
        "Here's your language model and it has read:\n",
        "\n",
        "- \"I'd like to eat a hot ___\": Obviously, \"dog\", right?\n",
        "\n",
        "- \"It was a hot ___\": Probably \"day\"\n",
        "\n",
        "\n",
        "**Now previous approaches to NLP use something called n-grams largely which is basically saying how often do these pairs or triplets of words tend to appear next to each other. And n-grams are terrible at this kind of thing. As you can see, there's not enough information here to decide what the next word probably is. But with a neural net, you absolutely can**.\n",
        "\n",
        "So here's the nice thing. If you train a neural net to predict the next word of a sentence then you actually have a lot of information. Rather than having a single bit to every 2,000 word movie review: \"liked it\" or \"didn't like it\", every single word, you can try and predict the next word. So in a 2,000 word movie review, there are 1,999 opportunities to predict the next word. Better still, you don't just have to look at movie reviews. Because really the hard thing isn't so much as \"does this person like the movie or not?\" but \"how do you speak English?\". So you can learn \"how do you speak English?\" (roughly) from some much bigger set of documents. So what we did was we started with Wikipedia."
      ]
    },
    {
      "metadata": {
        "id": "Lrq4JeTLKSLZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Wikitext 103 [8:30](https://youtu.be/C9UdVPE3ynA?t=510)\n",
        "\n",
        "Stephen Merity and some of his colleagues built something called Wikitext 103 dataset which is simply a subset of most of the largest articles from Wikipedia with a little bit of pre-processing that's available for download. So you're basically grabbing Wikipedia and then I built a language model on all of Wikipedia. So I just built a neural net which would predict the next word in every significantly sized Wikipedia article. That's a lot of information. If I remember correctly, it's something like a billion tokens. So we've got a billion separate things to predict. Every time we make a mistake on one of those predictions, we get the loss, we get gradients from that, and we can update our weights, and they can better and better until we can get pretty good at predicting the next word of Wikipedia.\n",
        "\n",
        "Why is that useful? Because at that point, I've got a model that knows probably how to complete sentences like this, so it knows quite a lot about English and quite a lot about how the world works﹣what kinds of things tend to be hot in different situations, for instance. Ideally, it would learn things like \"in 1996 in a speech to the United Nations, United States president _____ said \"... Now that would be a really good language model, because it would actually have to know who is this United States president in that year. So getting really good at training language models is a great way to teach a neural-net a lot about what is our world, what's in our world, how do things work in our world. It's a really fascinating topic, and it's actually one that philosophers have been studying for hundreds of years now. There's actually a whole theory of philosophy which is about what can be learned from studying language alone. So it turns out, apparently, quite a lot.\n",
        "\n",
        "So here's the interesting thing. You can start by training a language model on all of Wikipedia, and then we can make that available to all of you. Just like a pre-trained ImageNet model for vision, we've now made available a pre-trained Wikitext model for NLP not because it's particularly useful of itself (predicting the next word of sentences is somewhat useful, but not normally what we want to do), but it's a model that understands a lot about language and a lot about what language describes. So then, we can take that and we can do transfer learning to create a new language model that's specifically good at predicting the next word of movie reviews."
      ]
    },
    {
      "metadata": {
        "id": "UCX3ur5gNRgr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fine-tuning Wikitext to create a new language model [11:10](https://youtu.be/C9UdVPE3ynA?t=670)\n",
        "\n",
        "If we can build a language model that's good at predicting the next word of movie reviews pre-trained with the Wikitext model, then that's going to understand a lot about \"my favorite actor is Tom ____. \" Or \"I thought the photography was fantastic but I wasn't really so happy about the _____ (director).\" It's going to learn a lot about specifically how movie reviews are written. It'll even learn things like what are the names of some popular movies.\n",
        "\n",
        "**That would then mean we can still use a huge corpus of lots of movie reviews even if we don't know whether they're positive or negative to learn a lot about how movie reviews are written. So for all of this pre-training and all of this language model fine-tuning, we don't need any labels at all. It is what the researcher Yann LeCun calls self supervised learning**. In other words, it's a classic supervised model﹣we have labels, but the labels are not things that somebody else have created. They're built into the dataset itself. So this is really really neat. Because at this point, we've now got something that's good at understanding movie reviews and we can fine-tune that with transfer learning to do the thing we want to do which in this case is to classify movie reviews to be positive or negative. So my hope was (when I tried this last year) that at that point, 25,000 ones and zeros would be enough feedback to fine-tune that model and it turned out it absolutely was.\n",
        "\n",
        "**Question**: Does the language model approach works for text in forums that are informal English, misspelled words or slangs or shortforms like s6 instead of Samsung S 6? [12:47](https://youtu.be/C9UdVPE3ynA?t=767)\n",
        "\n",
        "- Yes, absolutely it does. Particularly if you start with your wikitext model and then fine-tune it with your \"target\" corpus. Corpus is just a bunch of documents (emails, tweets, medical reports, or whatever). You could fine-tune it so it can learn a bit about the specifics of the slang , abbreviations, or whatever that didn't appear in the full corpus. So interestingly, this is one of the big things that people were surprised about when we did this research last year. People thought that learning from something like Wikipedia wouldn't be that helpful because it's not that representative of how people tend to write. But it turns out it's extremely helpful because there's a much a difference between Wikipedia and random words than there is between like Wikipedia and reddit. So it kind of gets you 99% of the way there.\n",
        "\n",
        "So language models themselves can be quite powerful. For example there was a [blog post](https://blog.swiftkey.com/swiftkey-debuts-worlds-first-smartphone-keyboard-powered-by-neural-networks/) from SwiftKey (the folks that do the mobile-phone predictive text keyboard) and they describe how they kind of rewrote their underlying model to use neural nets. This was a year or two ago. Now most phone keyboards seem to do this. You'll be typing away on your mobile phone, and in the prediction there will be something telling you what word you might want next. So that's a language model in your phone.\n",
        "\n",
        "Another example was the researcher Andrej Karpathy who now runs all this stuff at Tesla, back when he was a PhD student, he created [a language model of text in LaTeX documents] and created these automatic generation of LaTeX documents that then became these automatically generated papers. That's pretty cute.\n",
        "\n",
        "We're not really that interested in the output of the language model ourselves. We're just interested in it because it's helpful with this process."
      ]
    },
    {
      "metadata": {
        "id": "O9q-R5G0OktT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Review of the basic process [15:14](https://youtu.be/C9UdVPE3ynA?t=914)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BWkFk3VXOtYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag8ayUQSO0TT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_vI5QJ-lPBYx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preparing the data**\n",
        "\n",
        "First let's download the dataset we are going to study. The dataset has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).\n",
        "\n",
        "We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset."
      ]
    },
    {
      "metadata": {
        "id": "VEl2JmxwO-Q-",
        "colab_type": "code",
        "outputId": "838b3a6a-5b55-4502-9b74-79f854ba343d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "path.ls()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb_sample/texts.csv')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "reR5T8KDPJSW",
        "colab_type": "code",
        "outputId": "bdd294cd-0b82-4a31-9999-e5f36dfc3107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "CIHNtytLPW6W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So there's an example of a movie review:"
      ]
    },
    {
      "metadata": {
        "id": "-cDTunJ8PSwl",
        "colab_type": "code",
        "outputId": "c4d04c12-3da2-4b9e-c0b5-3db5f91dd388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "df['text'][1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it\\'s also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6xAliwW_Peq_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code.\n",
        "\n",
        "So you can just go TextDataBunch.from_csv to grab a language model specific data bunch:"
      ]
    },
    {
      "metadata": {
        "id": "r7M7Tno1PdVR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = TextDataBunch.from_csv(path,'texts.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rilgrV19Pnj6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And then you can create a learner from that in the usual way and fit it. \n",
        "\n",
        "By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A TextDataBunch does all of that behind the scenes for you.\n",
        "\n",
        "Before we delve into the explanations, let's take the time to save the things that were calculated.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8XgmAoJ-PZyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHmOGg9jRCtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can save the data bunch which means that the pre-processing that is done, you don't have to do it again. You can just load it."
      ]
    },
    {
      "metadata": {
        "id": "cAmT2UZNQurx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = load_data(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6uEmFv-wR0jk",
        "colab_type": "code",
        "outputId": "1bbe655d-ebbd-4329-ea55-16137390b8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "??load_data\n",
        "\n",
        "def load_data(path:PathOrStr, file:PathLikeOrBinaryStream='data_save.pkl', bs:int=64, val_bs:int=None, num_workers:int=defaults.cpus,\n",
        "              dl_tfms:Optional[Collection[Callable]]=None, device:torch.device=None, collate_fn:Callable=data_collate,\n",
        "              no_check:bool=False, **kwargs)->DataBunch:\n",
        "              \n",
        "    \"Load a saved `DataBunch` from `path/file`. `file` can be file-like (file or buffer)\"\n",
        "    \n",
        "    source = Path(path)/file if is_pathlike(file) else file\n",
        "    ll = torch.load(source, map_location='cpu') if defaults.device == torch.device('cpu') else torch.load(source)\n",
        "    return ll.databunch(path=path, bs=bs, val_bs=val_bs, num_workers=num_workers, dl_tfms=dl_tfms, device=device,\n",
        "                        collate_fn=collate_fn, no_check=no_check, **kwargs)\n",
        "                \n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n??load_data\\n\\ndef load_data(path:PathOrStr, file:PathLikeOrBinaryStream=\\'data_save.pkl\\', bs:int=64, val_bs:int=None, num_workers:int=defaults.cpus,\\n              dl_tfms:Optional[Collection[Callable]]=None, device:torch.device=None, collate_fn:Callable=data_collate,\\n              no_check:bool=False, **kwargs)->DataBunch:\\n              \\n    \"Load a saved `DataBunch` from `path/file`. `file` can be file-like (file or buffer)\"\\n    \\n    source = Path(path)/file if is_pathlike(file) else file\\n    ll = torch.load(source, map_location=\\'cpu\\') if defaults.device == torch.device(\\'cpu\\') else torch.load(source)\\n    return ll.databunch(path=path, bs=bs, val_bs=val_bs, num_workers=num_workers, dl_tfms=dl_tfms, device=device,\\n                        collate_fn=collate_fn, no_check=no_check, **kwargs)\\n                \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "REZF0OCwROJT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What happens behind the scenes if we now load it as a classification data bunch (that's going to allow us to see the labels as well)?\n",
        "\n",
        "##Tokenization\n",
        "\n",
        "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
        "\n",
        "- we need to take care of punctuation\n",
        "- some words are contractions of two different words, like isn't or don't\n",
        "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
        "\n",
        "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
      ]
    },
    {
      "metadata": {
        "id": "NGq4eBrIRHbD",
        "colab_type": "code",
        "outputId": "a5d724a8-329a-4c95-94f2-cbd03b850433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "data = TextClasDataBunch.from_csv(path,'texts.csv')\n",
        "data.show_batch()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n \\n  xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj sydney , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj this film sat on my xxmaj tivo for weeks before i watched it . i dreaded a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj yorkers . \\n \\n  xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" xxmaj la xxmaj ronde</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first xxup 3d game , or even the first xxunk - up . xxmaj it 's also one of the first stealth games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - rounded gaming experience in general . xxmaj with graphics</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "O2Dmihm5SVrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols:\n",
        "\n",
        "- the \"'s\" are grouped together in one token\n",
        "- the contractions are separated like this: \"did\", \"n't\"\n",
        "- content has been cleaned for any HTML symbol and lower cased\n",
        "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
      ]
    },
    {
      "metadata": {
        "id": "L4s7mt64SmWw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Numericalization\n",
        "\n",
        "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token UNK.\n",
        "\n",
        "The correspondance from ids to tokens is stored in the vocab attribute of our datasets, in a dictionary call"
      ]
    },
    {
      "metadata": {
        "id": "ZU3yw9yzRs1u",
        "colab_type": "code",
        "outputId": "eb4d9bd9-1e18-42ee-ec51-ac9f541d4c2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "cell_type": "code",
      "source": [
        "data.vocab.itos[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'xxpad',\n",
              " 'xxbos',\n",
              " 'xxeos',\n",
              " 'xxfld',\n",
              " 'xxmaj',\n",
              " 'xxup',\n",
              " 'xxrep',\n",
              " 'xxwrep',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "EkrvjcCkS74H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And that big list of unique possible tokens is called the vocabulary which we just call it a \"vocab\". So what we then do is we replace the tokens with the ID of where is that token in the vocab:"
      ]
    },
    {
      "metadata": {
        "id": "VY1uEvGwS4DZ",
        "colab_type": "code",
        "outputId": "a4e14d32-4baa-416c-df99-d6cc15806980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "data.train_ds[0][0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos ... okay , maybe not all of it . xxmaj xxunk by the false promise of bikini - clad women on the movie 's cover ... but the xxup horror ... xxup the xxup horror ... ... whatever you do , do xxup not watch this movie . xxmaj xxunk out your eyes , repeatedly xxunk your skull in ... do what it takes . xxmaj never again -- never forget ! \n",
              " \n",
              " "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "gZF-13Y9TUPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But the underlying data is all numbers"
      ]
    },
    {
      "metadata": {
        "id": "NxXaSWfXTGNp",
        "colab_type": "code",
        "outputId": "651d2bd2-02b6-4b86-e200-9a20057f5503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "data.train_ds[0][0].data[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 101, 946,  11, 284,  39,  46,  14,  17,  10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "v26-9-THTw1_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's numericalization. Here's the thing though. As you'll learn, every word in our vocab is going to require a separate row in a weight matrix in our neural net. So to avoid that weight matrix getting too huge, that why we restrict the vocab to no more than (by default) 60,000 words. And if a word doesn't appear more than two times, we don't put it in the vocab either. So we keep the vocab to a reasonable size in that way. When you see these xxunk, that's an unknown token. It just means this was something that was not a common enough word to appear in our vocab.\n",
        "\n",
        "We also have a couple of other special tokens like (see fastai.text.transform.py for up-to-date info):\n",
        "\n",
        "xxfld: This is a special thing where if you've got like title, summary, abstract, body, (i. e. separate parts of a document), each one will get a separate field and so they will get numbered (e.g. xxfld 2).\n",
        "xxup: If there's something in all caps, it gets lower cased and a token called xxup will get added to it."
      ]
    },
    {
      "metadata": {
        "id": "MxGMJIUZT8iq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##With the data block API [18:31](https://youtu.be/C9UdVPE3ynA?t=1111)\n",
        "\n",
        "Personally, I more often use the data block API because there's less to remember about exactly what data bunch to use, and what parameters and so forth, and it can be a bit more flexible.\n",
        "\n",
        "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the variaous arguments to pass will appear in the step they're revelant, so it'll be more readable."
      ]
    },
    {
      "metadata": {
        "id": "3uYdjEDZTZ58",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = (TextList.from_csv(path,'texts.csv',cols='text')\n",
        "       .split_from_df(col=2)\n",
        "       .label_from_df(cols=0)\n",
        "       .databunch())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKYKwvtoAUyD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So another approach to doing this is to just decide:\n",
        "\n",
        "- What kind of list you're creating (i.e. what's your independent variable)? So in this case, my independent variable is text.\n",
        "- What is it coming from? A CSV.\n",
        "- How do you want to split it into validation versus training? So in this case, column number two was the is_valid flag.\n",
        "- How do you want to label it? With positive or negative sentiment, for example. So column zero had that.\n",
        "- Then turn that into a data bunch."
      ]
    },
    {
      "metadata": {
        "id": "bJ6IXb0dAr-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Language model\n",
        "\n",
        "Now let's grab the full dataset for what follows.\n",
        "\n",
        "First, note that language models can use a lot of GPU, so you may need to decrease batchsize here."
      ]
    },
    {
      "metadata": {
        "id": "Fk2CnZujA3uo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bs = 48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DU2K7BrIBEd3",
        "colab_type": "code",
        "outputId": "d3d6da1e-7e67-4744-955a-94ff74180c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb/tmp_lm'),\n",
              " PosixPath('/root/.fastai/data/imdb/README'),\n",
              " PosixPath('/root/.fastai/data/imdb/tmp_clas'),\n",
              " PosixPath('/root/.fastai/data/imdb/unsup'),\n",
              " PosixPath('/root/.fastai/data/imdb/train'),\n",
              " PosixPath('/root/.fastai/data/imdb/imdb.vocab'),\n",
              " PosixPath('/root/.fastai/data/imdb/test')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "vKUxKNknBI-3",
        "colab_type": "code",
        "outputId": "3a6e3449-429e-4304-ead0-a5fe5ecec0bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "(path/'train').ls()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb/train/labeledBow.feat'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/unsupBow.feat'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/neg'),\n",
              " PosixPath('/root/.fastai/data/imdb/train/pos')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "_r8xs9-mB7oL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's grab the whole data set which has:\n",
        "\n",
        "- 25,000 reviews in training\n",
        "- 25,000 interviews in validation\n",
        "- 50,000 unsupervised movie reviews (50,000 movie reviews that haven't been scored at all)\n",
        "\n",
        "We're going to start with the language model. Now the good news is, we don't have to train the Wikitext 103 language model. Not that it's difficult﹣you can just download the wikitext 103 corpus, and run the same code. But it takes two or three days on a decent GPU, so not much point in you doing it. You may as well start with ours. Even if you've got a big corpus of like medical documents or legal documents, you should still start with Wikitext 103. There's just no reason to start with random weights. It's always good to use transfer learning if you can.\n",
        "\n",
        "So we're gonna start fine-tuning our IMDB language model.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an unsup folder on top of  train and test that contains the unlabelled data.\n",
        "\n",
        "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called wikitext-103). That model has been trained to guess what the next word, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
        "\n",
        "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on.\n",
        "\n",
        "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes).\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "s3zYOW_RBRAY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = (TextList.from_folder(path)\n",
        "          #Input: all the text files in path\n",
        "          .filter_by_folder(include=['train','test','unsub'])\n",
        "          # We may have other temp folders that contain text files so we only \n",
        "          # keep what is in train and test \n",
        "          .split_by_rand_pct(0.1)\n",
        "           # we randomly split and keep 10% (10,000 reviews) for validation\n",
        "          .label_for_lm()\n",
        "           # we want to do a language model so we label accordingly\n",
        "          .databunch(bs=bs))\n",
        "data_lm.save('data_lm.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X1eP9tnJDjiz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have to use a special kind of TextDataBunch for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
        "\n",
        "We can say:\n",
        "\n",
        "- It's a list of text files﹣the full IMDB actually is not in a CSV. Each document is a separate text file.\n",
        "- Say where it is﹣in this case we have to make sure we just to include the train and test folders.\n",
        "- We randomly split it by 0.1.\n",
        "\n",
        "    - Now this is interesting﹣10%. **Why are we randomly splitting it by 10% rather than using the predefined train and test they gave us? This is one of the cool things about transfer learning**. Even though our validation set has to be held aside, it's actually only the labels that we have to keep aside. So we're not allowed to use the labels in the test set. If you think about in a Kaggle competition, you certainly can't use the labels because they don't even give them to you. But you can certainly use the independent variables. So in this case, you could absolutely use the text that is in the test set to train your language model. **This is a good trick﹣when you do the language model, concatenate the training and test set together, and then just split out a smaller validation set so you've got more data to train your language model. So that's a little trick**.\n",
        "\n",
        "    - So if you're doing NLP stuff on Kaggle, for example, or you've just got a smaller subset of labeled data, make sure that you use all of the text you have to train in your language model, because there's no reason not to.\n",
        "\n",
        "- How are we going to label it? Remember, a language model kind of has its own labels. The text itself is labels so label for a language model (label_for_lm) does that for us.\n",
        "- And create a data bunch and save it. That takes a few minutes to tokenize and numericalize.\n",
        "\n",
        "Since it takes some few minutes, we save it. Later on you can just load it. No need to run it again."
      ]
    },
    {
      "metadata": {
        "id": "KPKMprsrDW2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_lm = load_data(path,'data_lm.pkl',bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGDHEIpMFPJ4",
        "colab_type": "code",
        "outputId": "fb106f3b-f906-41ac-ed5b-0c09818b7108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>is due to the fact that xxmaj gunnar xxmaj hansen appears ( ever so briefly ) as one of the film 's reprehensible characters . xxmaj how they ever lured xxmaj mr. xxmaj hansen into this piece of ... work , i 'll never know . xxmaj the story idea is interesting but poorly executed . xxmaj the direction is pedestrian and the acting is mediocre . xxmaj the only</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>n't try to create a tiny bit of tension . xxmaj the director ( xxmaj stephen xxmaj carpenter -- i guess it 's much easier to find money with a name like that ) also made the xxmaj kindred ( 1986 ) wich was rather enjoyable and recently he did xxmaj soul xxmaj survivors . xxmaj complete crap as well , but at least that one had xxmaj eliza xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>a daft little horror film that harks back to the style of eighties woodland flicks , you might find some enjoyment here . xxbos xxmaj in my opinion this movie advances no new thought . seems to me like taking a spear to a spear without looking to the side ! the director seems to have an agenda ! xxmaj duh ! i find that his rational is lacking there</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>it strayed into here is xxunk guess . \\n \\n  xxmaj there is other great stock footage of xxmaj ottawa in the old days when the capital of xxmaj canada was a wide spot in the road and especially wonderful footage of xxmaj new xxmaj york xxmaj city 's xxmaj times xxmaj square during one of the xxmaj civil xxmaj defense xxmaj drills in the early 50s . \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>then at least it was n't a total waste . xxmaj you 'd never know by watching it though . xxmaj script ? xxmaj are you kidding . xxmaj acting ? i think even the trees were faking . xxmaj cinematography ? xxmaj well , there must 've been a camera there . xxmaj period . i do n't think there was any actual planning involved in the making of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xq62Ni2QFgZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training the language model [22:29](https://youtu.be/C9UdVPE3ynA?t=1349)\n",
        "\n",
        "At this point things are going to look very familiar. We create a learner:"
      ]
    },
    {
      "metadata": {
        "id": "Dg9UgwXIFRcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = language_model_learner(data_lm,AWD_LSTM,drop_mult=0.3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lEGLP6XCGEzn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But instead of creating a CNN learner, we're going to create a language model learner. So behind the scenes, this is actually not going to create a CNN (a convolutional neural network), it's going to create an RNN (a recurrent neural network). We're going to be learning exactly how they're built over the coming lessons, but in short they're the same basic structure. The input goes into a weight matrix (i.e. a matrix multiply), that then you replace the negatives with zeros, and it goes into another matrix multiply, and so forth a bunch of times. So it's the same basic structure.\n",
        "\n",
        "As usual, when we create a learner, you have to pass in two things:\n",
        "\n",
        "- The data: so here's our language model data\n",
        "- What pre-trained model we want to use: here, the pre-trained model is the Wikitext 103 model (arch= AWD_LSTM) that will be downloaded for you from fastai if you haven't used it before just like ImageNet pre-trained models are downloaded for you.\n",
        "This here (drop_mult=0.3) sets the amount of dropout. We haven't talked about that yet. We've talked briefly about this idea that there is something called regularization and you can reduce the regularization to avoid underfitting. So for now, just know that by using a number lower than one is because when I first tried to run this, I was under fitting. So if you reduced that number, then it will avoid under fitting."
      ]
    },
    {
      "metadata": {
        "id": "tuhLWeoDFyPT",
        "colab_type": "code",
        "outputId": "e2a3d4a6-a27c-47fd-e01f-b5473a9f5850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EztPy4I5HdEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6c92b250-b75d-4b29-b234-0c71f5d8ec13"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "??language_model_learner\n",
        "\n",
        "def language_model_learner(data:DataBunch, arch, config:dict=None, drop_mult:float=1., pretrained:bool=True,\n",
        "                           pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> 'LanguageLearner':\n",
        "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
        "    model = get_language_model(arch, len(data.vocab.itos), config=config, drop_mult=drop_mult)\n",
        "    meta = _model_meta[arch]\n",
        "    learn = LanguageLearner(data, model, split_func=meta['split_lm'], **learn_kwargs)\n",
        "    if pretrained:\n",
        "        if 'url' not in meta: \n",
        "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
        "            return learn\n",
        "        model_path = untar_data(meta['url'], data=False)\n",
        "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
        "        learn.load_pretrained(*fnames)\n",
        "        learn.freeze()\n",
        "    if pretrained_fnames is not None:\n",
        "        fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
        "        learn.load_pretrained(*fnames)\n",
        "        learn.freeze()\n",
        "    return learn\n",
        "    \n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n??language_model_learner\\n\\ndef language_model_learner(data:DataBunch, arch, config:dict=None, drop_mult:float=1., pretrained:bool=True,\\n                           pretrained_fnames:OptStrTuple=None, **learn_kwargs) -> \\'LanguageLearner\\':\\n    \"Create a `Learner` with a language model from `data` and `arch`.\"\\n    model = get_language_model(arch, len(data.vocab.itos), config=config, drop_mult=drop_mult)\\n    meta = _model_meta[arch]\\n    learn = LanguageLearner(data, model, split_func=meta[\\'split_lm\\'], **learn_kwargs)\\n    if pretrained:\\n        if \\'url\\' not in meta: \\n            warn(\"There are no pretrained weights for that architecture yet!\")\\n            return learn\\n        model_path = untar_data(meta[\\'url\\'], data=False)\\n        fnames = [list(model_path.glob(f\\'*.{ext}\\'))[0] for ext in [\\'pth\\', \\'pkl\\']]\\n        learn.load_pretrained(*fnames)\\n        learn.freeze()\\n    if pretrained_fnames is not None:\\n        fnames = [learn.path/learn.model_dir/f\\'{fn}.{ext}\\' for fn,ext in zip(pretrained_fnames, [\\'pth\\', \\'pkl\\'])]\\n        learn.load_pretrained(*fnames)\\n        learn.freeze()\\n    return learn\\n    \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "xeLF0aZ6GDpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "65e0dc96-6b37-49e1-dbb2-073f4b5c2a0f"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "??AWD_LSTM\n",
        "\n",
        "class AWD_LSTM(nn.Module):\n",
        "    \"AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182.\"\n",
        "\n",
        "    initrange=0.1\n",
        "\n",
        "    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int=1, hidden_p:float=0.2,\n",
        "                 input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, qrnn:bool=False, bidir:bool=False):\n",
        "        super().__init__()\n",
        "        self.bs,self.qrnn,self.emb_sz,self.n_hid,self.n_layers = 1,qrnn,emb_sz,n_hid,n_layers\n",
        "        self.n_dir = 2 if bidir else 1\n",
        "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
        "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
        "        if self.qrnn:\n",
        "            #Using QRNN requires an installation of cuda\n",
        "            from .qrnn import QRNN\n",
        "            self.rnns = [QRNN(emb_sz if l == 0 else n_hid, n_hid if l != n_layers - 1 else emb_sz, 1,\n",
        "                              save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) \n",
        "                         for l in range(n_layers)]\n",
        "            for rnn in self.rnns: \n",
        "                rnn.layers[0].linear = WeightDropout(rnn.layers[0].linear, weight_p, layer_names=['weight'])\n",
        "        else:\n",
        "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, 1,\n",
        "                                 batch_first=True, bidirectional=bidir) for l in range(n_layers)]\n",
        "            self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
        "        self.rnns = nn.ModuleList(self.rnns)\n",
        "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
        "        self.input_dp = RNNDropout(input_p)\n",
        "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
        "\n",
        "    def forward(self, input:Tensor, from_embeddings:bool=False)->Tuple[Tensor,Tensor]:\n",
        "        if from_embeddings: bs,sl,es = input.size()\n",
        "        else: bs,sl = input.size()\n",
        "        if bs!=self.bs:\n",
        "            self.bs=bs\n",
        "            self.reset()\n",
        "        raw_output = self.input_dp(input if from_embeddings else self.encoder_dp(input))\n",
        "        new_hidden,raw_outputs,outputs = [],[],[]\n",
        "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
        "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
        "            new_hidden.append(new_h)\n",
        "            raw_outputs.append(raw_output)\n",
        "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
        "            outputs.append(raw_output)\n",
        "        self.hidden = to_detach(new_hidden, cpu=False)\n",
        "        return raw_outputs, outputs\n",
        "\n",
        "    def _one_hidden(self, l:int)->Tensor:\n",
        "        \"Return one hidden state.\"\n",
        "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\n",
        "        return one_param(self).new(1, self.bs, nh).zero_()\n",
        "\n",
        "    def select_hidden(self, idxs):\n",
        "        if self.qrnn: self.hidden = [h[:,idxs,:] for h in self.hidden]\n",
        "        else: self.hidden = [(h[0][:,idxs,:],h[1][:,idxs,:]) for h in self.hidden]\n",
        "        self.bs = len(idxs)\n",
        "\n",
        "    def reset(self):\n",
        "        \"Reset the hidden states.\"\n",
        "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
        "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
        "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
        "        \n",
        "'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n??AWD_LSTM\\n\\nclass AWD_LSTM(nn.Module):\\n    \"AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182.\"\\n\\n    initrange=0.1\\n\\n    def __init__(self, vocab_sz:int, emb_sz:int, n_hid:int, n_layers:int, pad_token:int=1, hidden_p:float=0.2,\\n                 input_p:float=0.6, embed_p:float=0.1, weight_p:float=0.5, qrnn:bool=False, bidir:bool=False):\\n        super().__init__()\\n        self.bs,self.qrnn,self.emb_sz,self.n_hid,self.n_layers = 1,qrnn,emb_sz,n_hid,n_layers\\n        self.n_dir = 2 if bidir else 1\\n        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\\n        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\\n        if self.qrnn:\\n            #Using QRNN requires an installation of cuda\\n            from .qrnn import QRNN\\n            self.rnns = [QRNN(emb_sz if l == 0 else n_hid, n_hid if l != n_layers - 1 else emb_sz, 1,\\n                              save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) \\n                         for l in range(n_layers)]\\n            for rnn in self.rnns: \\n                rnn.layers[0].linear = WeightDropout(rnn.layers[0].linear, weight_p, layer_names=[\\'weight\\'])\\n        else:\\n            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.n_dir, 1,\\n                                 batch_first=True, bidirectional=bidir) for l in range(n_layers)]\\n            self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\\n        self.rnns = nn.ModuleList(self.rnns)\\n        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\\n        self.input_dp = RNNDropout(input_p)\\n        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\\n\\n    def forward(self, input:Tensor, from_embeddings:bool=False)->Tuple[Tensor,Tensor]:\\n        if from_embeddings: bs,sl,es = input.size()\\n        else: bs,sl = input.size()\\n        if bs!=self.bs:\\n            self.bs=bs\\n            self.reset()\\n        raw_output = self.input_dp(input if from_embeddings else self.encoder_dp(input))\\n        new_hidden,raw_outputs,outputs = [],[],[]\\n        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\\n            raw_output, new_h = rnn(raw_output, self.hidden[l])\\n            new_hidden.append(new_h)\\n            raw_outputs.append(raw_output)\\n            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\\n            outputs.append(raw_output)\\n        self.hidden = to_detach(new_hidden, cpu=False)\\n        return raw_outputs, outputs\\n\\n    def _one_hidden(self, l:int)->Tensor:\\n        \"Return one hidden state.\"\\n        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz) // self.n_dir\\n        return one_param(self).new(1, self.bs, nh).zero_()\\n\\n    def select_hidden(self, idxs):\\n        if self.qrnn: self.hidden = [h[:,idxs,:] for h in self.hidden]\\n        else: self.hidden = [(h[0][:,idxs,:],h[1][:,idxs,:]) for h in self.hidden]\\n        self.bs = len(idxs)\\n\\n    def reset(self):\\n        \"Reset the hidden states.\"\\n        [r.reset() for r in self.rnns if hasattr(r, \\'reset\\')]\\n        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\\n        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\\n        \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "KW2Yoav8GqZp",
        "colab_type": "code",
        "outputId": "46c35f06-0498-4cd8-830e-7db901e693b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot(skip_end=15)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GEWM-aeUIydq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we can fit one cycle."
      ]
    },
    {
      "metadata": {
        "id": "eW3tc0DZIdsM",
        "colab_type": "code",
        "outputId": "4719eddd-d4b5-44f2-bc8a-9e01ef351e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1,1e-2,moms=(0.8,0.7))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.185115</td>\n",
              "      <td>4.040494</td>\n",
              "      <td>0.293801</td>\n",
              "      <td>20:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LCmOQLBQI-TB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('fit_head')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPJcrlIxI5Zd",
        "colab_type": "code",
        "outputId": "685b6aa1-198c-4616-81f0-63876a7543bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4196
        }
      },
      "cell_type": "code",
      "source": [
        "learn.load('fit_head')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (25000 items)\n",
              "x: TextList\n",
              "xxbos xxup ok this movie had a terrible premise . xxmaj be serious according to the movie they had just been through an apocalyptic war yet they have money to buy huge robots and pit them against each other . xxmaj each country decides instead of investing into rebuilding their country they would rather fight with robots no one could afford . xxmaj here 's a better idea , lets rely on our most inept resource , jocks , to fight our battles . \n",
              " \n",
              "  xxmaj everyone says what about the director , what about him . xxmaj he makes a good movie , he makes a bad movie . xxmaj there is no reason to give this movie some credit just because of the director , maybe he was asleep ? i thoroughly enjoyed this movie , because it was so cheesy and ridiculous i had to laugh . i actually had a good time watching it , well except for the cowboy mentor who turns out to be an xxunk me no one would see this guy as an assassin , so it is a surprise , however lame ) xxmaj what kind of training exercise is a jungle jim anyway . i was sad to see xxmaj mst3k had not done this one . i am giving a two star rating however because nothing could be as bad as \" manos the hands of fate . \" \n",
              " \n",
              "  xxmaj the budget does not matter either , i have seen plenty of reasonable movies that had nothing for budgets like cube . xxmaj the storyline was not even plausible and i have seen better acting in school plays . xxmaj surly they could have afforded an eleven year old from any middle school play . \n",
              " \n",
              "  xxmaj anyway pick it up , it is a fun movie to watch .,xxbos i saw \" xxmaj rachel 's xxmaj attic , \" thinking that i would be in for an enjoyably visceral , ride . xxmaj however , it was not to be the case . xxmaj visceral , yes , but enjoyable ? xxmaj that would be a big , fat , no ! xxmaj in fact , the only reason that i gave it a \" 3 , \" is due to the fact that xxmaj gunnar xxmaj hansen appears ( ever so briefly ) as one of the film 's reprehensible characters . xxmaj how they ever lured xxmaj mr. xxmaj hansen into this piece of ... work , i 'll never know . xxmaj the story idea is interesting but poorly executed . xxmaj the direction is pedestrian and the acting is mediocre . xxmaj the only thing that is worse than that , are the special effects . xxup yikes ! ! ! i 've seen better effects in a grade school play . xxmaj give it up , xxmaj mr. w , it 's time for a career change ... i hear they 're hiring at xxmaj mel 's xxmaj diner ! xxmaj there are very few , well made , xxmaj xxunk movies coming out of xxmaj michigan ... and \" xxmaj rachel 's xxmaj attic \" is n't one of them .,xxbos i 'm actually watching this film as i write this . . . xxmaj if the following comments \" prove my lack of development as a true , artistic film maker \" , then so be it . . . \n",
              " \n",
              "  xxmaj but . . . i thought ( am still thinking as i 'm presently viewing ) that this film . . . to put it mildly , is very , very overrated . xxmaj again , very . \n",
              " \n",
              "  xxmaj it looks like a really , really bad student film done by a someone with beyond extremely limited resources . . . and who did n't pay that much attention to detail . \n",
              " \n",
              "  i do n't want to go on and on regarding all the different ways that i find this film lacking , but . . . well . . . i just do n't get it ( rememeber , i fully admit that maybe it 's xxup me that 's the idiot here - not the film maker - for not getting this \" piece of imaginative genius \" ) . . . i rented this on a whim because the reviews were very , very outstanding . . . \n",
              " \n",
              "  xxmaj sheesh . . .,xxbos xxmaj from the start this film was awful ! xxmaj why was it that bad ? ? xxmaj if it is n't the naked women , not only in need of a decent plastic surgeon but also the expertise of a dentist followed by a free hand out of xxmaj xxunk xxunk ! ! xxmaj then it 's the ' crazy ' old guy at the gas station , who is n't so much crazy , but more \" i 'm not sure how to act a great deal so i will stare straight ahead and look as stupid as i can while pretending to shout in robotic tones about something in the woods \" ! ! xxmaj then back to these naked nymphs in need of a cure for xxunk xxrep 4 . apparently , without touching you ... and this is according to the opening scene xxrep 4 . they can cause a nasty looking red rash on your neck , which i assumed to be a chunk of flesh missing but just looks as though it could do with some xxup xxunk to clear it right up . xxmaj then you have xxmaj sophie xxmaj holland who plays xxmaj ally , i have never seen such b xxrep 6 a d acting , she is more of a \" me me me , if i 'm not having fun no - one else is , and i do n't wanna do this so i wo n't , and i 'm the meanest cow on the planet , i 'm sarcastic , petty and if i do n't get things my way i will sulk ! \" , kind of person xxrep 4 . reminds me more of a 6 yr old girl 's attitude . i do n't think it 's even worth mentioning the dire camera angles that remind me of xxmaj blair xxmaj witch , or how low - budget the film actually was that when xxmaj judd was hacking at the ' locked ' door it was in fact open before he reached to unlock it from the other side ! ! xxmaj this film is completely laughable ! xxmaj if it were a spoof then it would have been successful ... only just though , but , as a horror film is was just plain wrong ! ! ! i ca n't even being to describe everything that went xxunk up in this movie i would run out of room ! xxmaj although it was funny to watch xxmaj andrew drip raspberry juice in his ear every time he opened his mouth while xxmaj tom xxmaj savini 's character was completely blind to the two hiding under the table directly in his line of vision ! ! xxmaj it was even funnier when these two thought they could escape on a god damn tractor , which as we all know is thee number one hated vehicle to get stuck behind since its so god damn slow ! xxmaj so is it any wonder they do n't get away with it ? ? xxmaj and how many people do you know that can slice open their wrist and then run around for hours as if nothing ever happened ! xxmaj no pain , no weakening from blood loss , nothing ! ? xxmaj but the silliest part is when all of a sudden ( and i mean that literally ) it 's one xxup year later and xxmaj molly is still wandering the woods after having escaped the nymphs , and then lo and behold , xxmaj shaun xxmaj hutson picks her up ... of course not without a line to promote his books ! ! ( altho admittedly he is one of my fav authors ) but suddenly , and with absolutely no hint of an xxunk as to how and why ... she 's evil herself and lures xxmaj hutson to his death , then we cut to the crazy dude from the beginning suddenly wandering round the woods with a petrol can , even after his ' dazzling ' performance on why no - one should ever venture there for whatever reason ... cue the nymphs stupidly xxunk each other around a bit for fun while xxmaj crazy pours petrol everywhere xxrep 4 . and here endeth the film xxrep 4 . finally ! xxmaj my conclusion xxrep 4 . if you had n't already guessed by now xxrep 4 . absolute rubbish ! xxmaj there was no proper thought went into it at all , whoever was aiming the camera needed firing ... and come to think of it so did 99 % of the cast ! xxmaj if the right director , actors , and budget got behind this it could have been decent . xxmaj but , once again , low - budget xxmaj english horror films but the rest of the genre , the country , and the xxmaj english film - making industry to shame ! ! ( xxmaj and i 'm xxmaj english so i 'm allowed to say that ) ! xxmaj in fact the only decent and exciting part of the movie is in the first 15 - 20 mins when we watch it turn from night to day over a field type area . xxmaj all i kept thinking throughout this was \" xxmaj jesus xxmaj christ in heaven why oh why did you allow someone to make this , its absolute cow 's testicles ! ! \" xxmaj but i ca n't turn a film off after i 've started watching it unfortunately . i had to watch xxmaj from xxmaj dusk xxmaj til xxmaj dawn afterwards just to remind myself that xxmaj tom xxmaj savini does have it in him to act well ! xxmaj if there was an option for 0 / 10 then believe me i woulda chose that , cuz this film is n't even worth the one point i did give it ! \n",
              " \n",
              "  xxmaj but this is just my opinion , watch it and decide for yourself .,xxbos i did n't think this was as absolutely horrible as some people apparently do . xxmaj it passes as one of those cheesy horror movies you might waste time with in the middle of the night when you ca n't sleep , although admittedly it 's no better quality than that . xxmaj it 's true that the acting is n't great - i thought xxmaj marianne mcandrew as xxmaj cathy xxmaj beck , for example , came across as completely passionless - but the main problem is that several aspects of the plot did n't really make sense to me . xxmaj the xxmaj becks are on a trip described by xxmaj john ( xxmaj stewart xxmaj moss ) as part work and partly the honeymoon they never had ( now that 's romantic ! ) xxmaj the work part has something to do with touring caves , which in itself sounds strange ( how does being part of a tour group through a cave relate to anyone 's work ? ) but it gets stranger when we find out that he 's a doctor doing research in the area of xxunk medicine ( huh ? xxmaj that connection completely lost me . ) xxmaj bitten by a bat while he 's in the cave , he begins to transform into what i guess was supposed to be a human - bat hybrid ( although when we finally see him in makeup he looks a lot more like an ape - man of some sort ) and a killing spree starts . xxmaj here 's another problem . xxmaj the first killing is a nurse in a hospital . xxmaj at first , everyone thinks her death was an accident . xxmaj the second murder is of a young girl , who is described as having her throat ripped out . xxmaj the sheriff ( xxmaj michael xxmaj pataki ) then tells us that her death was similar to the nurse 's ( meaning throat ripped out ? - xxmaj how could anyone think that was an accident ? ) xxmaj and what 's with the sheriff ? xxmaj he seems pretty no - nonsense until the scene in xxmaj cathy 's hotel room when he takes a swig of liquor and then almost rapes her , after which everything seems to go back to normal . xxmaj it 's saddled with an ending that left almost everything unresolved , and also with one of the most irritating theme songs i 've ever heard in a movie . xxmaj even for all that , there was something here that kept me watching . xxmaj sometimes pure cheesiness can get you through an hour and a half . xxmaj pretty bad , yeah - but not as awful as some people say .\n",
              "y: CategoryList\n",
              "neg,neg,neg,neg,neg\n",
              "Path: /root/.fastai/data/imdb;\n",
              "\n",
              "Valid: LabelList (25000 items)\n",
              "x: TextList\n",
              "xxbos * * xxmaj may xxmaj contain xxmaj spoilers * * \n",
              " \n",
              "  xxmaj the main character , a nobleman named xxmaj fallon , is stranded on an island with characters so looney and lethal he might have been better off drowning . xxmaj count xxmaj xxunk de xxmaj sade ( pronounced \" dee - xxunk \" ) talks to his own hallucinations and sees all intruders on the island as invading pirates . xxmaj he routinely beats mute servant xxmaj anne and tortures his unwilling guests in the dungeon . xxmaj xxunk laughs are provided by giant \" xxmaj xxunk \" slave xxmaj mantis who talks with a xxmaj deep xxmaj south accent and helps de xxmaj sade hunt down trespassers in the style of xxup the xxup most xxup dangerous xxup game . xxmaj de xxmaj sade 's crazed wife , ravaged by leprosy , provides some truly scary moments as she prowls the dungeon and embraces a helplessly chained prisoner . ( xxmaj this scene was viewed on late - night xxup tv by many kids who carried the memory into adulthood . ) xxmaj the one nearly - normal person in sight is xxmaj cassandra , who has self - deprecation down to a science . ( \" i used to be a nurse , now i 'm not much of anything . \" ) xxmaj she and xxmaj fallon plan their escape and ultimately encounter an enemy more fearsome than de xxmaj sade and xxmaj mantis combined . \n",
              " \n",
              " \t xxmaj this movie was shot in xxmaj san xxmaj antonio and directed by a man more competent at drawing horror comics than making horror movies . ( i 'll say this much for xxmaj mr. xxmaj boyette -- he does showcase his xxunk with contagion here , as he did in his comics . ) xxmaj it 's rather like an xxmaj andy xxmaj milligan melodrama minus the meat cleavers . xxmaj the period wardrobe , library music , abuse of the handicapped and all - around misanthropy makes one wonder if xxmaj andy was n't called in as a consultant . xxmaj however , xxmaj milligan made better costumes and wrote better dialogue . xxmaj technical gaffes are too numerous to list here but you know this flick is in trouble when you see the opening shipwreck , which looks like it was shot in a fish tank . xxmaj also , a film made in xxmaj texas should have had real spiders and snakes rather than rubber ones . xxmaj glorious xxmaj xxunk gives this melodrama the garish look it so richly deserves . xxmaj fallon 's initial encounter with the leprous xxmaj countess is truly horrifying , as is the movie 's parting shot . xxmaj if the rest had been half as harrowing , xxup the xxup dungeon xxup of xxup harrow would have been a terror classic . xxmaj instead it 's a funny piece of schlock that trash - fiends will love , for all the wrong reasons .,xxbos i can find no redeeming value to this movie . xxmaj it appears to be loosely based on the xxmaj lion xxmaj king school of thought . xxmaj father gets killed , son ca n't fill the shoes and tries to run away , etc , etc , etc . xxmaj the only difference ( other than being in a barnyard instead of a jungle ) is that xxmaj barnyard tries to \" liven things up \" with club - type music . xxmaj they go way over the top in trying to be cool . xxmaj the problem is that it really is n't cool . xxmaj it 's like \" that guy \" . xxmaj everybody knows at least one of \" those guys \" who are older and still hang out with the younger crowd in a futile attempt to cling onto their youth . xxmaj they try to be cool to fit in but they really are n't . xxmaj that 's this movie . \n",
              " \n",
              "  xxmaj but hey , if you have money to burn and you feel like paying someone to suck 90 minutes of you life away , by all means do n't let me stop you .,xxbos xxmaj produced by xxmaj nott xxmaj entertainment , this movie is \" nott \" very good at all . i sat through the first 15 minutes of the film before judging that the acting is bad , the casting is bad and camera work is bad . xxmaj as i hear that there is a download of this film floating around on the internet , it is \" nott \" even worth the bandwidth . \n",
              " \n",
              "  xxmaj up until the time i wrote this review , the average vote for this movie was an 8.5 , which prompted me to view it and there was an average high majority of 10 's for it , obviously voted on by liars and shills . xxmaj this movie is \" nott \" for everyone . xxmaj or parents , if you want to punish your kids with this awful film , have them sit through this one for xxmaj halloween .,xxbos xxmaj everyone knows that late night movies are n't xxmaj oscar contenders . xxmaj fine . i mean i 'll admit that i was a bit tipsy and bored and figured i 'd get to some skin - a - max . xxmaj it 's pretty bad when the info on the xxup tv guide channel makes fun of the movie in the description . xxmaj it even gave it half a star . xxmaj to be fair , i did sit throw the whole thing cause man it was s xxrep 10 o bad . i could n't stop laughing . i mean the words coming out of these people mouth and how they were trying to be serious . xxmaj most of the time i think the people on the screen were trying their hardest to not to laugh . xxmaj in fact i think in one scene they did laugh . xxmaj anyways the movie did n't make sense . xxmaj it was like that one xxmaj sopranos episode with the fat gay guy . xxmaj only the xxmaj sopranos is great show . xxmaj but it was terrible , i mean , no nudity , just sex scenes out of the 90 's . xxmaj you know the kind that use shadows and silhouettes instead of flesh . i gave it a two cause this flick makes for a good drinking game movie . i mean with all the cheese , it helps to get the wine out . xxmaj if its late at night , and all that is on xxup tv is this and that xxmaj tony xxmaj little guy and his exercise bike , then i suggest xxmaj tony xxmaj little .,xxbos xxmaj after watching the xxmaj steven xxmaj spielberg version of xxmaj war xxmaj of xxmaj the xxmaj worlds in theaters , i was hooked on the topic . i could think back to my favorite parts in the movie , people getting vaporized , people panicking , fire , explosions , it was all so great ... \n",
              " \n",
              "  xxmaj so a few weeks later i enter my video store , and i see xxmaj david xxmaj michael xxmaj latt 's version of xxmaj war xxmaj of xxmaj the xxmaj worlds on the shelf . \" xxmaj it could n't have come onto xxup dvd , that fast , could it ? \" i said to myself . i read the back of the case and saw xxup c. xxmaj thomas xxmaj howell , instead . \" xxmaj oh , i remember him from xxmaj the xxmaj outsiders ! \" xxmaj so i thought , it might have been a try . \n",
              " \n",
              "  i was wrong , dead wrong . xxmaj as soon as i watched the opening credits , watched them take forever , i knew something was wrong . xxmaj something was going to disappoint me in this film and it did . xxmaj the whole movie stunk like a cheese sauce that was left in the fridge for 10 years . xxmaj from the acting , the special effects ( stupid looking tripod things , when people get vaporized they turn into orange skeletons ) , and most of all , it did n't even come close to being as interesting as the xxmaj spielberg version , in fact , the plot was boring , and there were only 3 scenes of destruction ! xxmaj what the crap ? i ended up being so bored , that i had to fast forward through the movie until i found something that looked even remotely interesting . xxmaj and nothing was really . \n",
              " \n",
              "  xxmaj my advice : xxmaj do n't even touch this movie , stay 100 feet away from it . xxmaj the xxmaj spielberg version is coming out near the end of this month , buy that one ! xxmaj but please , please , i beg of you ! xxmaj stay away from this turd before it smothers us all !\n",
              "y: CategoryList\n",
              "neg,neg,neg,neg,neg\n",
              "Path: /root/.fastai/data/imdb;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(49855, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(49855, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1150, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1150, 1150, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1150, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f05e1643b70>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True)], callbacks=[RNNTrainer\n",
              "learn: RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (25000 items)\n",
              "x: TextList\n",
              "xxbos xxup ok this movie had a terrible premise . xxmaj be serious according to the movie they had just been through an apocalyptic war yet they have money to buy huge robots and pit them against each other . xxmaj each country decides instead of investing into rebuilding their country they would rather fight with robots no one could afford . xxmaj here 's a better idea , lets rely on our most inept resource , jocks , to fight our battles . \n",
              " \n",
              "  xxmaj everyone says what about the director , what about him . xxmaj he makes a good movie , he makes a bad movie . xxmaj there is no reason to give this movie some credit just because of the director , maybe he was asleep ? i thoroughly enjoyed this movie , because it was so cheesy and ridiculous i had to laugh . i actually had a good time watching it , well except for the cowboy mentor who turns out to be an xxunk me no one would see this guy as an assassin , so it is a surprise , however lame ) xxmaj what kind of training exercise is a jungle jim anyway . i was sad to see xxmaj mst3k had not done this one . i am giving a two star rating however because nothing could be as bad as \" manos the hands of fate . \" \n",
              " \n",
              "  xxmaj the budget does not matter either , i have seen plenty of reasonable movies that had nothing for budgets like cube . xxmaj the storyline was not even plausible and i have seen better acting in school plays . xxmaj surly they could have afforded an eleven year old from any middle school play . \n",
              " \n",
              "  xxmaj anyway pick it up , it is a fun movie to watch .,xxbos i saw \" xxmaj rachel 's xxmaj attic , \" thinking that i would be in for an enjoyably visceral , ride . xxmaj however , it was not to be the case . xxmaj visceral , yes , but enjoyable ? xxmaj that would be a big , fat , no ! xxmaj in fact , the only reason that i gave it a \" 3 , \" is due to the fact that xxmaj gunnar xxmaj hansen appears ( ever so briefly ) as one of the film 's reprehensible characters . xxmaj how they ever lured xxmaj mr. xxmaj hansen into this piece of ... work , i 'll never know . xxmaj the story idea is interesting but poorly executed . xxmaj the direction is pedestrian and the acting is mediocre . xxmaj the only thing that is worse than that , are the special effects . xxup yikes ! ! ! i 've seen better effects in a grade school play . xxmaj give it up , xxmaj mr. w , it 's time for a career change ... i hear they 're hiring at xxmaj mel 's xxmaj diner ! xxmaj there are very few , well made , xxmaj xxunk movies coming out of xxmaj michigan ... and \" xxmaj rachel 's xxmaj attic \" is n't one of them .,xxbos i 'm actually watching this film as i write this . . . xxmaj if the following comments \" prove my lack of development as a true , artistic film maker \" , then so be it . . . \n",
              " \n",
              "  xxmaj but . . . i thought ( am still thinking as i 'm presently viewing ) that this film . . . to put it mildly , is very , very overrated . xxmaj again , very . \n",
              " \n",
              "  xxmaj it looks like a really , really bad student film done by a someone with beyond extremely limited resources . . . and who did n't pay that much attention to detail . \n",
              " \n",
              "  i do n't want to go on and on regarding all the different ways that i find this film lacking , but . . . well . . . i just do n't get it ( rememeber , i fully admit that maybe it 's xxup me that 's the idiot here - not the film maker - for not getting this \" piece of imaginative genius \" ) . . . i rented this on a whim because the reviews were very , very outstanding . . . \n",
              " \n",
              "  xxmaj sheesh . . .,xxbos xxmaj from the start this film was awful ! xxmaj why was it that bad ? ? xxmaj if it is n't the naked women , not only in need of a decent plastic surgeon but also the expertise of a dentist followed by a free hand out of xxmaj xxunk xxunk ! ! xxmaj then it 's the ' crazy ' old guy at the gas station , who is n't so much crazy , but more \" i 'm not sure how to act a great deal so i will stare straight ahead and look as stupid as i can while pretending to shout in robotic tones about something in the woods \" ! ! xxmaj then back to these naked nymphs in need of a cure for xxunk xxrep 4 . apparently , without touching you ... and this is according to the opening scene xxrep 4 . they can cause a nasty looking red rash on your neck , which i assumed to be a chunk of flesh missing but just looks as though it could do with some xxup xxunk to clear it right up . xxmaj then you have xxmaj sophie xxmaj holland who plays xxmaj ally , i have never seen such b xxrep 6 a d acting , she is more of a \" me me me , if i 'm not having fun no - one else is , and i do n't wanna do this so i wo n't , and i 'm the meanest cow on the planet , i 'm sarcastic , petty and if i do n't get things my way i will sulk ! \" , kind of person xxrep 4 . reminds me more of a 6 yr old girl 's attitude . i do n't think it 's even worth mentioning the dire camera angles that remind me of xxmaj blair xxmaj witch , or how low - budget the film actually was that when xxmaj judd was hacking at the ' locked ' door it was in fact open before he reached to unlock it from the other side ! ! xxmaj this film is completely laughable ! xxmaj if it were a spoof then it would have been successful ... only just though , but , as a horror film is was just plain wrong ! ! ! i ca n't even being to describe everything that went xxunk up in this movie i would run out of room ! xxmaj although it was funny to watch xxmaj andrew drip raspberry juice in his ear every time he opened his mouth while xxmaj tom xxmaj savini 's character was completely blind to the two hiding under the table directly in his line of vision ! ! xxmaj it was even funnier when these two thought they could escape on a god damn tractor , which as we all know is thee number one hated vehicle to get stuck behind since its so god damn slow ! xxmaj so is it any wonder they do n't get away with it ? ? xxmaj and how many people do you know that can slice open their wrist and then run around for hours as if nothing ever happened ! xxmaj no pain , no weakening from blood loss , nothing ! ? xxmaj but the silliest part is when all of a sudden ( and i mean that literally ) it 's one xxup year later and xxmaj molly is still wandering the woods after having escaped the nymphs , and then lo and behold , xxmaj shaun xxmaj hutson picks her up ... of course not without a line to promote his books ! ! ( altho admittedly he is one of my fav authors ) but suddenly , and with absolutely no hint of an xxunk as to how and why ... she 's evil herself and lures xxmaj hutson to his death , then we cut to the crazy dude from the beginning suddenly wandering round the woods with a petrol can , even after his ' dazzling ' performance on why no - one should ever venture there for whatever reason ... cue the nymphs stupidly xxunk each other around a bit for fun while xxmaj crazy pours petrol everywhere xxrep 4 . and here endeth the film xxrep 4 . finally ! xxmaj my conclusion xxrep 4 . if you had n't already guessed by now xxrep 4 . absolute rubbish ! xxmaj there was no proper thought went into it at all , whoever was aiming the camera needed firing ... and come to think of it so did 99 % of the cast ! xxmaj if the right director , actors , and budget got behind this it could have been decent . xxmaj but , once again , low - budget xxmaj english horror films but the rest of the genre , the country , and the xxmaj english film - making industry to shame ! ! ( xxmaj and i 'm xxmaj english so i 'm allowed to say that ) ! xxmaj in fact the only decent and exciting part of the movie is in the first 15 - 20 mins when we watch it turn from night to day over a field type area . xxmaj all i kept thinking throughout this was \" xxmaj jesus xxmaj christ in heaven why oh why did you allow someone to make this , its absolute cow 's testicles ! ! \" xxmaj but i ca n't turn a film off after i 've started watching it unfortunately . i had to watch xxmaj from xxmaj dusk xxmaj til xxmaj dawn afterwards just to remind myself that xxmaj tom xxmaj savini does have it in him to act well ! xxmaj if there was an option for 0 / 10 then believe me i woulda chose that , cuz this film is n't even worth the one point i did give it ! \n",
              " \n",
              "  xxmaj but this is just my opinion , watch it and decide for yourself .,xxbos i did n't think this was as absolutely horrible as some people apparently do . xxmaj it passes as one of those cheesy horror movies you might waste time with in the middle of the night when you ca n't sleep , although admittedly it 's no better quality than that . xxmaj it 's true that the acting is n't great - i thought xxmaj marianne mcandrew as xxmaj cathy xxmaj beck , for example , came across as completely passionless - but the main problem is that several aspects of the plot did n't really make sense to me . xxmaj the xxmaj becks are on a trip described by xxmaj john ( xxmaj stewart xxmaj moss ) as part work and partly the honeymoon they never had ( now that 's romantic ! ) xxmaj the work part has something to do with touring caves , which in itself sounds strange ( how does being part of a tour group through a cave relate to anyone 's work ? ) but it gets stranger when we find out that he 's a doctor doing research in the area of xxunk medicine ( huh ? xxmaj that connection completely lost me . ) xxmaj bitten by a bat while he 's in the cave , he begins to transform into what i guess was supposed to be a human - bat hybrid ( although when we finally see him in makeup he looks a lot more like an ape - man of some sort ) and a killing spree starts . xxmaj here 's another problem . xxmaj the first killing is a nurse in a hospital . xxmaj at first , everyone thinks her death was an accident . xxmaj the second murder is of a young girl , who is described as having her throat ripped out . xxmaj the sheriff ( xxmaj michael xxmaj pataki ) then tells us that her death was similar to the nurse 's ( meaning throat ripped out ? - xxmaj how could anyone think that was an accident ? ) xxmaj and what 's with the sheriff ? xxmaj he seems pretty no - nonsense until the scene in xxmaj cathy 's hotel room when he takes a swig of liquor and then almost rapes her , after which everything seems to go back to normal . xxmaj it 's saddled with an ending that left almost everything unresolved , and also with one of the most irritating theme songs i 've ever heard in a movie . xxmaj even for all that , there was something here that kept me watching . xxmaj sometimes pure cheesiness can get you through an hour and a half . xxmaj pretty bad , yeah - but not as awful as some people say .\n",
              "y: CategoryList\n",
              "neg,neg,neg,neg,neg\n",
              "Path: /root/.fastai/data/imdb;\n",
              "\n",
              "Valid: LabelList (25000 items)\n",
              "x: TextList\n",
              "xxbos * * xxmaj may xxmaj contain xxmaj spoilers * * \n",
              " \n",
              "  xxmaj the main character , a nobleman named xxmaj fallon , is stranded on an island with characters so looney and lethal he might have been better off drowning . xxmaj count xxmaj xxunk de xxmaj sade ( pronounced \" dee - xxunk \" ) talks to his own hallucinations and sees all intruders on the island as invading pirates . xxmaj he routinely beats mute servant xxmaj anne and tortures his unwilling guests in the dungeon . xxmaj xxunk laughs are provided by giant \" xxmaj xxunk \" slave xxmaj mantis who talks with a xxmaj deep xxmaj south accent and helps de xxmaj sade hunt down trespassers in the style of xxup the xxup most xxup dangerous xxup game . xxmaj de xxmaj sade 's crazed wife , ravaged by leprosy , provides some truly scary moments as she prowls the dungeon and embraces a helplessly chained prisoner . ( xxmaj this scene was viewed on late - night xxup tv by many kids who carried the memory into adulthood . ) xxmaj the one nearly - normal person in sight is xxmaj cassandra , who has self - deprecation down to a science . ( \" i used to be a nurse , now i 'm not much of anything . \" ) xxmaj she and xxmaj fallon plan their escape and ultimately encounter an enemy more fearsome than de xxmaj sade and xxmaj mantis combined . \n",
              " \n",
              " \t xxmaj this movie was shot in xxmaj san xxmaj antonio and directed by a man more competent at drawing horror comics than making horror movies . ( i 'll say this much for xxmaj mr. xxmaj boyette -- he does showcase his xxunk with contagion here , as he did in his comics . ) xxmaj it 's rather like an xxmaj andy xxmaj milligan melodrama minus the meat cleavers . xxmaj the period wardrobe , library music , abuse of the handicapped and all - around misanthropy makes one wonder if xxmaj andy was n't called in as a consultant . xxmaj however , xxmaj milligan made better costumes and wrote better dialogue . xxmaj technical gaffes are too numerous to list here but you know this flick is in trouble when you see the opening shipwreck , which looks like it was shot in a fish tank . xxmaj also , a film made in xxmaj texas should have had real spiders and snakes rather than rubber ones . xxmaj glorious xxmaj xxunk gives this melodrama the garish look it so richly deserves . xxmaj fallon 's initial encounter with the leprous xxmaj countess is truly horrifying , as is the movie 's parting shot . xxmaj if the rest had been half as harrowing , xxup the xxup dungeon xxup of xxup harrow would have been a terror classic . xxmaj instead it 's a funny piece of schlock that trash - fiends will love , for all the wrong reasons .,xxbos i can find no redeeming value to this movie . xxmaj it appears to be loosely based on the xxmaj lion xxmaj king school of thought . xxmaj father gets killed , son ca n't fill the shoes and tries to run away , etc , etc , etc . xxmaj the only difference ( other than being in a barnyard instead of a jungle ) is that xxmaj barnyard tries to \" liven things up \" with club - type music . xxmaj they go way over the top in trying to be cool . xxmaj the problem is that it really is n't cool . xxmaj it 's like \" that guy \" . xxmaj everybody knows at least one of \" those guys \" who are older and still hang out with the younger crowd in a futile attempt to cling onto their youth . xxmaj they try to be cool to fit in but they really are n't . xxmaj that 's this movie . \n",
              " \n",
              "  xxmaj but hey , if you have money to burn and you feel like paying someone to suck 90 minutes of you life away , by all means do n't let me stop you .,xxbos xxmaj produced by xxmaj nott xxmaj entertainment , this movie is \" nott \" very good at all . i sat through the first 15 minutes of the film before judging that the acting is bad , the casting is bad and camera work is bad . xxmaj as i hear that there is a download of this film floating around on the internet , it is \" nott \" even worth the bandwidth . \n",
              " \n",
              "  xxmaj up until the time i wrote this review , the average vote for this movie was an 8.5 , which prompted me to view it and there was an average high majority of 10 's for it , obviously voted on by liars and shills . xxmaj this movie is \" nott \" for everyone . xxmaj or parents , if you want to punish your kids with this awful film , have them sit through this one for xxmaj halloween .,xxbos xxmaj everyone knows that late night movies are n't xxmaj oscar contenders . xxmaj fine . i mean i 'll admit that i was a bit tipsy and bored and figured i 'd get to some skin - a - max . xxmaj it 's pretty bad when the info on the xxup tv guide channel makes fun of the movie in the description . xxmaj it even gave it half a star . xxmaj to be fair , i did sit throw the whole thing cause man it was s xxrep 10 o bad . i could n't stop laughing . i mean the words coming out of these people mouth and how they were trying to be serious . xxmaj most of the time i think the people on the screen were trying their hardest to not to laugh . xxmaj in fact i think in one scene they did laugh . xxmaj anyways the movie did n't make sense . xxmaj it was like that one xxmaj sopranos episode with the fat gay guy . xxmaj only the xxmaj sopranos is great show . xxmaj but it was terrible , i mean , no nudity , just sex scenes out of the 90 's . xxmaj you know the kind that use shadows and silhouettes instead of flesh . i gave it a two cause this flick makes for a good drinking game movie . i mean with all the cheese , it helps to get the wine out . xxmaj if its late at night , and all that is on xxup tv is this and that xxmaj tony xxmaj little guy and his exercise bike , then i suggest xxmaj tony xxmaj little .,xxbos xxmaj after watching the xxmaj steven xxmaj spielberg version of xxmaj war xxmaj of xxmaj the xxmaj worlds in theaters , i was hooked on the topic . i could think back to my favorite parts in the movie , people getting vaporized , people panicking , fire , explosions , it was all so great ... \n",
              " \n",
              "  xxmaj so a few weeks later i enter my video store , and i see xxmaj david xxmaj michael xxmaj latt 's version of xxmaj war xxmaj of xxmaj the xxmaj worlds on the shelf . \" xxmaj it could n't have come onto xxup dvd , that fast , could it ? \" i said to myself . i read the back of the case and saw xxup c. xxmaj thomas xxmaj howell , instead . \" xxmaj oh , i remember him from xxmaj the xxmaj outsiders ! \" xxmaj so i thought , it might have been a try . \n",
              " \n",
              "  i was wrong , dead wrong . xxmaj as soon as i watched the opening credits , watched them take forever , i knew something was wrong . xxmaj something was going to disappoint me in this film and it did . xxmaj the whole movie stunk like a cheese sauce that was left in the fridge for 10 years . xxmaj from the acting , the special effects ( stupid looking tripod things , when people get vaporized they turn into orange skeletons ) , and most of all , it did n't even come close to being as interesting as the xxmaj spielberg version , in fact , the plot was boring , and there were only 3 scenes of destruction ! xxmaj what the crap ? i ended up being so bored , that i had to fast forward through the movie until i found something that looked even remotely interesting . xxmaj and nothing was really . \n",
              " \n",
              "  xxmaj my advice : xxmaj do n't even touch this movie , stay 100 feet away from it . xxmaj the xxmaj spielberg version is coming out near the end of this month , buy that one ! xxmaj but please , please , i beg of you ! xxmaj stay away from this turd before it smothers us all !\n",
              "y: CategoryList\n",
              "neg,neg,neg,neg,neg\n",
              "Path: /root/.fastai/data/imdb;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(49855, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(49855, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1150, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1150, 1150, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1150, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f05e1643b70>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True)], callbacks=[...], layer_groups=[Sequential(\n",
              "  (0): Embedding(49855, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(49855, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1150, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1150, 1150, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1150, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True)\n",
              "alpha: 2.0\n",
              "beta: 1.0], layer_groups=[Sequential(\n",
              "  (0): Embedding(49855, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(49855, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1150, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1150, 1150, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1150, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "wNF2nrdnJIHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What's happening here is we are just fine-tuning the last layers. Normally after we fine-tune the last layers, the next thing we do is we go unfreeze and train the whole thing. So here it is:"
      ]
    },
    {
      "metadata": {
        "id": "J4cTwhcuJDXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k2YEchutJGzr",
        "colab_type": "code",
        "outputId": "1640ea5c-846b-4dcf-b894-26eafcf15b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1,1e-3,moms=(0.8,0.7))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.889421</td>\n",
              "      <td>3.835869</td>\n",
              "      <td>0.316218</td>\n",
              "      <td>23:46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "40zgXt5QJe_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, even on a pretty beefy GPU that takes two or three hours. In fact, I'm still under fitting. So probably tonight, I might train it overnight and try and do a little bit better. I'm guessing I could probably train this a bit longer because you can see the accuracy hasn't started going down again. So I wouldn't mind try to train that a bit longer. But the accuracy, it's interesting. 0.3 means we're guessing the next word of the movie review correctly about a third of the time. That sounds like a pretty high number﹣the idea that you can actually guess the next word that often. So it's a good sign that my language model is doing pretty well. For more limited domain documents (like medical transcripts and legal transcripts), you'll often find this accuracy gets a lot higher. So sometimes this can be even 50% or more. But 0.3 or more is pretty good."
      ]
    },
    {
      "metadata": {
        "id": "cpUXxHgfKAzx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('fine_tuned')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H9obACYOJxlk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Predicting with Language Model [25:43](https://youtu.be/C9UdVPE3ynA?t=1543)\n",
        "\n",
        "You can now run **learn.predict** and pass in the start of a sentence, and it will try and finish off that sentence for you."
      ]
    },
    {
      "metadata": {
        "id": "RdHrBLdgKENn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('fine_tuned');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOSlu3XaJdDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TEXT = 'I liked this movie because'\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NKKsIj3yKcVp",
        "colab_type": "code",
        "outputId": "9b51c150-d700-452f-e183-13ff1fd9f42a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print('\\n'.join(learn.predict(TEXT,N_WORDS,temperature=0.75) for _ in range(N_SENTENCES)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I liked this movie because it 's a nice subject than The Da Vinci Code . i feel that i 've grown into a - movie when i saw the movie ! Because i was in the juvenile watching it\n",
            "I liked this movie because it was great . i loved it . The movie has a good storyline and that i am a big fan of classic movies . This movie is a great movie , it has a great cast ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BG3XemAHLIaO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now I should mention, this is not designed to be a good text generation system. This is really more designed to check that it seems to be creating something that's vaguely sensible. There's a lot lot of tricks that you can use to generate much higher quality text﹣none of which we're using here. But you can kind of see that it's certainly not random words that it's generating. It sounds vaguely English like even though it doesn't make any sense.\n",
        "\n",
        "At this point, we have a movie review model. So now we're going to save that in order to load it into our classifier (i.e. to be a pre-trained model for the classifier). But I actually don't want to save the whole thing. A lot of the second half of the language model is all about predicting the next word rather than about understanding the sentence so far. So the bit which is specifically about understanding the sentence so far is called the **encoder**, so I just save that (i.e. the bit that understands the sentence rather than the bit that generates the word).\n",
        "\n",
        "**`--> encoder : understanding the sentence`**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ay-Xnp8yLckr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save_encoder('fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPlsy-LmM2sM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Making Classifier[27:18](https://youtu.be/C9UdVPE3ynA?t=1638)\n",
        "\n",
        "Now we're ready to create our classifier. Step one, as per usual, is to create a data bunch, and we're going to do basically exactly the same thing.\n",
        "\n",
        "We'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
      ]
    },
    {
      "metadata": {
        "id": "PjQICXU8M1ja",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#path = untar_data(URLs.IMDB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OrjaEZd5N2B6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_clas = (TextList.from_folder(path,vocab=data_lm.vocab)\n",
        "            #grab all the text files in path\n",
        "            .split_by_folder(valid='test')\n",
        "            #split by train and valid folder (that only keeps 'train' and 'test')\n",
        "            # so no need to filter)\n",
        "            .label_from_folder(classes=['neg','pos'])\n",
        "            #label them all with their folders\n",
        "            .databunch(bs=bs)\n",
        "            )\n",
        "data_clas.save('data_clas.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRUg_Zv8PMNC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- **vocab = data_lm.vocab** -> But we want to **make sure that it uses exactly the same vocab that are used for the language model**. If word number 10 was **'the'** in the language model, we need to make sure that word number 10 is **'the'** in the classifier. Because otherwise, the pre-trained model is going to be totally meaningless. So that's why we pass in the vocab from the language model to make sure that this data bunch is going to have exactly the same vocab. That's an important step.\n",
        "\n",
        "- **split_by_folder** -> remember, the last time we had split randomly, but this time we need to make sure that the labels of the test set are not touched. So we split by folder.\n",
        "\n",
        "And then this time we label it not for a language model but we label these classes (['neg', 'pos']). Then finally create a data bunch.\n",
        "\n",
        "Sometimes you'll find that you ran out of GPU memory. I was running this in an 11G machine, so you should make sure this number (bs) is a bit lower if you run out of memory. You may also want to make sure you restart the notebook and kind of start it just from here (classifier section). Batch size 50 is as high as I could get on an 11G card. If you're using a p2 or p3 on Amazon or the K80 on Google, for example, I think you'll get 16G so you might be able to make this bit higher, get it up to 64. So you can find whatever batch size fits on your card.\n",
        "\n",
        "So here is our data bunch:"
      ]
    },
    {
      "metadata": {
        "id": "Thc7hLU8QKXV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_clas = load_data(path,'data_clas.pkl',bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKoLBdW2QWsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "b451f547-38ba-442d-800c-11f76308a0ae"
      },
      "cell_type": "code",
      "source": [
        "data_clas.show_batch()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj this movie was recently released on xxup dvd in the xxup us and i finally got the chance to see this hard - to - find gem . xxmaj it even came with original theatrical previews of other xxmaj italian horror classics like \" xxup xxunk \" and \" xxup beyond xxup the xxup darkness \" . xxmaj unfortunately , the previews were the best thing about this</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos i thought that xxup rotj was clearly the best out of the three xxmaj star xxmaj wars movies . i find it surprising that xxup rotj is considered the weakest installment in the xxmaj trilogy by many who have voted . xxmaj to me it seemed like xxup rotj was the best because it had the most profound plot , the most suspense , surprises , most xxunk the</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos xxmaj waitress : xxmaj honey , here 's them eggs you ordered . xxmaj honey , like bee , get it ? xxmaj that 's called pointless foreshadowing . \\n \\n  xxmaj edward xxmaj basket : xxmaj huh ? ( xxmaj on the road ) xxmaj basket : xxmaj here 's your doll back , little girl . xxmaj you really should n't be so careless with your</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "89qoLxTiQcny",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = text_classifier_learner(data_clas,AWD_LSTM,drop_mult=0.5)\n",
        "learn.load_encoder('fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE1fTCSiQxyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This time, rather than creating a language model learner, we're creating a text classifier learner. But again, same thing﹣pass in the data that we want, **figure out how much regularization we need. If you're overfitting then you can increase this number (drop_mult). If you're underfitting, you can decrease the number**. \n",
        "\n",
        "**And most importantly, load in our pre train model**. Remember, specifically it's this half of the model called the encoder which is the bit that we want to load in."
      ]
    },
    {
      "metadata": {
        "id": "OnQP5Pb0SY9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "815f81bc-61b2-48ae-8922-875307332998"
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lY_QuTduSUbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7e46d5e6-4649-4e38-a3e7-e54ab0235117"
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndZUl2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qQxUkDvoSb_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "9e0a0bfb-c35e-452f-b744-3513066e1321"
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1,2e-2,moms=(0.8,0.7))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.349817</td>\n",
              "      <td>0.271961</td>\n",
              "      <td>0.889200</td>\n",
              "      <td>06:56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "m2P6kWaZThlx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're already up nearly to 92% accuracy after less than three minutes of training. So this is a nice thing. In your particular domain (whether it be law, medicine, journalism, government, or whatever), you probably only need to train your domain's language model once. And that might take overnight to train well. But once you've got it, you can now very quickly create all kinds of different classifiers and models with that. In this case, already a pretty good model after three minutes. So when you first start doing this. you might find it a bit annoying that your first models take four hours or more to create that language model. But the key thing to remember is you only have to do that once for your entire domain of stuff that you're interested in. And then you can build lots of different classifiers and other models on top of that in a few minutes."
      ]
    },
    {
      "metadata": {
        "id": "DNQhh7b8SkQP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LP67svn8Sl73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('first');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h0lrUTCDTi8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "ed6f042c-6535-4872-8055-b62f73538104"
      },
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2),moms=(0.8,0.7))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.292109</td>\n",
              "      <td>0.212522</td>\n",
              "      <td>0.917960</td>\n",
              "      <td>07:21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vCMTaeOLS_qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can save that to make sure we don't have to run it again.\n",
        "\n",
        "And then, here's something interesting. I'm not going to say **unfreeze**. Instead, I'm going to say **freeze_to**. What that says is unfreeze the last two layers, don't unfreeze the whole thing. **We've just found it really helps with these text classification not to unfreeze the whole thing, but to unfreeze one layer at a time**. **--> important note** \n",
        "\n",
        "- unfreeze the last two layers\n",
        "- train it a little bit more\n",
        "- unfreeze the next layer again\n",
        "- train it a little bit more\n",
        "- unfreeze the whole thing\n",
        "- train it a little bit more"
      ]
    },
    {
      "metadata": {
        "id": "hmMnRTD6UN7M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('second')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1a7IvS7VUQI9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('second');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8musPDMgUfGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "838d2030-7a2c-4a71-8c92-e8cdb938dc18"
      },
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1,slice(5e-3/(2.6**4),5e-3),moms=(0.8,0.7))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.246719</td>\n",
              "      <td>0.177915</td>\n",
              "      <td>0.931520</td>\n",
              "      <td>10:17</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3zyQXOSzUwh7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('third')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2SkAe1CUyBw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('third');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CoHLRoAQU2A3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "b82d52dd-0a14-47f5-f01d-c0a472848e13"
      },
      "cell_type": "code",
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2,slice(1e-3/(2.6**4),1e-3),moms=(0.8,0.7))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.206873</td>\n",
              "      <td>0.172875</td>\n",
              "      <td>0.933480</td>\n",
              "      <td>11:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.199514</td>\n",
              "      <td>0.170912</td>\n",
              "      <td>0.934920</td>\n",
              "      <td>12:38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MP1ME7TRVLo0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You also see I'm passing in this thing (moms=(0.8,0.7))﹣momentums equals 0.8,0.7. We are going to learn exactly what that means probably next week. We may even automate it. So maybe by the time you watch the video of this, this won't even be necessary anymore. Basically we found for training recurrent neural networks (RNNs), it really helps to decrease the momentum a little bit. So that's what that is.\n",
        "\n",
        "That gets us a 94.4% accuracy after about half an hour or less of training. There's quite a lot less of training the actual classifier. We can actually get this quite a bit better with a few tricks. I don't know if we'll learn all the tricks this part. It might be next part. But even this very simple standard approach is pretty great.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson4/2.png?raw=true)\n",
        "\n",
        "If we compare it to last year's state of the art on IMDb, this is from The [CoVe paper](https://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf) from McCann et al. at Salesforce Research. Their paper was 91.8% accurate. And the best paper they could find, they found a fairly domain-specific sentiment analysis paper from 2017, they've got 94.1%. And here, we've got 94.4%. And the best models I've been able to build aince have been about 95.1%. So if you're looking to do text classification, this really standardized transfer learning approach works super well."
      ]
    },
    {
      "metadata": {
        "id": "0uzpD28kYs6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6084bab6-3cfa-4efe-d81f-adce73c0c154"
      },
      "cell_type": "code",
      "source": [
        "learn.predict(\"I really loved that movie, it was awesome!\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category pos, tensor(1), tensor([5.6003e-04, 9.9944e-01]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "metadata": {
        "id": "3kku24ybsVxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64d0cafe-86be-4d84-f0ba-87db359ba8a4"
      },
      "cell_type": "code",
      "source": [
        "learn.predict('the movie is very messy')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Category neg, tensor(0), tensor([0.9064, 0.0936]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "JnkWi7dBsf4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "??learn.predict()\n",
        "\n",
        "def predict(self, item:ItemBase, **kwargs):\n",
        "        \"Return predicted class, label and probabilities for `item`.\"\n",
        "        batch = self.data.one_item(item)\n",
        "        res = self.pred_batch(batch=batch)\n",
        "        pred,x = res[0],batch[0]\n",
        "        norm = getattr(self.data,'norm',False)\n",
        "        if norm:\n",
        "            x = self.data.denorm(x)\n",
        "            if norm.keywords.get('do_y',False): pred = self.data.denorm(pred)\n",
        "        ds = self.data.single_ds\n",
        "        pred = ds.y.analyze_pred(pred, **kwargs)\n",
        "        out = ds.y.reconstruct(pred, ds.x.reconstruct(x[0])) if has_arg(ds.y.reconstruct, 'x') else ds.y.reconstruct(pred)\n",
        "        return out, pred, res[0]\n",
        "        \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B03xH5CUWZ2W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question**: Where does the magic number of 2.6^{4} in the learning rate come from? [33:38](https://youtu.be/C9UdVPE3ynA?t=2018)\n",
        "\n",
        "`learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))`\n",
        "\n",
        "Good question. So the learning rate is various things divided by 2.6 to the fourth. The reason it's to the fourth, you will learn about at the end of today. So let's focus on the 2.6. Why 2.6? Basically, as we're going to see in more detail later today, this number, the difference between the bottom of the slice and the top of the slice is basically what's the difference between how quickly the lowest layer of the model learns versus the highest layer of the model learns. So this is called discriminative learning rates. So really the question is as you go from layer to layer, how much do I decrease the learning rate by? And we found out that for NLP RNNs, the answer is 2.6.\n",
        "\n",
        "How do we find out that it's 2.6? I ran lots and lots of different models using lots of different sets of hyper parameters of various types (dropout, learning rates, and discriminative learning rate and so forth), and then I created something called a random forest which is a kind of model where I attempted to predict how accurate my NLP classifier would be based on the hyper parameters. And then I used random forest interpretation methods to basically figure out what the optimal parameter settings were, and I found out that the answer for this number was 2.6. So that's actually not something I've published or I don't think I've even talked about it before, so there's a new piece of information. Actually, a few months after I did this, Stephen Merity and somebody else did publish a paper describing a similar approach, so the basic idea may be out there already.\n",
        "\n",
        "Some of that idea comes from a researcher named Frank Hutter and one of his collaborators. They did some interesting work showing how you can use random forests to actually find optimal hyperparameters. So it's kind of a neat trick. A lot of people are very interested in this thing called Auto ML which is this idea of like building models to figure out how to train your model. We're not big fans of it on the whole. But we do find that building models to better understand how your hyper parameters work, and then finding those rules of thumb like oh basically it can always be 2.6 quite helpful. So there's just something we've kind of been playing with."
      ]
    }
  ]
}