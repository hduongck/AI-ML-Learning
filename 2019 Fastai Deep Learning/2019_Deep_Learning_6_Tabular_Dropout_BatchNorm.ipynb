{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019_Deep_Learning_6_Tabular_Dropout_BatchNorm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/2019%20Fastai%20Deep%20Learning/2019_Deep_Learning_6_Tabular_Dropout_BatchNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "iZUNFc578qKh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Lesson 6**\n",
        "\n",
        "[Video](https://youtu.be/U7c-nYXrKD4) / [Course Forum](https://forums.fast.ai/t/lesson-6-official-resources-and-updates/31441)\n",
        "\n",
        "Welcome to lesson 6 where we're going to do a deep dive into computer vision, convolutional neural networks, what is a convolution, and we're also going to learn the final regularization tricks after last lesson learning about weight decay/L2 regularization.\n",
        "\n",
        "**Platform.ai**\n",
        "\n",
        "I want to start by showing you something that I'm really excited about and I've had a small hand and helping to to create. For those of you that saw [my talk on ted.com](https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn/up-next?language=en), you might have noticed this really interesting demo that we did about four years ago showing a way to quickly build models with unlabeled data. It's been four years but we're finally at a point where we're ready to put this out in the world and let people use it. And the first people we're going to let use it are you folks.\n",
        "\n",
        "So the company is called platform.ai and the reason I'm mentioning it here is that it's going to let you create models on different types of datasets to what you can do now, that is to say datasets that you don't have labels for yet. We're actually going to help you label them. So this is the first time this has been shown before, so I'm pretty thrilled about it. Let me give you a quick demo.\n",
        "\n",
        "If you'd go to platform.ai and choose \"get started\" you'll be able to create a new project. And if you create a new project you can either upload your own images. Uploading it at 500 or so works pretty well. You can upload a few thousand, but to start, upload 500 or so. They all have to be in a single folder. So we're assuming that you've got a whole bunch of images that you haven't got any labels for or you can start with one of the existing collections if you want to play around, so I've started with the cars collection kind of going back to what we did four years ago.\n",
        "\n",
        "This is what happens when you first go into platform.ai and look at the collection of images you've uploaded - a random sample of them will appear on the screen. As you'll recognize, they are projected from a deep learning space into a 2D space using a pre-trained model. For this initial version, it's an ImageNet model we're using. As things move along, we'll be adding more and more pre train models. And what I'm going to do is I want to add labels to this data set representing which angle a photo of the car was taken from which is something that actually ImageNet is going to be really bad at because ImageNet has learnt to recognize the difference between cars versus bicycles and ImageNet knows that the angle you take a photo on actually doesn't matter. So we want to try and create labels using the kind of thing that actually ImageNet specifically learn to ignore.\n",
        "\n",
        "So the projection that you see, we can click these layer buttons at the top to switch to user projection using a different layer of the neural net. Here's the last layer which is going to be a total waste of time for us because it's really going to be projecting things based on what kind of thing it thinks it is. The first layer is probably going to be a waste of time for us as well because there's very little interesting semantic content there. But if I go into the middle, in layer 3, we may well be able to find some differences there.\n",
        "\n",
        "Then what you can do is you can click on the projection button here (you can actually just press up and down rather than just pressing the the arrows at the top) to switch between projections or left and right so switch between layers. And what you can do is you can basically look around until you notice that there's a projection which is kind of separated out things you're interested in. So this one actually I notice that it's got a whole bunch of cars that are from the front right over here. So if we zoom in a little bit, we can double check - \"yeah that looks pretty good, they're all kind of front right.\" So we can click on here to go to selection mode, and we can grab a few, and then you should check:\n",
        "\n",
        "![](https://github.com/hiromis/notes/raw/master/lesson6/1.png?raw=true)\n",
        "\n",
        "What we're doing here is we're trying to take advantage of the combination of human plus machine. The machine is pretty good at quickly doing calculations, but as a human I'm pretty good at looking at a lot of things at once and seeing the odd one out. So in this case I'm looking for cars that aren't front right, and so by laying them in front of me, I can do that really quickly. It's like \"okay definitely that one\" so just click on the ones that you don't want. All right, it's all good.\n",
        "\n",
        "Then you can just go back. Then what you can do is you can either put them into a new category by typing in \"create a new label\" or you can click on one of the existing ones. So before I came, I just created a few. So here's front right, so I just click on it here.\n",
        "\n",
        "The basic idea is that you keep flicking through different layers or projections to try and find groups that represent the things you're interested in, and then over time you'll start to realize that there are some things that are a little bit harder. For example, I'm having trouble finding sides, so what I can do is I can see over here there's a few sides, so I can zoom in here and click on a couple of them. Then I'll say \"find similar\" and this is going to basically look in that projection space and not just at the images that are currently displayed but all of the images that you uploaded, and hopefully I might be able to label a few more side images at that point. It's going through and checking all of the images that you uploaded to see if any of them have projections in this space which are similar to the ones I've selected. Hopefully we'll find a few more of what I'm interested in.\n",
        "\n",
        "Now if I want to try to find a projection that separates the sides from the front right, I can click on each of those two and then over here this button is now called \"switch to the projection that maximizes the distance between the labels.\" What this is going to do is it's going to try and find the best projection that separates out those classes. The goal here is to help me visually inspect and quickly find a bunch of things that I can use to label.\n",
        "\n",
        "They're the kind of the the key features and it's done a good job. You can see down here, we've now got a whole bunch of sides which I can now grab because I was having a lot of trouble finding them before. And it's always worth double-checking. It's kind of interesting to see how the neural nets behave - like there seems to be more sports cars in this group than average as well. So it's kind of found side angles of sports cars, so that's kind of interesting. So I've got those, now I clicks \"side\" and there we go.\n",
        "\n",
        "Once you've done that a few times, I find if you've got a hundred or so labels, you can then click on the train model button, and it'll take a couple of minutes, and come back and show you your train model. After it's trained, which I did it on a smaller number of labels earlier, you can then switch this vary opacity button, and it'll actually fade out the ones that are already predicted pretty well. It'll also give you a estimate as to how accurate it thinks the model is. The main reason I mentioned this for you is so that you can now click the download button and it'll download the predictions, which is what we hope will be interesting to most people. But what I think will be interesting to you as deep learning students is it'll download your labels. So now you can use that labeled subset of data along with the unlabeled set that you haven't labeled yet to see if you can build a better model than platform.ai has done for you. See if you can use that initial set of data to get going, creating models which you weren't able to label before.\n",
        "\n",
        "Clearly, there are some things that this system is better at than others. For things that require really zooming in closely and taking a very very close inspection, this isn't going to work very well. This is really designed for things that the human eye can kind of pick up fairly readily. But we'd love to get feedback as well, and you can click on the Help button to give feedback. Also there's a [platform.ai discussion topic in our forum](https://forums.fast.ai/t/platform-ai-discussion/31445). So Arshak if you can stand up, Arshak is the CEO of the company. He'll be there helping out answering questions and so forth. I hope people find that useful. It's been many years getting to this point, and I'm glad we're finally there."
      ]
    },
    {
      "metadata": {
        "id": "rdFAss7UAkOX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Finishing up regularization for the Tabular Learner[9:48](https://youtu.be/U7c-nYXrKD4?t=588)\n",
        "\n",
        "One of the reasons I wanted to mention this today is that we're going to be doing a big dive into convolutions later in this lesson. So I'm going to circle back to this to try and explain a little bit more about how that is working under the hood, and give you a kind of a sense of what's going on. But before we do, we have to finish off last week's discussion of regularization. We were talking about regularization specifically in the context of the tabular learner because the tabular learner, this is the init method in the tabular learner:\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson6/2.png?raw=True)\n",
        "\n",
        "And our goal was to understand everything here, and we're not quite there yet. Last week we were looking at the adult data set which is a really simple (kind of over simple) data set that's just for toy purposes. So this week, let's look at a data set that's much more interesting - a Kaggle competition data set so we know what the the best in the world and Kaggle competitions' results tend to be much harder to beat than academic state-of-the-art results tend to be because a lot more people work on Kaggle competitions than most academic data sets. So it's a really good challenge to try and do well on a Kaggle competition data set.\n",
        "\n",
        "The rossmann data set is they've got 3,000 drug stores in Europe and you're trying to predict how many products they're going to sell in the next couple of weeks. One of the interesting things about this is that the test set for this is from a time period that is more recent than the training set. This is really common. If you want to predict things, there's no point predicting things that are in the middle of your training set. You want to predict things in the future.\n",
        "\n",
        "Another interesting thing about it is the evaluation metric they provided is the root mean squared percent error.\n",
        "\n",
        "$\\textrm{RMSPE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{y_i - \\hat{y}_i}{y_i}\\right)^2}$\n",
        "\n",
        "This is just a normal root mean squared error except we go actual minus prediction divided by actual, so in other words it's the \"percent\" error that we're taking the root mean squared of. There's a couple of interesting features.\n",
        "\n",
        "Always interesting to look at the leaderboard. So the leaderboard, the winner was 0.1. The paper that we've roughly replicated was 0.105 ~ 0.106, and the 10th place out of 3,000 was 0.11ish - a bit less.\n",
        "\n",
        "We're gonna skip over a little bit. The data that was provided here was they provided a small number of files but they also let competitors provide additional external data as long as they shared it with all the competitors. So in practice the data set we're going to use contains six or seven tables. The way that you join tables and stuff isn't really part of a deep learning course. So I'm going to skip over it, and instead I'm going to refer you to [Introduction to Machine Learning for Coders](http://course.fast.ai/ml) which will take you step-by-step through the data preparation for this. We've provided it for you in [rossman_data_clean.ipynb](https://github.com/fastai/course-v3/blob/master/nbs/dl1/rossman_data_clean.ipynb) so you'll see the whole process there. You'll need to run through that notebook to create these pickle files that we read here ([lesson6-rossmann.ipynb](https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson6-rossmann.ipynb)):\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kh6sUubJ69Sz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from fastai.tabular import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_jG2xboK0UT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget https://drive.google.com/uc?export=download&id=1Q1dh9JTR3U0eS3DiKWCKB9ZyonsUhI6d\n",
        "#!wget https://drive.google.com/uc?export=download&confirm=_m2l&id=1PFjtY9c5f4Oe_CDgYK_kltaDCOtOWik2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sKDu79r9X_hE",
        "colab_type": "code",
        "outputId": "ff63ae90-a453-49b9-9740-a686956deaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8rebATm4K5Tb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.makedirs('/root/.fastai/data/rossmann',exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lDa3wsXILpyu",
        "colab_type": "code",
        "outputId": "01d680db-e7dc-4f3d-e107-c197bcc64ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "shutil.copy('/content/drive/My Drive/Colab Notebooks/Exercise_files/Rossmann/train_clean','/root/.fastai/data/rossmann')\n",
        "shutil.copy('/content/drive/My Drive/Colab Notebooks/Exercise_files/Rossmann/test_clean','/root/.fastai/data/rossmann')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.fastai/data/rossmann/test_clean'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "n1j6QwteAs9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = Config().data_path()/'rossmann'\n",
        "train_df = pd.read_pickle(path/'train_clean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yynp3l-vHkj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Time Series and add_datepart [13:21]\n",
        "\n",
        "I just want to mention one particularly interesting part of the rossmann data clean notebook which is you'll see there's something that says **add_datepart** and I wanted to explain what's going on here."
      ]
    },
    {
      "metadata": {
        "id": "cR4daQrMHqdq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#add_datepart(train,'Date',drop=False)\n",
        "#add_datepart(test,'Date',drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AgNUjYnhIg-n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've been mentioning for a while that we're going to look at time series. Pretty much everybody whom I've spoken to about it has assumed that I'm going to do some kind of recurrent neural network. But I'm not. Interestingly, the main academic group that studies time series is econometrics but they tend to study one very specific kind of time series which is where the only data you have is a sequence of time points of one thing. That's the only thing you have is one sequence. In real life, that's almost never the case. Normally, we would have some information about the store that represents or the people that it represents. We'd have metadata, we'd have sequences of other things measured at similar time periods or different time periods. So most of the time, I find in practice the the state-of-the-art results when it comes to competitions on more real-world data sets don't tend to use recurrent neural networks. But instead, they tend to take the time piece which in this case it was a date we were given in the data, and they add a whole bunch of metadata. So in our case, for example, we've added day of week. We were given a date. We've added a day of week, year, month, week of year, day of month, day of week, day of year, and then a bunch of booleans is it at the month start/end, quarter year start/end, elapsed time since 1970, so forth.\n",
        "\n",
        "If you run this one function add_datepart and pass it a date, it'll add all of these columns to your data set for you. What that means is that, let's take a very reasonable example. Purchasing behavior probably changes on payday. Payday might be the fifteenth of the month. So if you have a thing here called this is day of month, then it'll be able to recognize every time something is a fifteen there and associated it with a higher, in this case, embedding matrix value. Basically, we can't expect a neural net to do all of our feature engineering for us. We can expect it to find nonlinearities and interactions and stuff like that. But for something like taking a date like this (2015-07-31 00:00:00) and figuring out that the fifteenth of the month is something when interesting things happen. It's much better if we can provide that information for it.\n",
        "\n",
        "So this is a really useful function to use. Once you've done this, you can treat many kinds of time-series problems as regular tabular problems. I say \"many\" kinds not \"all\". If there's very complex kind of state involved in a time series such as equity trading or something like that, this probably won't be the case or this won't be the only thing you need. But in this case, it'll get us a really good result and in practice, most of the time I find this works well.\n",
        "\n",
        "Tabular data is normally in Pandas, so we just stored them as standard Python pickle files. We can read them in. We can take a look at the first five records."
      ]
    },
    {
      "metadata": {
        "id": "XhobaoQVJnfD",
        "colab_type": "code",
        "outputId": "c05f9658-3548-4f6b-e616-6a7778ed8d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        }
      },
      "cell_type": "code",
      "source": [
        "train_df.head().T"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Store</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DayOfWeek</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "      <td>2015-07-31 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sales</th>\n",
              "      <td>5263</td>\n",
              "      <td>6064</td>\n",
              "      <td>8314</td>\n",
              "      <td>13995</td>\n",
              "      <td>4822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Customers</th>\n",
              "      <td>555</td>\n",
              "      <td>625</td>\n",
              "      <td>821</td>\n",
              "      <td>1498</td>\n",
              "      <td>559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Open</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Year</th>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Month</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Week</th>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Day</th>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dayofweek</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dayofyear</th>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_month_end</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_month_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_quarter_end</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_quarter_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_year_end</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Is_year_start</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Elapsed</th>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "      <td>1438300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StoreType</th>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Assortment</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <td>1270</td>\n",
              "      <td>570</td>\n",
              "      <td>14130</td>\n",
              "      <td>620</td>\n",
              "      <td>29910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionOpenSinceMonth</th>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionOpenSinceYear</th>\n",
              "      <td>2008</td>\n",
              "      <td>2007</td>\n",
              "      <td>2006</td>\n",
              "      <td>2009</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2SinceWeek</th>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionOpenSince</th>\n",
              "      <td>2008-09-15 00:00:00</td>\n",
              "      <td>2007-11-15 00:00:00</td>\n",
              "      <td>2006-12-15 00:00:00</td>\n",
              "      <td>2009-09-15 00:00:00</td>\n",
              "      <td>2015-04-15 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionDaysOpen</th>\n",
              "      <td>2510</td>\n",
              "      <td>2815</td>\n",
              "      <td>3150</td>\n",
              "      <td>2145</td>\n",
              "      <td>107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CompetitionMonthsOpen</th>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2Since</th>\n",
              "      <td>1900-01-01 00:00:00</td>\n",
              "      <td>2010-03-29 00:00:00</td>\n",
              "      <td>2011-04-04 00:00:00</td>\n",
              "      <td>1900-01-01 00:00:00</td>\n",
              "      <td>1900-01-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2Days</th>\n",
              "      <td>0</td>\n",
              "      <td>1950</td>\n",
              "      <td>1579</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo2Weeks</th>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterSchoolHoliday</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforeSchoolHoliday</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterStateHoliday</th>\n",
              "      <td>57</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforeStateHoliday</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterPromo</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforePromo</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday_bw</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday_bw</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo_bw</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday_fw</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday_fw</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo_fw</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterSchoolHoliday_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforeSchoolHoliday_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterStateHoliday_y</th>\n",
              "      <td>57</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "      <td>67</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforeStateHoliday_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AfterPromo_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BeforePromo_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday_bw_y</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday_bw_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo_bw_y</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SchoolHoliday_fw_y</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>StateHoliday_fw_y</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Promo_fw_y</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>105 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             0                    1  \\\n",
              "index                                        0                    1   \n",
              "Store                                        1                    2   \n",
              "DayOfWeek                                    5                    5   \n",
              "Date                       2015-07-31 00:00:00  2015-07-31 00:00:00   \n",
              "Sales                                     5263                 6064   \n",
              "Customers                                  555                  625   \n",
              "Open                                         1                    1   \n",
              "Promo                                        1                    1   \n",
              "StateHoliday                             False                False   \n",
              "SchoolHoliday                                1                    1   \n",
              "Year                                      2015                 2015   \n",
              "Month                                        7                    7   \n",
              "Week                                        31                   31   \n",
              "Day                                         31                   31   \n",
              "Dayofweek                                    4                    4   \n",
              "Dayofyear                                  212                  212   \n",
              "Is_month_end                              True                 True   \n",
              "Is_month_start                           False                False   \n",
              "Is_quarter_end                           False                False   \n",
              "Is_quarter_start                         False                False   \n",
              "Is_year_end                              False                False   \n",
              "Is_year_start                            False                False   \n",
              "Elapsed                             1438300800           1438300800   \n",
              "StoreType                                    c                    a   \n",
              "Assortment                                   a                    a   \n",
              "CompetitionDistance                       1270                  570   \n",
              "CompetitionOpenSinceMonth                    9                   11   \n",
              "CompetitionOpenSinceYear                  2008                 2007   \n",
              "Promo2                                       0                    1   \n",
              "Promo2SinceWeek                              1                   13   \n",
              "...                                        ...                  ...   \n",
              "CompetitionOpenSince       2008-09-15 00:00:00  2007-11-15 00:00:00   \n",
              "CompetitionDaysOpen                       2510                 2815   \n",
              "CompetitionMonthsOpen                       24                   24   \n",
              "Promo2Since                1900-01-01 00:00:00  2010-03-29 00:00:00   \n",
              "Promo2Days                                   0                 1950   \n",
              "Promo2Weeks                                  0                   25   \n",
              "AfterSchoolHoliday                           0                    0   \n",
              "BeforeSchoolHoliday                          0                    0   \n",
              "AfterStateHoliday                           57                   67   \n",
              "BeforeStateHoliday                           0                    0   \n",
              "AfterPromo                                   0                    0   \n",
              "BeforePromo                                  0                    0   \n",
              "SchoolHoliday_bw                             5                    5   \n",
              "StateHoliday_bw                              0                    0   \n",
              "Promo_bw                                     5                    5   \n",
              "SchoolHoliday_fw                             5                    5   \n",
              "StateHoliday_fw                              0                    0   \n",
              "Promo_fw                                     5                    5   \n",
              "AfterSchoolHoliday_y                         0                    0   \n",
              "BeforeSchoolHoliday_y                        0                    0   \n",
              "AfterStateHoliday_y                         57                   67   \n",
              "BeforeStateHoliday_y                         0                    0   \n",
              "AfterPromo_y                                 0                    0   \n",
              "BeforePromo_y                                0                    0   \n",
              "SchoolHoliday_bw_y                           5                    5   \n",
              "StateHoliday_bw_y                            0                    0   \n",
              "Promo_bw_y                                   5                    5   \n",
              "SchoolHoliday_fw_y                           5                    5   \n",
              "StateHoliday_fw_y                            0                    0   \n",
              "Promo_fw_y                                   5                    5   \n",
              "\n",
              "                                             2                    3  \\\n",
              "index                                        2                    3   \n",
              "Store                                        3                    4   \n",
              "DayOfWeek                                    5                    5   \n",
              "Date                       2015-07-31 00:00:00  2015-07-31 00:00:00   \n",
              "Sales                                     8314                13995   \n",
              "Customers                                  821                 1498   \n",
              "Open                                         1                    1   \n",
              "Promo                                        1                    1   \n",
              "StateHoliday                             False                False   \n",
              "SchoolHoliday                                1                    1   \n",
              "Year                                      2015                 2015   \n",
              "Month                                        7                    7   \n",
              "Week                                        31                   31   \n",
              "Day                                         31                   31   \n",
              "Dayofweek                                    4                    4   \n",
              "Dayofyear                                  212                  212   \n",
              "Is_month_end                              True                 True   \n",
              "Is_month_start                           False                False   \n",
              "Is_quarter_end                           False                False   \n",
              "Is_quarter_start                         False                False   \n",
              "Is_year_end                              False                False   \n",
              "Is_year_start                            False                False   \n",
              "Elapsed                             1438300800           1438300800   \n",
              "StoreType                                    a                    c   \n",
              "Assortment                                   a                    c   \n",
              "CompetitionDistance                      14130                  620   \n",
              "CompetitionOpenSinceMonth                   12                    9   \n",
              "CompetitionOpenSinceYear                  2006                 2009   \n",
              "Promo2                                       1                    0   \n",
              "Promo2SinceWeek                             14                    1   \n",
              "...                                        ...                  ...   \n",
              "CompetitionOpenSince       2006-12-15 00:00:00  2009-09-15 00:00:00   \n",
              "CompetitionDaysOpen                       3150                 2145   \n",
              "CompetitionMonthsOpen                       24                   24   \n",
              "Promo2Since                2011-04-04 00:00:00  1900-01-01 00:00:00   \n",
              "Promo2Days                                1579                    0   \n",
              "Promo2Weeks                                 25                    0   \n",
              "AfterSchoolHoliday                           0                    0   \n",
              "BeforeSchoolHoliday                          0                    0   \n",
              "AfterStateHoliday                           57                   67   \n",
              "BeforeStateHoliday                           0                    0   \n",
              "AfterPromo                                   0                    0   \n",
              "BeforePromo                                  0                    0   \n",
              "SchoolHoliday_bw                             5                    5   \n",
              "StateHoliday_bw                              0                    0   \n",
              "Promo_bw                                     5                    5   \n",
              "SchoolHoliday_fw                             5                    5   \n",
              "StateHoliday_fw                              0                    0   \n",
              "Promo_fw                                     5                    5   \n",
              "AfterSchoolHoliday_y                         0                    0   \n",
              "BeforeSchoolHoliday_y                        0                    0   \n",
              "AfterStateHoliday_y                         57                   67   \n",
              "BeforeStateHoliday_y                         0                    0   \n",
              "AfterPromo_y                                 0                    0   \n",
              "BeforePromo_y                                0                    0   \n",
              "SchoolHoliday_bw_y                           5                    5   \n",
              "StateHoliday_bw_y                            0                    0   \n",
              "Promo_bw_y                                   5                    5   \n",
              "SchoolHoliday_fw_y                           5                    5   \n",
              "StateHoliday_fw_y                            0                    0   \n",
              "Promo_fw_y                                   5                    5   \n",
              "\n",
              "                                             4  \n",
              "index                                        4  \n",
              "Store                                        5  \n",
              "DayOfWeek                                    5  \n",
              "Date                       2015-07-31 00:00:00  \n",
              "Sales                                     4822  \n",
              "Customers                                  559  \n",
              "Open                                         1  \n",
              "Promo                                        1  \n",
              "StateHoliday                             False  \n",
              "SchoolHoliday                                1  \n",
              "Year                                      2015  \n",
              "Month                                        7  \n",
              "Week                                        31  \n",
              "Day                                         31  \n",
              "Dayofweek                                    4  \n",
              "Dayofyear                                  212  \n",
              "Is_month_end                              True  \n",
              "Is_month_start                           False  \n",
              "Is_quarter_end                           False  \n",
              "Is_quarter_start                         False  \n",
              "Is_year_end                              False  \n",
              "Is_year_start                            False  \n",
              "Elapsed                             1438300800  \n",
              "StoreType                                    a  \n",
              "Assortment                                   a  \n",
              "CompetitionDistance                      29910  \n",
              "CompetitionOpenSinceMonth                    4  \n",
              "CompetitionOpenSinceYear                  2015  \n",
              "Promo2                                       0  \n",
              "Promo2SinceWeek                              1  \n",
              "...                                        ...  \n",
              "CompetitionOpenSince       2015-04-15 00:00:00  \n",
              "CompetitionDaysOpen                        107  \n",
              "CompetitionMonthsOpen                        3  \n",
              "Promo2Since                1900-01-01 00:00:00  \n",
              "Promo2Days                                   0  \n",
              "Promo2Weeks                                  0  \n",
              "AfterSchoolHoliday                           0  \n",
              "BeforeSchoolHoliday                          0  \n",
              "AfterStateHoliday                           57  \n",
              "BeforeStateHoliday                           0  \n",
              "AfterPromo                                   0  \n",
              "BeforePromo                                  0  \n",
              "SchoolHoliday_bw                             5  \n",
              "StateHoliday_bw                              0  \n",
              "Promo_bw                                     5  \n",
              "SchoolHoliday_fw                             5  \n",
              "StateHoliday_fw                              0  \n",
              "Promo_fw                                     5  \n",
              "AfterSchoolHoliday_y                         0  \n",
              "BeforeSchoolHoliday_y                        0  \n",
              "AfterStateHoliday_y                         57  \n",
              "BeforeStateHoliday_y                         0  \n",
              "AfterPromo_y                                 0  \n",
              "BeforePromo_y                                0  \n",
              "SchoolHoliday_bw_y                           5  \n",
              "StateHoliday_bw_y                            0  \n",
              "Promo_bw_y                                   5  \n",
              "SchoolHoliday_fw_y                           5  \n",
              "StateHoliday_fw_y                            0  \n",
              "Promo_fw_y                                   5  \n",
              "\n",
              "[105 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "mactF2z5JvMy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The key thing here is that we're trying to on a particular date for a particular store ID, we want to predict the number of sales. Sales is the dependent variable."
      ]
    },
    {
      "metadata": {
        "id": "Zt2IU58mJ__j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Preprocesses [16:52](https://youtu.be/U7c-nYXrKD4?t=1012) : experiencing with small dataset\n",
        "\n",
        "The first thing I'm going to show you is something called pre-processes. You've already learned about transforms. **Transforms** are bits of code that **run every time something is grabbed from a data set** so it's really good for data augmentation that we'll learn about today, which is that it's going to get a different random value every time it's sampled. Preprocesses are like transforms, but they're a little bit different which is that they run once before you do any training. Really importantly, they **run once on the training set and then any kind of state or metadata that's created is then shared with the validation and test set**.\n",
        "\n",
        "Let me give you an example. When we've been doing image recognition and we've had a set of classes to all the different pet breeds and they've been turned into numbers. The thing that's actually doing that for us is a preprocessor that's being created in the background. That makes sure that the classes for the training set are the same as the classes for the validation and the classes of the test set. So we're going to do something very similar here. For example, if we create a little small subset of a data for playing with. This is a really good idea when you start with a new data set."
      ]
    },
    {
      "metadata": {
        "id": "2-Zp_q_raLuc",
        "colab_type": "code",
        "outputId": "aa5ae2ec-0b1d-4d5b-c2cc-7148303bbc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n = len(train_df); n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "844338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "NSrlSyyLOA_N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = np.random.permutation(range(n))[:2000]\n",
        "idx.sort()\n",
        "small_train_df = train_df.iloc[idx[:1000]]\n",
        "small_test_df = train_df.iloc[idx[1000:]]\n",
        "small_cont_vars = ['CompetitionDistance','Mean_Humidity']\n",
        "small_cat_vars = ['Store','DayOfWeek','PromoInterval']\n",
        "small_train_df = small_train_df[small_cat_vars+small_cont_vars+['Sales']]\n",
        "small_test_df = small_test_df[small_cat_vars+small_cont_vars+['Sales']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NP8gFUAjPKJn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've just grabbed 2,000 IDs at random. Then I'm just going to grab a little training set and a little test set - half and half of those 2,000 IDs, and it's going to grab five columns. Then we can just play around with this. Nice and easy. Here's the first few of those from the training set:"
      ]
    },
    {
      "metadata": {
        "id": "2jMTNIu_PMcG",
        "colab_type": "code",
        "outputId": "d462e993-5165-41b4-cff2-de7fa967786e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "small_train_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>PromoInterval</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>Mean_Humidity</th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>710</th>\n",
              "      <td>712</td>\n",
              "      <td>5</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>4870.0</td>\n",
              "      <td>50</td>\n",
              "      <td>7421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>115</td>\n",
              "      <td>4</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>5740.0</td>\n",
              "      <td>59</td>\n",
              "      <td>9370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>605</td>\n",
              "      <td>4</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>10310.0</td>\n",
              "      <td>60</td>\n",
              "      <td>9623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1919</th>\n",
              "      <td>808</td>\n",
              "      <td>4</td>\n",
              "      <td>Feb,May,Aug,Nov</td>\n",
              "      <td>18620.0</td>\n",
              "      <td>51</td>\n",
              "      <td>9579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3316</th>\n",
              "      <td>1093</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10450.0</td>\n",
              "      <td>55</td>\n",
              "      <td>9169</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Store  DayOfWeek    PromoInterval  CompetitionDistance  Mean_Humidity  \\\n",
              "710     712          5  Jan,Apr,Jul,Oct               4870.0             50   \n",
              "1227    115          4  Jan,Apr,Jul,Oct               5740.0             59   \n",
              "1716    605          4  Jan,Apr,Jul,Oct              10310.0             60   \n",
              "1919    808          4  Feb,May,Aug,Nov              18620.0             51   \n",
              "3316   1093          3              NaN              10450.0             55   \n",
              "\n",
              "      Sales  \n",
              "710    7421  \n",
              "1227   9370  \n",
              "1716   9623  \n",
              "1919   9579  \n",
              "3316   9169  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "LCwqszp8PSw_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see, one of them is called promo interval and it has these strings, and sometimes it's missing. In Pandas, missing is NaN."
      ]
    },
    {
      "metadata": {
        "id": "B5xRk4naPg34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Categorify [18:39](https://youtu.be/U7c-nYXrKD4?t=1119)\n",
        "\n",
        "The first preprocessor I'll show you is Categorify."
      ]
    },
    {
      "metadata": {
        "id": "hhxz_eDNPUkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "categorify = Categorify(small_cat_vars,small_cont_vars)\n",
        "categorify(small_train_df)\n",
        "categorify(small_test_df,test=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HKRhEOGPQ1Vr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Categorify does basically the same thing that **.classes** thing for image recognition does for a dependent variable. It's going to take these strings, it's going to find all of the possible unique values of it, and it's going to create a list of them, and then it's going to turn the strings into numbers. \n",
        "\n",
        "So if I call it on my training set, that'll create categories there (**small_train_df**) and then I call it on my test set passing in **test=true**, that makes sure it's going to use the same categories that I had before. Now when I say **.head**, it looks exactly the same:"
      ]
    },
    {
      "metadata": {
        "id": "GgIJJDFWRBTt",
        "colab_type": "code",
        "outputId": "a8cc635c-c95c-4a59-d940-3443d45a2a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "small_test_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>PromoInterval</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>Mean_Humidity</th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>418751</th>\n",
              "      <td>182</td>\n",
              "      <td>4</td>\n",
              "      <td>Mar,Jun,Sept,Dec</td>\n",
              "      <td>1390.0</td>\n",
              "      <td>55</td>\n",
              "      <td>5656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418861</th>\n",
              "      <td>292</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1100.0</td>\n",
              "      <td>58</td>\n",
              "      <td>6893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418926</th>\n",
              "      <td>357</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2060.0</td>\n",
              "      <td>51</td>\n",
              "      <td>8474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420049</th>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2640.0</td>\n",
              "      <td>73</td>\n",
              "      <td>6573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420622</th>\n",
              "      <td>941</td>\n",
              "      <td>3</td>\n",
              "      <td>Jan,Apr,Jul,Oct</td>\n",
              "      <td>1200.0</td>\n",
              "      <td>72</td>\n",
              "      <td>5848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Store DayOfWeek     PromoInterval  CompetitionDistance  Mean_Humidity  \\\n",
              "418751   182         4  Mar,Jun,Sept,Dec               1390.0             55   \n",
              "418861   292         4               NaN               1100.0             58   \n",
              "418926   357         4               NaN               2060.0             51   \n",
              "420049   NaN         3               NaN               2640.0             73   \n",
              "420622   941         3   Jan,Apr,Jul,Oct               1200.0             72   \n",
              "\n",
              "        Sales  \n",
              "418751   5656  \n",
              "418861   6893  \n",
              "418926   8474  \n",
              "420049   6573  \n",
              "420622   5848  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Mb9UP7_MRZF9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's because Pandas has turned this into a categorical variable which internally is storing numbers but externally is showing me the strings. But I can look inside promo interval to look at the **cat.categories**, this is all standard Pandas here, to show me a list of all of what we would call \"classes\" in fast.ai or would be called just \"categories\" in Pandas."
      ]
    },
    {
      "metadata": {
        "id": "DXhsgj3JRec7",
        "colab_type": "code",
        "outputId": "0e8fd432-5746-4a3d-d596-a829e0bd6541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "small_train_df['PromoInterval'].cat.codes[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710     1\n",
              "1227    1\n",
              "1716    1\n",
              "1919    0\n",
              "3316   -1\n",
              "dtype: int8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "b5k1-K6CaZyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So then if I look at the **cat.codes**, you can see here this list here is the numbers that are actually stored (-1, -1, 1, -1, 1). What are these minus ones? The minus ones represent NaN - they represent \"missing\". So Pandas uses the special code -1 to be mean missing.\n",
        "\n",
        "As you know, these are going to end up in an embedding matrix, and we can't look up item -1 in an embedding matrix. So internally in fast.ai, we add one to all of these."
      ]
    },
    {
      "metadata": {
        "id": "fUfHqNZ7agYo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Fill Missing [20:18](https://youtu.be/U7c-nYXrKD4?t=1218)\n",
        "\n",
        "Another useful preprocessor is **FillMissing**. Again, you can call it on the data frame, you can call on the test passing in **test=true**."
      ]
    },
    {
      "metadata": {
        "id": "TASCqCHpaSUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fill_missing = FillMissing(small_cat_vars,small_cont_vars)\n",
        "fill_missing(small_train_df)\n",
        "fill_missing(small_test_df,test=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yodsUI-aa4Op",
        "colab_type": "code",
        "outputId": "43b9febe-32ec-4b8b-c209-2803a01d5e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "cell_type": "code",
      "source": [
        "small_train_df[small_train_df['CompetitionDistance_na']==True]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Store</th>\n",
              "      <th>DayOfWeek</th>\n",
              "      <th>PromoInterval</th>\n",
              "      <th>CompetitionDistance</th>\n",
              "      <th>Mean_Humidity</th>\n",
              "      <th>Sales</th>\n",
              "      <th>CompetitionDistance_na</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>187665</th>\n",
              "      <td>291</td>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2155.0</td>\n",
              "      <td>80</td>\n",
              "      <td>4027</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215082</th>\n",
              "      <td>291</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2155.0</td>\n",
              "      <td>88</td>\n",
              "      <td>13018</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Store DayOfWeek PromoInterval  CompetitionDistance  Mean_Humidity  \\\n",
              "187665   291         6           NaN               2155.0             80   \n",
              "215082   291         5           NaN               2155.0             88   \n",
              "\n",
              "        Sales  CompetitionDistance_na  \n",
              "187665   4027                    True  \n",
              "215082  13018                    True  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "jnXfTct4bYHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will create, for anything that has a missing value, it'll create an additional column with the column name underscore na (e.g. CompetitionDistance_na) and it will set it for true for any time that was missing. Then what we do is, we replace competition distance with the median for those. Why do we do this? Well, because very commonly the fact that something's missing is of itself interesting (i.e. it turns out the fact that this is missing helps you predict your outcome). So we certainly want to keep that information in a convenient boolean column, so that our deep learning model can use it to predict things.\n",
        "\n",
        "But then, we need competition distance to be a continuous variable so we can use it in the continuous variable part of our model. So we can replace it with almost any number because if it turns out that the missingness is important, it can use the interaction of **CompetitionDistance_na** and **CompetitionDistance** to make predictions. So that's what FillMissing does.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JLrDbpSZc4NQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Preparing full data set\n",
        "\n",
        "[21:31](https://youtu.be/U7c-nYXrKD4?t=1291)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mC3q0_G2dCfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df= pd.read_pickle(path/'train_clean')\n",
        "test_df= pd.read_pickle(path/'test_clean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xkobuUcAdONp",
        "colab_type": "code",
        "outputId": "8c488a01-7c0b-4b7f-84f3-c146ed541a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_df),len(test_df)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(844338, 41088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "XolDip76d169",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You don't have to manually call preprocesses yourself. When you call any kind of item list creator, you can pass in a list of pre processes which you can create like this:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "procs=[FillMissing, Categorify, Normalize]\n",
        "\n",
        "data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n",
        "                   .split_by_idx(valid_idx)\n",
        "                   .label_from_df(cols=dep_var, label_cls=FloatList, log=True)\n",
        "                   .databunch())\n",
        "```\n",
        "\n",
        "This is saying \"ok, I want to fill missing, I want to categorify, I want to normalize (i.e. for continuous variables, it'll subtract the mean and divide by the standard deviation to help a train more easily).\" So you just say, those are my procs and then you can just pass it in there and that's it.\n",
        "\n",
        "Later on, you can go **data.export** and it'll save all the metadata for that data bunch so you can, later on, load it in knowing exactly what your category codes are, exactly what median values used for replacing the missing values, and exactly what means and standard deviations you normalize by.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yRSCX0NgbvXT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "procs = [FillMissing,Categorify,Normalize]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IMCTlmDfRrA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Categorical and Continuous Variables [22:23](https://youtu.be/U7c-nYXrKD4?t=1343)\n",
        "\n",
        "The main thing you have to do if you want to create a data bunch of tabular data is tell it what are your categorical variables and what are your continuous variables. As we discussed last week briefly, your categorical variables are not just strings and things, but also I include things like day of week and month and day of month. Even though they're numbers, I make them categorical variables. Because, for example, day of month, I don't think it's going to have a nice smooth curve. I think that the fifteenth of the month and the first of the month and the 30th of the month are probably going to have different purchasing behavior to other days of the month. Therefore, if I make it a categorical variable, it's going to end up creating an embedding matrix and those different days of the month can get different behaviors.\n",
        "\n",
        "**You've actually got to think carefully about which things should be categorical variables**. On the whole, if in doubt and there are not too many levels in your category (that's called the **cardinality**), **f your cardinality is not too high, I would put it as a categorical variable. You can always try an each and see which works best.**"
      ]
    },
    {
      "metadata": {
        "id": "QG4gzV8ibKtV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n",
        "            'CompetitionMonthsOpen', 'Promo2Weeks', 'StoreType', 'Assortment',\n",
        "            'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'State',\n",
        "            'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', \n",
        "            'StateHoliday_bw','SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
        "\n",
        "cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC',\n",
        "             'Min_TemperatureC', 'Max_Humidity', 'Mean_Humidity', 'Min_Humidity',\n",
        "             'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend',\n",
        "             'trend_DE','AfterStateHoliday', 'BeforeStateHoliday', 'Promo', \n",
        "             'SchoolHoliday']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fAgbT4I0glLA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our final data frame that we're going to pass in is going to be a training set with the categorical variables, the continuous variables, the dependent variable, and the date. The date, we're just going to use to create a validation set where we are basically going to say the validation set is going to be the same number of records at the end of the time period that the test set is for Kaggle. That way, we should be able to validate our model nicely."
      ]
    },
    {
      "metadata": {
        "id": "w8UItEZWgY4y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dep_var = 'Sales'\n",
        "df = train_df[cat_vars+cont_vars+[dep_var,'Date']].copy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQ2kaoszg6dB",
        "colab_type": "code",
        "outputId": "e054af6b-3c7d-44af-bfc6-16898a36b07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test_df['Date'].min(),test_df['Date'].max()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Timestamp('2015-08-01 00:00:00'), Timestamp('2015-09-17 00:00:00'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "AqHN1NZHhIs7",
        "colab_type": "code",
        "outputId": "841d4797-1bf5-41a3-cd58-97ff27a55547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cut = train_df['Date'][(train_df['Date'] == train_df['Date'][len(test_df)])].index.max()\n",
        "cut"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41395"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "kVqdnV-Mh3Vd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_idx=range(cut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_xghNyK7h9e9",
        "colab_type": "code",
        "outputId": "b488dddb-511b-4dbd-f311-ac279ad25b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "df[dep_var].head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     5263\n",
              "1     6064\n",
              "2     8314\n",
              "3    13995\n",
              "4     4822\n",
              "Name: Sales, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "Pb7uIgWSiMeU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can create a tabular list."
      ]
    },
    {
      "metadata": {
        "id": "TA503Us4iG9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = (TabularList.from_df(df,path=path,cat_names=cat_vars,cont_names=cont_vars,procs=procs)\n",
        "                   .split_by_idx(valid_idx)\n",
        "                   .label_from_df(cols=dep_var,label_cls=FloatList,log=True)\n",
        "                   .databunch())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DDcsJXA1j-vG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is our standard data block API that you've seen a few times:\n",
        "\n",
        "- From a data frame, passing all of that information.\n",
        "- Split it into valid vs. train.\n",
        "- Label it with a dependent variable.\n",
        "\n",
        "Here's something I don't think you've seen before - **label class (label_cls=FloatList)**. This is our dependent variable (df[dep_var].head() above), and as you can see, this is sales. It's not a float. It's int64. If this was a float, then fast.ai would automatically guess that you want to do a regression. But this is not a float, it's an int. So fast.ai is going to assume you want to do a classification. So when we label it, we have to tell it that the class of the labels we want is a list of floats, not a list of categories (which would otherwise be the default). So this is the thing that's going to automatically turn this into a regression problem for us. Then we create a data bunch. **--> important note**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_rzyNeMIkqWk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Reminder about Doc [25:09](https://youtu.be/U7c-nYXrKD4?t=1509)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0yMC182Lisr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#??FloatList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gG0bY2gkizzN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Init signature: FloatList(items:Iterator, log:bool=False, classes:Collection=None, **kwargs)\n",
        "Source:        \n",
        "class FloatList(ItemList):\n",
        "    \"`ItemList` suitable for storing the floats in items for regression. Will add a `log` if this flag is `True`.\"\n",
        "    def __init__(self, items:Iterator, log:bool=False, classes:Collection=None, **kwargs):\n",
        "        super().__init__(np.array(items, dtype=np.float32), **kwargs)\n",
        "        self.log = log\n",
        "        self.copy_new.append('log')\n",
        "        self.c = self.items.shape[1] if len(self.items.shape) > 1 else 1\n",
        "        self.loss_func = MSELossFlat()\n",
        "\n",
        "    def get(self, i):\n",
        "        o = super().get(i)\n",
        "        return FloatItem(np.log(o) if self.log else o)\n",
        "\n",
        "    def reconstruct(self,t): return FloatItem(t.numpy())\n",
        "File:           /usr/local/lib/python3.6/dist-packages/fastai/data_block.py\n",
        "Type:           type\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GQP_mBGQj2sa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qYetO4nnk3fH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/3.png?raw=True)\n",
        "\n",
        "And I can even jump into the full documentation, and it shows me here that log is something which if true, it's going to take the logarithm of my dependent variable. Why am I doing that? So this is the thing that's actually going to automatically take the log of my y. The reason I'm doing that is because as I mentioned before, the evaluation metric is root mean squared percentage error.\n",
        "\n",
        "$\\textrm{RMSPE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{y_i - \\hat{y}_i}{y_i}\\right)^2}$\n",
        "\n",
        "Neither fast.ai nor PyTorch has a root mean squared percentage error loss function built-in. I don't even know if such a loss function would work super well. But if you want to spend the time thinking about it, you'll notice that this ratio if you first take the log of y and $\\hat{y}$, then becomes a difference rather than the ratio. In other words, if you take the log of y then RMSPE becomes root mean squared error. So that's what we're going to do. We're going to take the log of y and then we're just going to use root mean square error which is the default for a regression problems we won't even have to mention it.\n",
        "\n",
        "![](https://github.com/hiromis/notes/blob/master/lesson6/4.png?raw=True)\n",
        "\n",
        "**The reason that we have this (log=True) here is because this is so common**. Basically anytime you're trying to predict something like a population or a dollar amount of sales, these kind of things tend to have long tail distributions where you care more about percentage differences and exact/absolute differences. So you're very likely to want to do things with log=True and to measure the root mean squared percent error."
      ]
    },
    {
      "metadata": {
        "id": "XtT3QSWAleL7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##y_range [27:12](https://youtu.be/U7c-nYXrKD4?t=1632)"
      ]
    },
    {
      "metadata": {
        "id": "twPosvVVldyq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_log_y = np.log(np.max(train_df['Sales'])*1.2)\n",
        "y_range = torch.tensor([0,max_log_y],device=defaults.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nV-H7cgumYoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We've learned about the **y_range** before which is going to use that sigmoid to help us get in the right range. Because this time the y values are going to be taken the log of it first, we need to make sure that the **y_range** we want is also the log. So I'm going to take the maximum of the sales column. I'm going to multiply it by a little bit because remember how we said it's nice if your range is a bit wider than the range of the data. Then we're going to take the log. That's going to be our maximum. Then our **y_range** will be from zero to a bit more than the maximum.\n",
        "\n",
        "Now we've got our data bunch, we can create a tabular learner from it. Then we have to pass in our architecture. As we briefly discussed, for a tabular model, our architecture is literally the most basic fully connected network - just like we showed in this picture:\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/5.png?raw=True)\n",
        "\n",
        "It's an input, matrix multiply, non-linearity, matrix multiply, non-linearity, matrix multiply, non-linearity, done. What are the interesting things about this is that this competition is three years old, but I'm not aware of any significant advances at least in terms of architecture that would cause me to choose something different to what the third-placed folks did three years ago. We're still basically using simple fully connected models for this problem.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "h63t_gm-iuK2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn = tabular_learner(data,layers=[1000,500],ps=[0.001,0.01],emb_drop=0.04,\n",
        "                       y_range=y_range,metrics=exp_rmspe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APjH2O9VfkXn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Signature: tabular_learner(data:fastai.basic_data.DataBunch, layers:Collection[int], emb_szs:Dict[str, int]=None, metrics=None, ps:Collection[float]=None, emb_drop:float=0.0, y_range:Union[Tuple[float, float], NoneType]=None, use_bn:bool=True, **learn_kwargs)\n",
        "Source:   \n",
        "def tabular_learner(data:DataBunch, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n",
        "        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n",
        "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
        "    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
        "    model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
        "                         y_range=y_range, use_bn=use_bn)\n",
        "    return Learner(data, model, metrics=metrics, **learn_kwargs)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Nffr23yYngE7",
        "colab_type": "code",
        "outputId": "2d67d9dd-681c-4480-9c9d-dc76abe47252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TabularModel(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(1116, 81)\n",
              "    (1): Embedding(8, 5)\n",
              "    (2): Embedding(4, 3)\n",
              "    (3): Embedding(13, 7)\n",
              "    (4): Embedding(32, 11)\n",
              "    (5): Embedding(3, 3)\n",
              "    (6): Embedding(26, 10)\n",
              "    (7): Embedding(27, 10)\n",
              "    (8): Embedding(5, 4)\n",
              "    (9): Embedding(4, 3)\n",
              "    (10): Embedding(4, 3)\n",
              "    (11): Embedding(24, 9)\n",
              "    (12): Embedding(9, 5)\n",
              "    (13): Embedding(13, 7)\n",
              "    (14): Embedding(53, 15)\n",
              "    (15): Embedding(22, 9)\n",
              "    (16): Embedding(7, 5)\n",
              "    (17): Embedding(7, 5)\n",
              "    (18): Embedding(4, 3)\n",
              "    (19): Embedding(4, 3)\n",
              "    (20): Embedding(9, 5)\n",
              "    (21): Embedding(9, 5)\n",
              "    (22): Embedding(3, 3)\n",
              "    (23): Embedding(3, 3)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.04)\n",
              "  (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=233, out_features=1000, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.001)\n",
              "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.01)\n",
              "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "ZVmTawrkn-DN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the intermediate weight matrix is going to have to go from a 1000 activation input to a 500 activation output, which means it's going to have to be 500,000 elements in that weight matrix. That's an awful lot for a data set with only a few hundred thousand rows. So this is going to overfit, and we need to make sure it doesn't. **The way to make sure it doesn't is to use regularization; not to reduce the number of parameters**.\n",
        "- So one way to do that will be to use **weight decay** which fast.ai will use automatically, and you can vary it to something other than the default if you wish. \n",
        "- It turns out in this case, we're going to want more regularization. So we're going to pass in something called **ps** **--> This is going to provide dropout.**\n",
        "- And also this one here **emb_drop - this is going to provide embedding dropout**."
      ]
    },
    {
      "metadata": {
        "id": "ouxZHwOGYrtf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dropout [29:47](https://youtu.be/U7c-nYXrKD4?t=1787)\n",
        "\n",
        "Let's learn about what is dropout. The short version is dropout is a kind of regularization. This is the [dropout paper Nitish Srivastava](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) it was Srivastava's master's thesis under Geoffrey Hinton.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/6.png?raw=true)\n",
        "\n",
        "This picture from the original paper is a really good picture of what's going on. This first picture is a picture of a standard fully connected network and what each line shows is a multiplication of an activation times a weight. Then when you've got multiple arrows coming in, that represents a sum. So this activation here (circled in red) is the sum of all of these inputs times all of these activations. So that's what a normal fully connected neural net looks like.\n",
        "\n",
        "For dropout, we throw that away. **At random, we throw away some percentage of the activations not the weights**, not the parameters. Remember, there's only two types of number in a neural net - parameters also called weights (kind of) and activations. So we're going to throw away some activations.\n",
        "\n",
        "So you can see that when we throw away this activation, all of the things that were connected to it are gone too. For each mini batch, we throw away a different subset of activations. How many do we throw away? We throw each one away with a probability **p**. A common value of p is 0.5. So what does that mean? And you'll see in this case, not only have they deleted at random some of these hidden layers, but they've actually deleted some of the inputs as well. Deleting the inputs is pretty unusual. Normally, we only delete activations in the hidden layers. So what does this do? Well, every time I have a mini batch going through, I, at random, throw away some of the activations. And then the next mini batch, I put them back and I throw away some different ones.\n",
        "\n",
        "It means that no 1 activation can memorize some part of the input because that's what happens if we over fit. If we over fit, some part of the model is basically learning to recognize a particular image rather than a feature in general or a particular item. With dropout, it's going to be very hard for it to do that. In fact, Geoffrey Hinton described part of the thinking behind this as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "I went to my bank. The tellers kept changing and I asked one of them why. He said he didn't know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.\n",
        "```\n",
        "[Hinton: Reddit AMA](https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/d6dgyse)\n",
        "\n",
        "He noticed every time he went to his bank that all the tellers and staff moved around, and he realized the reason for this must be that they're trying to avoid fraud. If they keep moving them around, nobody can specialize so much in that one thing that they're doing that they can figure out a conspiracy to defraud the bank. Now, of course, depends when you ask Hinton. At other times he says that the reason for this was because he thought about how spiking neurons work and he's a neuroscientist by training:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "We don't really know why neurons spike. One theory is that they want to be noisy so as to regularize, because we have many more parameters than we have data points. The idea of dropout is that if you have noisy activations, you can afford to use a much bigger model.\n",
        "```\n",
        "\n",
        "[Hinton: O'Reilly](https://www.oreilly.com/ideas/adapting-ideas-from-neuroscience-for-ai)\n",
        "\n",
        "There's a view that spiking neurons might help regularization, and dropout is a way of matching this idea of spiking neurons. **It's interesting. When you actually ask people where did your idea for some algorithm come from, it basically never comes from math; it always comes from intuition and thinking about physical analogies and stuff like that**.\n",
        "\n",
        "Anyway the truth is a bunch of ideas I guess all flowing around and they came up with this idea of dropout. But the important thing to know is it worked really really well. So we can use it in our models to get generalization for free.\n",
        "\n",
        "Now too much dropout, of course, is reducing the capacity of your model, so it's going to under fit. So you've got to play around with different dropout values for each of your layers to decide.\n",
        "\n",
        "In pretty much **every fast.ai learner, there's a parameter called ps which will be the p-value for the dropout for each layer. So you can just pass in a list, or you can pass it an int and it'll create a list with that value everywhere**. Sometimes it's a little different. For CNN, for example, if you pass in an int, it will use that for the last layer, and half that value for the earlier layers. We basically try to do things represent best practice. But you can always pass in your own list to get exactly the dropout that you want.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YuQkLNSLczPl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Dropout and test time [34:47](https://youtu.be/U7c-nYXrKD4?t=2087)\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/7.png?raw=true)\n",
        "\n",
        "There is an interesting feature of dropout. We talk about training time and test time (we also call inference time). Training time is when we're actually doing that those weight updates - the backpropagation. The training time, dropout works the way we just saw. At test time we turn off dropout. We're not going to do dropout anymore because we wanted to be as accurate as possible. We're not training so we can't cause it to overfit when we're doing inference. So we remove dropout. But what that means is if previously p was 0.5, then half the activations were being removed. Which means when they're all there, now our overall activation level is twice of what it used to be. Therefore, in the paper, they suggest multiplying all of your weights at test time by p.\n",
        "\n",
        "Interestingly, you can dig into the PyTorch source code and you can find the actual C code where dropout is implemented.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/8.png?raw=true)\n",
        "\n",
        "And you can see what they're doing is something quite interesting. They first of all do a Bernoulli trial. So **a Bernoulli trial is with probability 1 - p**, return the value 1 otherwise return the value 0. That's all it means. \n",
        "- **noise.bernoulli_(1-p)** : In this case, p is the probability of dropout, so 1 - p is a probability that we keep the activation. So we end up here with either a 1 or a 0. \n",
        "- **noise.div_(1-p)**: Then (this is interesting) we divide in place (remember underscore means \"in place\" in PyTorch) we divide in place that 1 or 0 by 1 - p. If it's a 0 nothing happens it's still 0. If it's a 1 and p was 0.5, that one now becomes 2. \n",
        "- **`multiply<inplace>(input,noise)`**: Then finally, we multiply in place our input by this noise (i.e. this dropout mask).\n",
        "\n",
        "So in other words, in PyTorch, we don't do the change at test time. We actually do the change at training time - which means that you don't have to do anything special at inference time with PyTorch. It's not just PyTorch, it's quite a common pattern. But it's kind of nice to look inside the PyTorch source code and see dropout; this incredibly cool, incredibly valuable thing, is really just these three lines of code which they do in C because I guess it ends up a bit faster when it's all fused together. But lots of libraries do it in Python and that works well as well. You can even write your own dropout layer, and it should give exactly the same results as this. That'd be a good exercise to try. See if you can create your own dropout layer in Python, and see if you can replicate the results that we get with this dropout layer.\n",
        "\n",
        "[37:38][https://youtu.be/U7c-nYXrKD4?t=2258]\n",
        "\n",
        "So that's dropout. In this case, we're going to use a tiny bit of dropout on the first layer (0.001) and a little bit of dropout on the next layer (0.01), and then we're going to use special dropout on the embedding layer. Now why do we do special dropout on the embedding layer? If you look inside the fast.ai source code, here is our tabular model:\n",
        "\n",
        "```\n",
        "learn = tabular_learner(data,layers=[1000,500],ps=[0.001,0.01],emb_drop=0.04,\n",
        "                                                  y_range=y_range,metrics=exp_rmspe)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BJ6bmVAffJj1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#??tabular_learner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hdBeOdZfzZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        " def tabular_learner(data:DataBunch, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n",
        "        ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n",
        "    \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
        "    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
        "    model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
        "                         y_range=y_range, use_bn=use_bn)\n",
        "    return Learner(data, model, metrics=metrics, **learn_kwargs)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "STc10eJxgUV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So that's dropout. In this case, we're going to use a tiny bit of dropout on the first layer (0.001) and a little bit of dropout on the next layer (0.01), and then we're going to use special dropout on the embedding layer. Now why do we do special dropout on the embedding layer? If you look inside the fast.ai source code, here is our tabular model:"
      ]
    },
    {
      "metadata": {
        "id": "ptjDqQgPnipf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#??TabularModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KuV0po28gDSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class TabularModel(nn.Module):\n",
        "    \"Basic model for tabular data.\"\n",
        "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
        "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n",
        "        super().__init__()\n",
        "        ps = ifnone(ps, [0]*len(layers))\n",
        "        ps = listify(ps, layers)\n",
        "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(emb_drop)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
        "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
        "        sizes = self.get_sizes(layers, out_sz)\n",
        "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
        "        layers = []\n",
        "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
        "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
        "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def get_sizes(self, layers, out_sz):\n",
        "        return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
        "\n",
        "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
        "        if self.n_emb != 0:\n",
        "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.emb_drop(x)\n",
        "        if self.n_cont != 0:\n",
        "            x_cont = self.bn_cont(x_cont)\n",
        "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "        x = self.layers(x)\n",
        "        if self.y_range is not None:\n",
        "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
        "        return x\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Pvn3NzF0gow_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You'll see that in the section that checks that there's some embeddings (if self.n_emb != 0: in forward),\n",
        "\n",
        "- we call each embedding\n",
        "- we concatenate the embeddings into a single matrix\n",
        "- then we call embedding dropout\n",
        "\n",
        "An embedding dropout is simply just a dropout. So it's just an instance of a dropout module. This kind of makes sense, right? **For continuous variables, that continuous variable is just in one column. You wouldn't want to do dropout on that because you're literally deleting the existence of that whole input which is almost certainly not what you want. But for an embedding, and embedding is just effectively a matrix multiplied by a one hot encoded matrix, so it's just another layer. So it makes perfect sense to have dropout on the output of the embedding, because you're putting dropout on those activations of that layer**. So you're basically saying let's delete at random some of the results of that embedding (i.e. some of those activations). So that makes sense.\n",
        "\n",
        "The other reason we do it that way is because I did very extensive experiments about a year ago where on this data set I tried lots of different ways of doing kind of everything. And you can actually see it here:\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/10.png?raw=true)\n",
        "\n",
        "I put it all in a spreadsheet (of course Microsoft Excel), put them into a pivot table to summarize them all together to find out which different choices, hyper parameters, and architectures worked well and worked less well. Then I created all these little graphs:\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson6/11.png?raw=true)\n",
        "\n",
        "These are like little summary training graphs for different combinations of high parameters and architectures. And I found that there was one of them which ended up consistently getting a good predictive accuracy, the bumpiness of the training was pretty low, and you can see, it was just a nice smooth curve.\n",
        "\n",
        "This is an example of the experiments that I do that end up in the fastai library. **So embedding dropout was one of those things that I just found work really well**. Basically the results of these experiments is why it looks like this rather than something else. Well, it's a combination of these experiments but then why did I do these particular experiments? Well because it was very influenced by what worked well in that Kaggle prize winner's paper. But there are quite a few parts of that paper I thought \"there were some other choices they could have made, I wonder why they didn't\" and I tried them out and found out what actually works and what doesn't work as well, and found a few little improvements. So that's the kind of experiments that you can play around with as well when you try different models and architectures; different dropouts, layer numbers, number of activations, and so forth."
      ]
    },
    {
      "metadata": {
        "id": "AjYmqjBJgAJd",
        "colab_type": "code",
        "outputId": "1333dcb9-2844-4fad-b5a5-7a22c33bb3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TabularModel(\n",
              "  (embeds): ModuleList(\n",
              "    (0): Embedding(1116, 81)\n",
              "    (1): Embedding(8, 5)\n",
              "    (2): Embedding(4, 3)\n",
              "    (3): Embedding(13, 7)\n",
              "    (4): Embedding(32, 11)\n",
              "    (5): Embedding(3, 3)\n",
              "    (6): Embedding(26, 10)\n",
              "    (7): Embedding(27, 10)\n",
              "    (8): Embedding(5, 4)\n",
              "    (9): Embedding(4, 3)\n",
              "    (10): Embedding(4, 3)\n",
              "    (11): Embedding(24, 9)\n",
              "    (12): Embedding(9, 5)\n",
              "    (13): Embedding(13, 7)\n",
              "    (14): Embedding(53, 15)\n",
              "    (15): Embedding(22, 9)\n",
              "    (16): Embedding(7, 5)\n",
              "    (17): Embedding(7, 5)\n",
              "    (18): Embedding(4, 3)\n",
              "    (19): Embedding(4, 3)\n",
              "    (20): Embedding(9, 5)\n",
              "    (21): Embedding(9, 5)\n",
              "    (22): Embedding(3, 3)\n",
              "    (23): Embedding(3, 3)\n",
              "  )\n",
              "  (emb_drop): Dropout(p=0.04)\n",
              "  (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=233, out_features=1000, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.001)\n",
              "    (4): Linear(in_features=1000, out_features=500, bias=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Dropout(p=0.01)\n",
              "    (8): Linear(in_features=500, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "AWUiQiRAjqiF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you would expect, in that, there is a whole bunch of embeddings. Each of those embedding matrices tells you the number of levels for each input (the first number) . You can match these with your list **cat_vars**. So the first one will be **Store**, so that's not surprising there are 1,116 stores. Then the second number, of course, is the size of the embedding. That's a number that you get to choose.\n",
        "\n",
        "Fast.ai has some defaults which actually work really really well nearly all the time. So I almost never changed them. But when you create your **tabular_lerner**, you can absolutely pass in an embedding size dictionary which maps variable names to embedding sizes for anything where you want to override the defaults.\n",
        "\n",
        "Then we've got our embedding dropout layer, and then we've got a batch norm layer with 16 inputs. The 16 inputs make sense because we have 16 continuous variables."
      ]
    },
    {
      "metadata": {
        "id": "ftGFSzOwjNJm",
        "colab_type": "code",
        "outputId": "b846ffbd-4865-4cf9-841e-22320773d5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(data.train_ds.cont_names)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "GUcWCidnkQ7N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The length of **cont_names** is 16. So this is something for our continuous variables. Specifically, it's over here, **bn_cont** on our continuous variables: -> **x_cont = self.bn_cont(x_cont)**\n",
        "\n",
        "\n",
        "```\n",
        "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
        "        if self.n_emb != 0:\n",
        "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.emb_drop(x)\n",
        "        if self.n_cont != 0:\n",
        "            x_cont = self.bn_cont(x_cont)\n",
        "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "        x = self.layers(x)\n",
        "        if self.y_range is not None:\n",
        "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
        "        return x\n",
        "```\n",
        "\n",
        "And bn_cont is a nn.BatchNorm1d. What's that? The first short answer is it's one of the things that I experimented with as to having batchnorm not, and I found that it worked really well. Then specifically what it is is extremely unclear. Let me describe it to you.\n",
        "\n",
        "- It's kind of a bit of regularization\n",
        "- It's kind of a bit of training helper\n",
        "\n",
        "It's called batch normalization and it comes from [this paper](https://arxiv.org/abs/1502.03167).\n",
        "\n",
        "[43:06](https://youtu.be/U7c-nYXrKD4?t=2586)\n",
        "\n",
        "Actually before I do this, I just want to mention one other really funny thing dropout. I mentioned it was a master's thesis. Not only was it a master's thesis, one of the most influential papers of the last ten years.\n",
        "\n",
        "![](https://github.com/hiromis/notes/raw/master/lesson6/13.png?raw=true)\n",
        "\n",
        "It was rejected from the main neural nets conference what was then called NIPS, now called NeurIPS. I think it's very interesting because it's just a reminder that our academic community is generally extremely poor at recognizing which things are going to turn out to be important. Generally, people are looking for stuff that are in the field that they're working on and understand. So dropout kind of came out of left field. It's kind of hard to understand what's going on. So that's kind of interesting.\n",
        "\n",
        "It's a reminder that if you just follow as you develop beyond being just a practitioner into actually doing your own research, don't just focus on the stuff everybody's talking about. Focus on the stuff you think might be interesting. Because the stuff everybody's talking about generally turns out not to be very interesting. The community is very poor at recognizing high-impact papers when they come out.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4LdHNUlpliR3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Batch Normalization [44:28](https://youtu.be/U7c-nYXrKD4?t=2668)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "spqvom30M87T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Definition\n",
        "\n",
        "Batch normalization, on the other hand, was immediately recognized as high-impact. I definitely remember everybody talking about it in 2015 when it came out. That was because it's so obvious, they showed this picture:\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/14.png?raw=True)\n",
        "\n",
        "Showing the current then state of the art ImageNet model Inception. This is how long it took them to get a pretty good result, and then they tried the same thing with this new thing called batch norm, and they just did it way way way quickly. That was enough for pretty much everybody to go \"wow, this is interesting.\"\n",
        "\n",
        "Specifically they said **this thing is called batch normalization and it's accelerating training by reducing internal covariate shift**. So what is internal covariate shift? Well, it doesn't matter. Because this is one of those things where researchers came up with some intuition and some idea about this thing they wanted to try. They did it, it worked well, they then post hoc added on some mathematical analysis to try and claim why it worked. And it turned out they were totally wrong.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/blob/master/lesson6/15.png?raw=true)\n",
        "\n",
        "In the last two months, there's been two papers (so it took three years for people to really figure this out), in the last two months, there's been two papers that have shown batch normalization doesn't reduce covariate shift at all. And even if it did, that has nothing to do with why it works. I think that's an interesting insight, again, which is why we should be focusing on being practitioners and experimentalists and developing an intuition.\n",
        "\n",
        "**What batch norm does is what you see in this picture here in this paper. Here are steps or batches (x-axis). And here is loss (y-axis). The red line is what happens when you train without batch norm - very very bumpy. And here, the blue line is what happens when you train with batch norm - not very bumpy at all. What that means is, you can increase your learning rate with batch norm. Because these big bumps represent times that you're really at risk of your set of weights jumping off into some awful part of the weight space that it can never get out of again. So if it's less bumpy, then you can train at a higher learning rate. So that's actually what's going on.**\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson6/16.png?raw=true)\n",
        "\n",
        "This is the algorithm, and it's really simple. The algorithm is going to take a mini batch. So we have a mini batch, and remember this is a layer, so the thing coming into it is activations. Batch norm is a layer, and it's going to take in some activations. So the activations are what it's calling x_{1}, x_{2}, x_{3} and so forth.\n",
        "\n",
        "1. The first thing we do is we find the mean with those activations - sum divided by the count that is just the mean.\n",
        "2. The second thing we do is we find the variance of those activations - a difference squared divided by the mean is the variance.\n",
        "3. Then we normalize - the values minus the mean divided by the standard deviation is the normalized version. It turns out that bit is actually not that important. We used to think it was - it turns out it's not. The really important bit is the next bit.\n",
        "4. We take those values and we add a vector of biases (they call it beta here). We've seen that before. We've used a bias term before. So we're just going to add a bias term as per usual. Then we're going to use another thing that's a lot like a bias term, but rather than adding it, we're going to multiply by it. So there's these parameters gamma $\\gamma$ and beta $\\beta$ which are learnable parameters.\n",
        "\n",
        "Remember, in a neural net there's only two kinds of number; activations and parameters. These are parameters. They're things that are learnt with gradient descent. $\\beta$ is just a normal bias layer and $\\gamma$ is a multiplicative bias layer. Nobody calls it that, but that's all it is. It's just like bias, but we multiply rather than add. That's what batch norm is. That's what the layer does.\n",
        "\n",
        "So why is that able to achieve this fantastic result? I'm not sure anybody has exactly written this down before. If they have, I apologize for failing to site it because I haven't seen it. But let me explain. What's actually going on here. The value of our predictions y-hat is some function of our various weights. There could be millions of them (weight 1 million) and it's also a function, of course, of the inputs to our layer.\n",
        "\n",
        "$\\hat{y}=f(w_{1},w_{2}...w_{1000000},\\vec{x})$\n",
        "\n",
        "This function f is our neural net function whatever is going on in our neural net. Then our loss, let's say it's mean squared error, is just our actuals minus our predicted squared.\n",
        "\n",
        "L=$\\sum (y-\\hat{y})^{2}$\n",
        "\n",
        "Let's say we're trying to predict movie review outcomes, and they're between 1 and 5. And we've been trying to train our model and the activations at the very end currently between -1 and 1. So they're way off where they need to be. The scale is off, the mean is off, so what can we do? One thing we could do would be to try and come up with a new set of weights that cause the spread to increase, and cause the mean to increase as well. But that's going to be really hard to do, because remember all these weights interact in very intricate ways. We've got all those nonlinearities, and they all combine together. So to just move up, it's going to require navigating through this complex landscape and we use all these tricks like momentum and Adam and stuff like that to help us, but it still requires a lot of twiddling around to get there. So that's going to take a long time, and it's going to be bumpy.\n",
        "\n",
        "But what if we did this? What if we went times g plus b?\n",
        "\n",
        "$\\hat{y}=f(w_{1},w_{2}...w_{1000000},\\vec{x})\\times g+b$\n",
        "\n",
        "We added 2 more parameter vectors. Now it's really easy. In order to increase the scale, that number g has a direct gradient to increase the scale. To change the mean, that number b has a direct gradient to change the mean. There's no interactions or complexities, it's just straight up and down, straight in and out. That's what batch norm does. **Batch norm is basically making it easier for it to do this really important thing which is to shift the outputs up and down, and in and out**. And that's why we end up with these results.\n",
        "\n",
        "Those details, in some ways, don't matter terribly. **The really important thing to know is you definitely want to use it**. Or if not it, something like it. There's various other types of normalization around nowadays, but batch norm works great. The other main normalization type we use in fast.ai is something called weight norm which is much more just in the last few months' development."
      ]
    },
    {
      "metadata": {
        "id": "ydBPQ9wvNbrS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BatchNorm in FastAi lib\n",
        "\n",
        "[51:50](https://youtu.be/U7c-nYXrKD4?t=3110)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class TabularModel(nn.Module):\n",
        "    \"Basic model for tabular data.\"\n",
        "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
        "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False):\n",
        "        super().__init__()\n",
        "        ps = ifnone(ps, [0]*len(layers))\n",
        "        ps = listify(ps, layers)\n",
        "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(emb_drop)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
        "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
        "        sizes = self.get_sizes(layers, out_sz)\n",
        "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
        "        layers = []\n",
        "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
        "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
        "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def get_sizes(self, layers, out_sz):\n",
        "        return [self.n_emb + self.n_cont] + layers + [out_sz]\n",
        "\n",
        "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\n",
        "        if self.n_emb != 0:\n",
        "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.emb_drop(x)\n",
        "        if self.n_cont != 0:\n",
        "            x_cont = self.bn_cont(x_cont)\n",
        "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
        "        x = self.layers(x)\n",
        "        if self.y_range is not None:\n",
        "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
        "        return x\n",
        "```\n",
        "\n",
        "So that's batch norm and so what we do is we create a batch norm layer for every continuous variable. **n_cont is a number of continuous variables**. In fast.ai, n_something always means the count of that thing, cont always means continuous. Then here is where we use it. We grab our continuous variables and we throw them through a batch norm layer.\n",
        "\n",
        "So then over here you can see it in our model.\n",
        "\n",
        "![](https://github.com/hiromis/notes/blob/master/lesson6/18.png?raw=True)\n",
        "\n",
        "**One interesting thing is this momentum here. This is not momentum like in optimization, but this is momentum as in exponentially weighted moving average**. Specifically this mean and standard deviation (in batch norm algorithm), we don't actually use a different mean and standard deviation for every mini batch. If we did, it would vary so much that it be very hard to train. So instead, we take an exponentially weighted moving average of the mean and standard deviation.If you don't remember what I mean by that, look back at last week's lesson to remind yourself about exponentially weighted moving averages which we implemented in excel for the momentum and Adam gradient squared terms.\n",
        "\n",
        "[53:10](https://youtu.be/U7c-nYXrKD4?t=3190)\n",
        "\n",
        "**You can vary the amount of momentum in a batch norm layer by passing a different value to the constructor in PyTorch**. \n",
        "- If you use a smaller number, it means that the mean and standard deviation will vary less from mini batch to mini batch, and that will have less of a regularization effect. \n",
        "- A larger number will mean the variation will be greater for a mini batch to mini batch, that will have more of a regularization effect. \n",
        "\n",
        "So as well as this thing of training more nicely because it's parameterised better, this momentum term in the mean and standard deviation is the thing that adds this nice regularization piece.\n",
        "\n",
        "When you add batch norm, you should also be able to use a higher learning rate. So that's our model. So then you can go lr_find, you can have a look:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DbXOi4YJkFnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "1dfe3c6a-bc4b-411a-9ee2-59274518122f"
      },
      "cell_type": "code",
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYlyKgIRkSinIhARiXL/D4g0atR+HCBrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "juqyuLOxO8_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5e946c40-67d7-43ec-e5c1-b524b03f0e34"
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5,1e-3,wd=0.2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>exp_rmspe</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.023359</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.140823</td>\n",
              "      <td>02:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.018624</td>\n",
              "      <td>0.020258</td>\n",
              "      <td>0.150189</td>\n",
              "      <td>02:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.016661</td>\n",
              "      <td>0.016233</td>\n",
              "      <td>0.124304</td>\n",
              "      <td>02:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.012357</td>\n",
              "      <td>0.014717</td>\n",
              "      <td>0.114654</td>\n",
              "      <td>02:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.010356</td>\n",
              "      <td>0.012568</td>\n",
              "      <td>0.107831</td>\n",
              "      <td>02:23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "zLUv1AAfPVF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.save('1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yORwlYrkQCs1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "847f947f-8320-414e-cf75-d082092f4943"
      },
      "cell_type": "code",
      "source": [
        "learn.recorder.plot_losses(skip_start=10000)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEKCAYAAAAvlUMdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIIgCIKy8P+hWThRaSZnjQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bZUTzUpBQF3l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learn.load('1');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_l20fzVQKwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8b721153-d1e6-42c0-c2fb-d30939746c0d"
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5,3e-4)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>exp_rmspe</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.013366</td>\n",
              "      <td>0.016608</td>\n",
              "      <td>0.118101</td>\n",
              "      <td>02:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.012359</td>\n",
              "      <td>0.013664</td>\n",
              "      <td>0.116756</td>\n",
              "      <td>02:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.010961</td>\n",
              "      <td>0.017186</td>\n",
              "      <td>0.113797</td>\n",
              "      <td>02:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.010064</td>\n",
              "      <td>0.027363</td>\n",
              "      <td>0.113424</td>\n",
              "      <td>02:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.008913</td>\n",
              "      <td>0.015537</td>\n",
              "      <td>0.113042</td>\n",
              "      <td>02:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gX5UURaXQQdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b44cc83c-71f2-4202-c51f-b0e2d62ea190"
      },
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(5,3e-4)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>exp_rmspe</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.011376</td>\n",
              "      <td>0.015020</td>\n",
              "      <td>0.111256</td>\n",
              "      <td>02:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.011680</td>\n",
              "      <td>0.016675</td>\n",
              "      <td>0.119430</td>\n",
              "      <td>02:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.010192</td>\n",
              "      <td>0.027206</td>\n",
              "      <td>0.117143</td>\n",
              "      <td>02:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.012472</td>\n",
              "      <td>0.014693</td>\n",
              "      <td>0.117119</td>\n",
              "      <td>02:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.010241</td>\n",
              "      <td>0.014042</td>\n",
              "      <td>0.113780</td>\n",
              "      <td>02:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "XHV4EeZRQugy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(10th place in the competition was 0.108)\n",
        "\n",
        "We end up 0.103. 10th place in the competition was 0.108, so it's looking good. Again, take it with a slight grain of salt because what you actually need to do is use the real training set and submit it to Kaggle, but you can see we're very much amongst the cutting-edge of models at least as of 2015. As I say, they haven't really been any architectural improvements since then. There wasn't batch norm when this was around, so the fact we added batch norm means that we should get better results and certainly more quickly. If I remember correctly, in their model, they had to train at a lower learning rate for quite a lot longer. As you can see, this is less than 45 minutes of training. So that's nice and fast.\n",
        "\n",
        "**Question**: In what proportion would you use dropout vs. other regularization errors, like, weight decay, L2 norms, etc.? [54:49]\n",
        "\n",
        "- So remember that L2 regularization and weight decay are kind of two ways of doing the same thing? We should always use the weight decay version, not the L2 regularization version. So there's weight decay. There's batch norm which kind of has a regularizing effect. There's data augmentation which we'll see soon, and there's dropout. So batch norm, we pretty much always want. So that's easy. Data augmentation, we'll see in a moment. So then it's really between dropout versus weight decay. I have no idea. I don't think I've seen anybody to provide a compelling study of how to combine those two things. Can you always use one instead of the other? Why? Why not? I don't think anybody has figured that out. I think in practice, it seems that you generally want a bit of both. You pretty much always want some weight decay, but you often also want a bit of dropout. But honestly, I don't know why. I've not seen anybody really explain why or how to decide. So this is one of these things you have to try out and kind of get a feel for what tends to work for your kinds of problems. I think the defaults that we provide in most of our learners should work pretty well in most situations. But yeah, definitely play around with it."
      ]
    }
  ]
}