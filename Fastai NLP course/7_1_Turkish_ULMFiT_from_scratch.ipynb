{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_1_Turkish_ULMFiT_from_scratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/Fastai%20NLP%20course/7_1_Turkish_ULMFiT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqdoM4ZD1Fjl",
        "colab_type": "text"
      },
      "source": [
        "# Turkish ULMFiT from scratch\n",
        "\n",
        "[Video 10](https://youtu.be/MDX_x6rKXAs?t=4036)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp9RtlPi0g__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "from fastai import *\n",
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL_Iv5PP1Drg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=48\n",
        "#torch.cuda.set_device(2)\n",
        "data_path = Config.data_path()\n",
        "\n",
        "lang = 'tr'\n",
        "name = f'{lang}wiki'\n",
        "path = data_path/name\n",
        "path.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1IXyCdD8SMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mdl_path =path/'models'\n",
        "mdl_path.mkdir(exist_ok=True)\n",
        "lm_fns =[mdl_path/f'{lang}_wt',mdl_path/f'{lang}_wt_vocab']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbAOjmEd8Bx7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Some words in Chinese `而获得诺贝尔文学奖` there are no spaces so we can't rely when dealing with languages like Turkish or Chinese on using a space to tokenize our corpus. This is a big problem or really tricky . It's not as easy as you might imagine . Chinese is such gramatically interesting language that it's not even clear sometimes where words start and end. For example in Chinese you will insert a character in the middle of a word to give it some different meaning. Or you'll insert a character at the end of the word something called a result of the complement which means  you create a new word which tells you about the outcome of some other word. It's not a simple case of just saying let's just find the words.\n",
        "\n",
        "In recent times, a really cool solution was developed called **sentence peace**. It actually based on something that goes back even further called **byte-pair-encoding**. What **sentence peace** and **byte-pair-encoding** do is that the segment things into what are called  **subword units**. A **subword units** is a sequence of letters that appears frequently in a corpus. So frequently that you tokenize it into sub sequences .\n",
        "\n",
        "For example In Turkish , after we use **subword encoding** we end up with something like this \n",
        "\n",
        "\n",
        "```\n",
        "▁ele ▁geçirerek ▁politik ▁bir ▁özerklik ▁verdiğini ▁belirtmek le ▁birlikte ▁xxmaj ▁tibet ' in ▁yalnızca ▁1913 - 1950 ▁yılları ▁arasında ▁xxmaj ▁çin ' in ▁politik ▁nüfuz undan ▁çıktığını , ▁bölgenin ▁tarihi ▁olarak ▁xxmaj ▁çin ' e ▁ait ▁olduğunu ▁düşünmektedir . ▁xxmaj ▁tibet ' in ▁kendi ▁kültür ▁ve ▁zenginlik lerinin ▁\" kültürel ▁bir ▁soykırım \" a ▁tabi ▁tutulduğu ▁da ▁iddialar ▁arasındadır . ▁xxmaj ▁çin ▁hükümeti ▁ise ▁bu ▁\" kültürel ▁soykırım \" ▁iddia\n",
        "```\n",
        "\n",
        "These big underscore represents spaces and spaces represent token boundaries . You can see here each of the time, we have each token is a single word `▁politik` but sometimes we get things like `▁zenginlik lerinin`where being turned into two tokens, eventually split the word in the middle.\n",
        "\n",
        "The question is like how would you start with something like this \n",
        "\n",
        "    `而获得诺贝尔文学奖` - And won the Nobel Prize in Literature\n",
        "    or `evlerinizdenmiş` - `(he/she/it) was (apparently/said to be) from your houses`\n",
        "    \n",
        "and turn it into something like `▁zenginlik lerinin`. And these answer is at the high level what you basically do is you start out by looking through your corpus , you find ( using **byte-pair-encoding**) which two characters appear next to each other the most frequently. And you take that pair and you pull it out , and that's a token. \n",
        "\n",
        "So for example, if you're doing English, you may find T and H occur next to each other alot and you say TH is now a token. And then you repeat it, and you look for again two characters that often appear next to each other , now  you can treat TH as one character. Down the track you'll find TH and E often appear next to each other , that's now a token. So you keep doing that until eventually you have some set number of unique tokens. That's called your vocab. \n",
        "\n",
        "So with **sentence piece**, when you call sentence piece you can actually tell it how big a vocab do you want . And so by default to a vocab of size 30,000. So this here has been tokenized into Turkish subword units using a size of 30,000. Sentence piece actually goes a bit furthur than bye-pair-encoding, it actually creates a neural network language model at a character model. And finds combinations of characters that are most likely to appear together based on the language model but it's the same basic idea.\n",
        "\n",
        "So this idea of using subword units , it's not very well studied but it's absolutely necessary and super powerful. Not only for Turkish, Chinese, Japanese, but also for things like medical texts. Because actually those big long chemical names in the scientific and medical literature contain well-defined subworks units that are frequently reused. So you don't need separate vocab for everyone. The problem with having a separate vocab for everyone is none of those single words appears that often , so you really want to get the sense of meaning of the underlying subword units. \n",
        "\n",
        "So if we want to Turkish from scratch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsqYIchM8u_U",
        "colab_type": "text"
      },
      "source": [
        "## Turkish wikipedia model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPODOWJt88oh",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title from nlputils import get_wiki, split_wiki\n",
        "from fastai.basics import *\n",
        "import re\n",
        "\n",
        "\n",
        "def get_wiki(path,lang):\n",
        "    name = f'{lang}wiki'\n",
        "    if (path/name).exists():\n",
        "        print(f\"{path/name} already exists; not downloading\")\n",
        "        return\n",
        "\n",
        "    xml_fn = f\"{lang}wiki-latest-pages-articles.xml\"\n",
        "    zip_fn = f\"{xml_fn}.bz2\"\n",
        "\n",
        "    if not (path/xml_fn).exists():\n",
        "        print(\"downloading...\")\n",
        "        download_url(f'https://dumps.wikimedia.org/{name}/latest/{zip_fn}', path/zip_fn)\n",
        "        print(\"unzipping...\")\n",
        "        bunzip(path/zip_fn)\n",
        "\n",
        "    with working_directory(path):\n",
        "        if not (path/'wikiextractor').exists(): os.system('git clone https://github.com/attardi/wikiextractor.git')\n",
        "        print(\"extracting...\")\n",
        "        os.system(\"python wikiextractor/WikiExtractor.py --processes 4 --no_templates \" +\n",
        "            f\"--min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {xml_fn}\")\n",
        "    shutil.move(str(path/'text/AA/wiki_00'), str(path/name))\n",
        "    shutil.rmtree(path/'text')\n",
        "\n",
        "\n",
        "def split_wiki(path,lang):\n",
        "    dest = path/'docs'\n",
        "    name = f'{lang}wiki'\n",
        "    if dest.exists():\n",
        "        print(f\"{dest} already exists; not splitting\")\n",
        "        return dest\n",
        "\n",
        "    dest.mkdir(exist_ok=True, parents=True)\n",
        "    title_re = re.compile(rf'<doc id=\"\\d+\" url=\"https://{lang}.wikipedia.org/wiki\\?curid=\\d+\" title=\"([^\"]+)\">')\n",
        "    lines = (path/name).open()\n",
        "    f=None\n",
        "\n",
        "    for i,l in enumerate(lines):\n",
        "        if i%100000 == 0: print(i)\n",
        "        if l.startswith('<doc id=\"'):\n",
        "            title = title_re.findall(l)[0].replace('/','_')\n",
        "            if len(title)>150: continue\n",
        "            if f: f.close()\n",
        "            f = (dest/f'{title}.txt').open('w')\n",
        "        else: f.write(l)\n",
        "    f.close()\n",
        "    return dest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTHamgoW8-7z",
        "colab_type": "code",
        "outputId": "ebbdf8fa-79e7-4a77-89cb-713deecc709b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "get_wiki(path,lang)\n",
        "!head -n4 {path}/{name}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.fastai/data/trwiki/trwiki already exists; not downloading\n",
            "<doc id=\"10\" url=\"https://tr.wikipedia.org/wiki?curid=10\" title=\"Cengiz Han\">\n",
            "Cengiz Han\n",
            "\n",
            "Cengiz Han (\"Cenghis Khan\", \"Çinggis Haan\" ya da doğum adıyla Temuçin (anlamı: demirci), Moğolca: \"Чингис Хаан\" ya da \"Tengiz\" (anlamı: deniz), ; d. 1162 – ö. 18 Ağustos 1227), Moğol komutan, hükümdar ve Moğol İmparatorluğu'nun kurucusudur. Cengiz Han, 13. Yüzyılın başında Orta Asya'daki tüm göçebe bozkır kavimlerini birleştirerek bir ulus haline getirdi ve o ulusu \"Moğol\" siyasi kimliği çatısı altında topladı. Dünya tarihinin en büyük askeri dehalarından biri olarak kabul edilen Cengiz Han, hükümdarlığı döneminde 1206-1227 arasında Kuzey Çin'deki Batı Xia ve Jin Hanedanı, Türkistan'daki Kara Hıtay, Maveraünnehir, Harezm, Horasan ve İran'daki Harzemşahlar ile Kafkasya'da Gürcüler, Deşt-i Kıpçak'taki Rus Knezlikleri ve Kıpçaklar ile İdil Bulgarları üzerine gerçekleştirilen seferler sonucunda Pasifik Okyanusu'ndan Hazar Denizi’ne ve Karadeniz'in kuzeyine kadar uzanan bir imparatorluk kurdu.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTSvfwql9GlI",
        "colab_type": "code",
        "outputId": "a7217a92-a743-41a4-8089-8b956e648c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dest = split_wiki(path,lang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.fastai/data/trwiki/docs already exists; not splitting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fIN6p6G9Ovc",
        "colab_type": "text"
      },
      "source": [
        "Turkish is an [Agglutinative_language](https://en.wikipedia.org/wiki/Agglutinative_language) so it needs special care!\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/turkish.jpg?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9wm0nrI-MBF",
        "colab_type": "text"
      },
      "source": [
        "Here we got one extra step which we added a **processor**. So by default, fastai uses the Spacy tokenizer to tokenize the text. If you want to use something else, in our case, we want to use **sentence piece**, you can replace the processor with **SPprocessor** - **Sentence Piece processor**.\n",
        "\n",
        "If we do this, this say don't tokenize on space boundaries but instead learn a model from the text please. So now, when you go **data.show_batch()**, you can see there is the sentence piece tokenized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgP4UWW6DsUY",
        "colab_type": "code",
        "outputId": "34953f6f-daba-46fd-f652-ae849f6c4545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.82)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQhtqw7r9NfB",
        "colab_type": "code",
        "outputId": "9200d86f-7898-4724-9057-c5af60d7db68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = (TextList.from_folder(dest,processor=[OpenFileProcessor(),SPProcessor()])\n",
        "                .split_by_rand_pct(0.1,seed=42)\n",
        "                .label_for_lm()\n",
        "                .databunch(bs=bs,num_workers=1))\n",
        "\n",
        "data.save(f'{lang}_databunch')\n",
        "len(data.vocab.itos),len(data.train_ds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 45219)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvMK64xA921L",
        "colab_type": "code",
        "outputId": "b0a3698c-0098-4a1f-f48b-ac03cadfc4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "data.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>di , ▁xxmaj ▁v yat ka ▁xxmaj ▁gu ber ni ya sı , ▁xxmaj ▁rus ▁xxmaj ▁i ̇ mparatorluğu ▁ - ▁19 ▁xxmaj ▁mart ▁1955 ; ▁xxmaj ▁moskova , ▁xxup ▁sscb ), ▁xxmaj ▁kızıl ▁xxmaj ▁ordu ' nun ▁komutanlarından ▁biri , ▁xxmaj ▁sovyetler ▁xxmaj ▁birliği ▁xxmaj ▁mareşal i , ▁xxmaj ▁sovyetler ▁xxmaj ▁birliği ▁xxmaj ▁komünist ▁xxmaj ▁partisi ▁xxmaj ▁merkez ▁xxmaj ▁komitesi ▁aday ▁üyesi , ▁xxmaj ▁yüksek ▁xxmaj ▁sovyet ▁üyesi ▁ve</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>devlet in ▁tüm ▁bu ▁ayaklanma lardan ▁edindiği ▁ders , ▁ayaklanan ▁vatan ▁hain lerinin , ▁tüm ▁soyu ▁so pu yla ▁yok ed ilmesi ▁gerektiği , ▁geriye ▁\" de de m ▁dede m \" ▁diye cek ▁tek ▁bir ▁hain ▁döl ünün ▁bile ▁sağ ▁bırakılma ması ▁gerektiğini \" ▁savunmaktadır . ▁xxmaj ▁fırat , ▁xxmaj ▁kürt leri ▁\" ata sı ▁belli ▁olmayan \", ▁xxmaj ▁za gro s lar ' ın ▁\" kültürel ▁çukur unda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>▁vi . ▁xxmaj ▁ philippe ▁ te fe cilik ten ▁vazgeçerek ▁kendi ▁bölgesindeki ▁xxmaj ▁floransa lı ▁tüccarların ▁ve ▁bank er lerin ▁yüksek ▁miktarda ki ▁para larını ▁ şan ta j ▁yaparak ▁almıştır . ▁14. ▁yüzyılın ▁40 ' lı ▁yıllarındaki ▁xxmaj ▁ortaçağ ’ da ▁ve ba ▁ve ▁açlık ▁çok ▁büyük ▁zarar lara ▁yol ▁açmıştır ▁ve ▁kısmen ▁başarı ▁sağ lanamayan ▁son ▁savaşlar ▁ve ▁ekonomi deki ▁tatmin ▁edici ▁olmayan ▁durum ▁nedeniyle ▁xxmaj ▁grand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>▁uzayda ▁yapılmıştır . i ̇ mparatorluk ▁xxmaj ▁ordusu ▁her ▁savaşta ▁yenilgi ▁alıp ▁yıkılmıştır . bu ▁savaşın ▁kahramanları ▁olan ▁xxmaj ▁luke ▁skywalker , le ia ▁organ a , han ▁solo , land o ▁xxmaj ▁cal ris si an ▁ve ▁son ▁anda ▁taraf ▁değiştiren ▁xxmaj ▁darth ▁xxmaj ▁vader ▁yani ▁xxmaj ▁anakin ▁skywalker ' dır . en dor ▁xxmaj ▁savaşı ' nda ▁enerji ▁kalkanı ▁yok ▁edilince ▁xxmaj ▁i ̇ mparatorluk ▁ilk ▁darbesi ni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>. ▁xxmaj ▁koleksiyoncu , ▁taran ması ▁gereken ▁ uri ' lerin ▁listesini ▁alır ▁ve ▁ uri ' yi ▁barındıran ▁mağaza ya ▁erişmek ▁için ▁uygun ▁protokol ▁iş leyici sini ▁ve ▁sonra ▁da ▁( me ta ▁verileri ▁ ayık lamak ▁için ) ▁uygun ▁özellik ▁iş leyici sini ▁ve ▁belge ▁metnin i ▁ ayık lamak ▁için ▁i fil ter ' ı ▁çağırır . ▁xxmaj ▁farklı ▁indeks ler ▁farklı ▁koşu lar ▁sırasında ▁oluşturulur ;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpGIyLyi_-cy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = language_model_learner(data,AWD_LSTM,drop_mult=0.1,wd=0.1,pretrained=False).to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nuP1i8VALap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 3e-3\n",
        "lr *= bs/48  # Scale learning rate by batch size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D7tAdrQAO4e",
        "colab_type": "code",
        "outputId": "775637fe-36cf-43a0-8ac6-2ee2f7ab2782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(1, lr, moms=(0.8,0.7)) # use n of epochs 10 instead"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.221147</td>\n",
              "      <td>4.158035</td>\n",
              "      <td>0.330393</td>\n",
              "      <td>33:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQX33Je_ECVS",
        "colab_type": "text"
      },
      "source": [
        "We have 38% accuracy at predicting the next sub word in Turkish Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs5QrS5FATuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn.to_fp32().save(lm_fns[0], with_opt=False)\n",
        "learn.data.vocab.save(lm_fns[1].with_suffix('.pkl'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z94dJUAFEVOy",
        "colab_type": "text"
      },
      "source": [
        "## Turkish sentiment analysis\n",
        "\n",
        "https://www.win.tue.nl/~mpechen/projects/smm/\n",
        "\n",
        "[1:17](https://youtu.be/MDX_x6rKXAs?t=4639)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82WVW0OlEYWw",
        "colab_type": "text"
      },
      "source": [
        "### Language model\n",
        "\n",
        "In the dataset, we have two files tr_polarity.pos (positive reviews) and tr_poplarity.neg (negative reviews).\n",
        "\n",
        "If we use `pos = (path_clas/'tr_polarity.pos').open().readlines()` -> it got an error that it was unable to read the file using utf-8. Here the next thing that you're going to come across all the time as an NLP practitioner. The way that letters are stored on disk until recently was ill-defined . We use a character set called ASCII that had a unique mapping from the numbers I  think 0 to 127 and letters of the alphabet and punctuation and so forth. Then when folks in , the A in ASCII state stands for American. It's just designed for Americans . When folks in France decided that they wanted to put things on computers as well they changed some of the numbers to represent different characters. So if you were put in exactly the same on a French computer versus an American computer you'll see something different. \n",
        "\n",
        "This is particularly crazy in Japan, for example, because Japan has 4000 characters which obviously you can't fit in 127 . So they invented their own encoding using more than 127 numbers . There is something called Unicode which is consisted of millions of characters including emojis and they keep adding new emojis every year . \n",
        "\n",
        "The problem is that the dataset that i was looking at and this happens quite frequently , it was from 2013 where Unicode was around but not everybody using it. It was not saved in Unicode. So there's actually no correct way to open a file that's saved not in Unicode, you have to guess. So I googled for Turkish language encoding and I found a page that said quite often in Turkey people tend to use **iso-8859-9** encoding, except when they don'tk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFmk0ZzSi6fe",
        "colab_type": "code",
        "outputId": "78ebd24d-6911-40a9-cc22-1a0cf6351df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://www.win.tue.nl/~mpechen/projects/smm/Turkish_Movie_Sentiment.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-11 05:51:33--  https://www.win.tue.nl/~mpechen/projects/smm/Turkish_Movie_Sentiment.zip\n",
            "Resolving www.win.tue.nl (www.win.tue.nl)... 131.155.11.13\n",
            "Connecting to www.win.tue.nl (www.win.tue.nl)|131.155.11.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 533335 (521K) [application/zip]\n",
            "Saving to: ‘Turkish_Movie_Sentiment.zip.1’\n",
            "\n",
            "Turkish_Movie_Senti 100%[===================>] 520.83K   371KB/s    in 1.4s    \n",
            "\n",
            "2019-08-11 05:51:35 (371 KB/s) - ‘Turkish_Movie_Sentiment.zip.1’ saved [533335/533335]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBVczVWJk7KB",
        "colab_type": "code",
        "outputId": "79c45959-c23b-4b5f-fa84-c030172110ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/trwiki/log'),\n",
              " PosixPath('/root/.fastai/data/trwiki/wikiextractor'),\n",
              " PosixPath('/root/.fastai/data/trwiki/models'),\n",
              " PosixPath('/root/.fastai/data/trwiki/docs'),\n",
              " PosixPath('/root/.fastai/data/trwiki/trwiki'),\n",
              " PosixPath('/root/.fastai/data/trwiki/trwiki-latest-pages-articles.xml'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies'),\n",
              " PosixPath('/root/.fastai/data/trwiki/trwiki-latest-pages-articles.xml.bz2')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUoCjJDUWFm9",
        "colab_type": "code",
        "outputId": "0ac21267-6ea7-4d44-c0a2-85f061a06938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "path_clas = path/'movies'\n",
        "path_clas.mkdir(exist_ok=True)\n",
        "!unzip /content/Turkish_Movie_Sentiment.zip -d {path_clas}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Turkish_Movie_Sentiment.zip\n",
            "replace /root/.fastai/data/trwiki/movies/tr_polarity.neg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-68dbe56e30cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_clas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'movies'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_clas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip /content/Turkish_Movie_Sentiment.zip -d {path_clas}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m   \u001b[0mhide_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_remove_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhide_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngP-g-A9kBKF",
        "colab_type": "code",
        "outputId": "0eba83fc-f6f3-4bd4-ee4a-d2cea3407d89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "path_clas.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/trwiki/movies/tr_polarity.neg'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/models'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/tr_clas_databunch'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/tr_polarity.pos')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWtNnWyWPJC",
        "colab_type": "code",
        "outputId": "a14202a4-f8a9-49db-c1aa-a68bac7e826d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "pos = (path_clas/'tr_polarity.pos').open(encoding='iso-8859-9').readlines()\n",
        "pos_df = pd.DataFrame({'text':pos})\n",
        "pos_df['pos']=1\n",
        "pos_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gerçekten harika bir yapim birçok kez izledim ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>her izledigimde hayranlik duydugum gerçek klas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gerçekten tarihi savas filmleri arasinda tarti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aldigi ödülleri sonuna dek hak eden muhtesem b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>özgürlük denilince aklima gelen ilk film.bir b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  pos\n",
              "0  gerçekten harika bir yapim birçok kez izledim ...    1\n",
              "1  her izledigimde hayranlik duydugum gerçek klas...    1\n",
              "2  gerçekten tarihi savas filmleri arasinda tarti...    1\n",
              "3  aldigi ödülleri sonuna dek hak eden muhtesem b...    1\n",
              "4  özgürlük denilince aklima gelen ilk film.bir b...    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT-rPuroWoV6",
        "colab_type": "code",
        "outputId": "410c7d10-d101-4160-cd4a-3178654afa93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "neg = (path_clas/'tr_polarity.neg').open(encoding='iso-8859-9').readlines()\n",
        "neg_df = pd.DataFrame({'text':neg})\n",
        "neg_df['pos']=0\n",
        "neg_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>giseye oynayan bir film.mel gibson'in oyunculu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bircok yonden sahip olduklari zayifliklari pop...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1995 ten bu yana bu tür filmler artti , o zama...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mel gibson tam bir ingiliz düsmani her filmind...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>milliyetçi bir film tavsiye etmiyorum.... \\n</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  pos\n",
              "0  giseye oynayan bir film.mel gibson'in oyunculu...    0\n",
              "1  bircok yonden sahip olduklari zayifliklari pop...    0\n",
              "2  1995 ten bu yana bu tür filmler artti , o zama...    0\n",
              "3  mel gibson tam bir ingiliz düsmani her filmind...    0\n",
              "4       milliyetçi bir film tavsiye etmiyorum.... \\n    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm8tN9e3W9sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([pos_df,neg_df],sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrnE8rL8Xr1l",
        "colab_type": "text"
      },
      "source": [
        "Here we need to make sure that we use the same sentence piece of vocabulary and model which we used for the Turkish Wikipedia model , otherwise it's going to tokenize in different way and it won't make any sense. **SPProcessor.load(dest)** which passed in the path to your databunch from your pretrained Wikipedia file then that will load up the sentence piece vocab. When we do **SPProcessor()** above, it automatically saved the vocab for us. \n",
        "\n",
        "With this approach, we can make sure our data is tokenized in the same way as before.\n",
        "\n",
        "There has been dramatically changed in the last couple of years, almost nobody's familiar with everything's low hanging fruit, pretty much everything you apply and try it to, you'll probably get much beter results than everybody got before. There's lot of opportunities to create products which doesn't exist before. Any product currently use this langugage you'll probably be able to make it better and it's really all about taking advantage of transfer learning in this way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pPP_GxTXGmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = (TextList.from_df(df,path_clas,cols='text',processor=SPProcessor.load(dest))\n",
        "                   .split_by_rand_pct(0.1,seed=42)\n",
        "                   .label_for_lm()\n",
        "                   .databunch(bs=bs,num_workers=1))\n",
        "\n",
        "data_lm.save(f'{lang}_clas_databunch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqE2201BcDGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_lm = load_data(path_clas,f'{lang}_clas_databunch',bs=bs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFVsi1ZTck-w",
        "colab_type": "code",
        "outputId": "70f653ab-e421-407b-955a-a993c52e9d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data_lm.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>sa ▁bile ▁sinema ▁olgusu nun ▁en ▁üst ▁noktalar in dan . . ▁xxbos ▁gerçekten ▁tarihi ▁sava s ▁filmleri ▁ara si nda ▁tar tis ma siz ▁en ▁iyi si ▁ , ▁12 ▁ yi l ▁boyunca ▁ a caba ▁ikincisi ▁çek ir imi ▁diye ▁bekledi gi m ▁bir ▁film ▁ , bel ki ▁william ▁wallace ▁baba sinin ▁ölümünden ▁sonra ▁amca si ▁yani na ▁al mis ti ▁onu ▁ ye tis tir</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>ni ▁öldür se ler ▁o ▁kadar ▁ üzülme m özgürlük ▁xxrep ▁4 ▁! ▁ . ▁xxbos ▁bi ▁çok ▁aç i dan ▁kusur suz ▁bir ▁film ▁olsa da ▁benim ▁beklentileri mi ▁tam ▁kars i lama di . . . ama ▁kötü ▁film mi ▁kesinlikle ▁hay ir . . . özellikle ▁son ▁20 ▁da k ki kas in da ▁etkilenme k ▁elde ▁de gil . . . ama ▁film ▁bana ▁istedi gi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>, bi ▁gün i çinde ▁ na sil ▁fark li ▁yön lere ▁ çekil e bile ce ini ▁muh te sem ▁anlatan ▁bi ▁film ▁xxrep ▁4 ▁ . ▁xxbos ▁filmde ▁o ▁kadar ▁çok ▁ve ▁de ▁ hos ▁ay rin ti ▁var ▁ki ▁tekrar ▁tekrar ▁izlenebil cek ▁ender ▁filmlerden . ba mba ska ▁bir ▁film ▁konuyu ▁sunar ken ▁seçti gi ▁yöntem ▁itibar iyle . . . ▁xxbos ▁tam am ▁bi ▁sey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>ligi m ▁bunun ▁üstesinde n ▁geldi . ben ce ▁yine ▁de ▁izlenme ye ▁de ger ▁olan ▁bir ▁film . ay ri ca ▁filmin ▁yap ti gi ▁gönderme ler ▁var . bu nu ▁yak layan lar ▁zaten ▁biliyordu r . . ▁xxbos ▁ ' benim ▁ a dim ▁robert ▁neville . ▁new ▁york ▁ se hri nde ▁hayatta ▁kalan ▁biri yim . ▁sesi mi ▁duyan ▁biri ▁varsa ▁herhangi ▁biri . ▁</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>▁for rest , ▁run ▁xxrep ▁4 ▁ . ▁xxbos ▁gerçekten ▁bu da ▁bi ▁tom ▁han ks ▁ kla si gi , . ▁xxbos ▁ya ▁bu ▁filme ▁9 ▁puan ▁veren ▁adam ▁utan ma li ▁ya w ▁film ▁süper . ▁xxbos ▁be gen me gen ler ▁ya ▁sonuna ▁kadar ▁izleme mis ▁ya ▁da ▁geyik ▁ortam in da ▁geyik ▁yaparak ▁izle mistir . ▁hala ▁da ▁be gen meyen ▁varsa ▁hakikat en ▁ya ▁kendinden</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRHzs9cycmxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm = language_model_learner(data_lm,AWD_LSTM,pretrained_fnames=lm_fns,drop_mult=1.0,wd=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIs7xlshc1ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3\n",
        "lr *= bs/48 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYm6bXJGc7xu",
        "colab_type": "code",
        "outputId": "41f5dc11-8740-4a7f-98e1-646264605de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "learn_lm.fit_one_cycle(5,lr*10,moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.087432</td>\n",
              "      <td>4.409014</td>\n",
              "      <td>0.282120</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.415067</td>\n",
              "      <td>4.180242</td>\n",
              "      <td>0.308997</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.154958</td>\n",
              "      <td>4.093359</td>\n",
              "      <td>0.319620</td>\n",
              "      <td>00:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.963784</td>\n",
              "      <td>4.041077</td>\n",
              "      <td>0.326145</td>\n",
              "      <td>00:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.842597</td>\n",
              "      <td>4.029410</td>\n",
              "      <td>0.327106</td>\n",
              "      <td>00:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e066TFxcdWTW",
        "colab_type": "code",
        "outputId": "dfa22ecd-4fe2-4d2c-ac41-773bf5905857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "learn_lm.unfreeze()\n",
        "learn_lm.fit_one_cycle(5,slice(lr/10,lr*10),moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.847214</td>\n",
              "      <td>4.004792</td>\n",
              "      <td>0.329625</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.835030</td>\n",
              "      <td>3.966111</td>\n",
              "      <td>0.335440</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.650264</td>\n",
              "      <td>3.893359</td>\n",
              "      <td>0.346062</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.427696</td>\n",
              "      <td>3.865988</td>\n",
              "      <td>0.352953</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.264168</td>\n",
              "      <td>3.871879</td>\n",
              "      <td>0.353846</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo6srS5Edj7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_lm.save(f'{lang}fine_tuned')\n",
        "learn_lm.save_encoder(f'{lang}fine_tuned_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ56JwKOdxPD",
        "colab_type": "text"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmtl-owxrrMr",
        "colab_type": "code",
        "outputId": "d3f9c0c7-5391-48ce-b428-cbc30e0a4332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "path_clas.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/trwiki/movies/tr_polarity.neg'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/models'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/tr_clas_databunch'),\n",
              " PosixPath('/root/.fastai/data/trwiki/movies/tr_polarity.pos')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU5ZM2wPdwLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_clas = (TextList.from_df(df,path_clas,cols='text',processor=SPProcessor.load(dest))\n",
        "                     .split_by_rand_pct(0.1,seed=42)\n",
        "                     .label_from_df(cols='pos')\n",
        "                     .databunch(bs=bs,num_workers=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8u9kKIpemKg",
        "colab_type": "code",
        "outputId": "688de8ac-e242-445f-b022-0bf9e0c4224e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "learn_c = text_classifier_learner(path_clas,AWD_LSTM,drop_mult=0.5,pretrained=False,wd=0.1).to_fp16()\n",
        "learn_c.load_encoder(f'{lang}fine_tune_enc')\n",
        "learn_c.freeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-540b85b225d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_classifier_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_clas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{lang}fine_tune_enc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearn_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mtext_classifier_learner\u001b[0;34m(data, arch, bptt, max_len, config, pretrained, drop_mult, lin_ftrs, ps, **learn_kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                             ps:Collection[float]=None, **learn_kwargs) -> 'TextClassifierLearner':\n\u001b[1;32m    290\u001b[0m     \u001b[0;34m\"Create a `Learner` with a text classifier from `data` and `arch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     model = get_text_classifier(arch, len(data.vocab.itos), data.c, bptt=bptt, max_len=max_len,\n\u001b[0m\u001b[1;32m    292\u001b[0m                                 config=config, drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n\u001b[1;32m    293\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_model_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW0ygKI4ez9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=2e-2\n",
        "lr *= bs/48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXuqstIVe3fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.fit_one_cycle(2,lr,moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXM4Uu6Je_1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.fit_one_cycle(2,lr,moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrxYZX8xfDYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.freeze_to(-2)\n",
        "learn_c.fit_one_cycle(2,slice(lr/(2.6**4),lr),moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d_mpK2AfYzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.freeze_to(-3)\n",
        "learn_c.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQIGPS26ffjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.unfreeze()\n",
        "learn_c.fit_one_cycle(4, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paX8-9lTfi56",
        "colab_type": "text"
      },
      "source": [
        "Accuracy in Gezici (2018), Sentiment Analysis in Turkish is: 75.16%. With our result, we beat the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSZ1RLV1fhR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_c.save(f'{lang}clas')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5BSfUDYfnB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}