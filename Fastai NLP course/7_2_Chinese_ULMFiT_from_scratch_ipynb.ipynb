{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_2_Chinese_ULMFiT_from_scratch.ipynb.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/Fastai%20NLP%20course/7_2_Chinese_ULMFiT_from_scratch_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v48y7kqJ7jUx",
        "colab_type": "text"
      },
      "source": [
        "# Chinese\n",
        "\n",
        "[Video 10](https://youtu.be/MDX_x6rKXAs?t=3567)\n",
        "\n",
        "Chinese in Wikipedia is zh.\n",
        "\n",
        "1. **It has a particular interesting and important feature you have to deal with which is there are multiple ways to write the same sentence in Chinese.**\n",
        "\n",
        "The name of these two ways are very depending on who you ask on the mainland, they're known as simplified characters or traditional characters. \n",
        "\n",
        "In Chinese there is no exact mapping from simplified to traditional . There is actually multiple traditional characters can map to the same simplified characters. What the Chinese Wikipedia editors decided to do now . they had two totally separate encyclopedias , they merged them and then they decided basically \" let's write it largely in traditional characters and create some scripts  \".\n",
        "\n",
        "When you download the Chinese Wikipedia dump you'll find that they're in traditional characters.\n",
        "\n",
        "To create a simplifed version, there is a program called **opencc** which will take a Chinese language document that's in traditional and convert it to simplified or vice versa automatically . This program take a very very very long time to convert . So we add **parallel** - it will run any program over multiple processes . Here how you can convert chinese traditional characters to Chinese simplied characters automatically and quickly . It said look for all of the text files and pipe that to parallel , what command will run **-I%**. It will replace the **%** with ***.txt**\n",
        "\n",
        "The best thing to do NLP on a non-Englisht language is to find a partner who speak that langugae :)\n",
        "\n",
        "2. **Chinese has another interesting feature**\n",
        "\n",
        "Some words in Chinese `而获得诺贝尔文学奖` there are no spaces so we can't rely when dealing with languages like Turkish or Chinese on using a space to tokenize our corpus. This is a big problem or really tricky . It's not as easy as you might imagine . Chinese is such gramatically interesting language that it's not even clear sometimes where words start and end. For example in Chinese you will insert a character in the middle of a word to give it some different meaning. Or you'll insert a character at the end of the word something called a result of the complement which means  you create a new word which tells you about the outcome of some other word. It's not a simple case of just saying let's just find the words.\n",
        "\n",
        "In recent times, a really cool solution was developed called **sentence peace**. It actually based on something that goes back even further called **byte-pair-encoding**. What **sentence peace** and **byte-pair-encoding** do is that the segment things into what are called  **subword units**. A **subword units** is a sequence of letters that appears frequently in a corpus. So frequently that you tokenize it into sub sequences .\n",
        "\n",
        "For example In Turkish , after we use **subword encoding** we end up with something like this \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "▁ele ▁geçirerek ▁politik ▁bir ▁özerklik ▁verdiğini ▁belirtmek le ▁birlikte ▁xxmaj ▁tibet ' in ▁yalnızca ▁1913 - 1950 ▁yılları ▁arasında ▁xxmaj ▁çin ' in ▁politik ▁nüfuz undan ▁çıktığını , ▁bölgenin ▁tarihi ▁olarak ▁xxmaj ▁çin ' e ▁ait ▁olduğunu ▁düşünmektedir . ▁xxmaj ▁tibet ' in ▁kendi ▁kültür ▁ve ▁zenginlik lerinin ▁\" kültürel ▁bir ▁soykırım \" a ▁tabi ▁tutulduğu ▁da ▁iddialar ▁arasındadır . ▁xxmaj ▁çin ▁hükümeti ▁ise ▁bu ▁\" kültürel ▁soykırım \" ▁iddia\n",
        "```\n",
        "\n",
        "These big underscore represents spaces and spaces represent token boundaries . You can see here each of the time, we have each token is a single word `▁politik` but sometimes we get things like `▁zenginlik lerinin`where being turned into two tokens, eventually split the word in the middle.\n",
        "\n",
        "The question is like how would you start with something like this \n",
        "\n",
        "    `而获得诺贝尔文学奖` - And won the Nobel Prize in Literature\n",
        "    or `evlerinizdenmiş` - `(he/she/it) was (apparently/said to be) from your houses`\n",
        "    \n",
        "and turn it into something like `▁zenginlik lerinin`. And these answer is at the high level what you basically do is you start out by looking through your corpus , you find ( using **byte-pair-encoding**) which two characters appear next to each other the most frequently. And you take that pair and you pull it out , and that's a token. \n",
        "\n",
        "So for example, if you're doing English, you may find T and H occur next to each other alot and you say TH is now a token. And then you repeat it, and you look for again two characters that often appear next to each other , now  you can treat TH as one character. Down the track you'll find TH and E often appear next to each other , that's now a token. So you keep doing that until eventually you have some set number of unique tokens. That's called your vocab. \n",
        "\n",
        "So with **sentence piece**, when you call sentence piece you can actually tell it how big a vocab do you want . And so weight default to a vocab of size 30,000. So this here has been tokenized into Turkish subword units using a size of 30,000. Sentence piece actually goes a bit furthur than bye-pair-encoding, it actually creates a neural network language model at a character model. And finds combinations of characters that are most likely to appear together based on the language model but it's the same basic idea.\n",
        "\n",
        "So this idea of using subword units , it's not very well studied but it's absolutely necessary and super powerful. Not only for Turkish, Chinese, Japanese, but also for things like medical texts. Because actually those big long chemical names in the scientific and medical literature contain well-defined subworks units that are frequently reused. So you don't need separate vocab for everyone. The problem with having a separate vocab for everyone is none of those single words appears that often , so you really want to get the sense of meaning of the underlying subword units. \n",
        "\n",
        "So if we want to Turkish from scratch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vWDAkDH7fBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}