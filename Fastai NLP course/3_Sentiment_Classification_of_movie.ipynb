{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Sentiment_Classification_of_movie.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/Fastai%20NLP%20course/3_Sentiment_Classification_of_movie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUt1hpNoZ_It",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Classification of Movie Reviews (using Naive Bayes, Logistic Regression, and Ngrams)\n",
        "\n",
        "The purpose of this notebook is to cover Naive Bayes, Logistic regression, and ngrams (some pretty classic techniques!) for sentiment classification. We will be using sklearn and the fastai library.\n",
        "\n",
        "In a future lesson, we will tackle this same problem of sentiment classification using deep learning, so that you can compare the two approaches\n",
        "\n",
        "The content here was extended from [Lesson 10 of the fast.ai Machine Learning course](https://course.fast.ai/lessonsml1/lesson10.html). Linear model is pretty close to the state of the art here. Jeremy surpassed state of the art using a RNN in fall 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLRNSx13acfj",
        "colab_type": "text"
      },
      "source": [
        "## The fastai library\n",
        "\n",
        "We will begin using the fastai library (version 1.0) in this notebook. We will use it more once we move on to neural networks.\n",
        "\n",
        "The fastai library is built on top of PyTorch and encodes many state-of-the-art best practices. It is used in production at a number of companies. You can read more about it here:\n",
        "\n",
        "- [Fast.ai's software could radically democratize AI](https://www.zdnet.com/article/fast-ais-new-software-could-radically-democratize-ai/) (ZDNet)\n",
        "\n",
        "- [fastai v1 for PyTorch: Fast and accurate neural nets using modern best practices](https://www.fast.ai/2018/10/02/fastai-ai/) (fast.ai)\n",
        "\n",
        "- [fastai docs](https://docs.fast.ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNRFQaryajXJ",
        "colab_type": "text"
      },
      "source": [
        "## Installation\n",
        "\n",
        "With conda:\n",
        "\n",
        "`conda install -c pytorch -c fastai fastai=1.0`\n",
        "\n",
        "Or with pip:\n",
        "\n",
        "`pip install fastai==1.0`\n",
        "\n",
        "More installation information here.\n",
        "\n",
        "Beginning in lesson 4, we will be using GPUs, so if you want, you could switch to a cloud option now to setup fastai."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vqa2eIwbsfk",
        "colab_type": "text"
      },
      "source": [
        "##IMDB dataset\n",
        "\n",
        "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB, We will use the version hosted as part [fast.ai datasets](https://course.fast.ai/datasets.html) on AWS Open Datasets.\n",
        "\n",
        "The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
        "\n",
        "The sentiment classification task consists of predicting the polarity (positive or negative) of a given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmuYLE0BZpS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nV_ekYmcsbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n",
        "import sklearn.feature_extraction as sklearn_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtu8MQbc7JU",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing and term document matrix creation\n",
        "\n",
        "fast.ai has a number of datasets hosted via [AWS Open Datasets](https://course.fast.ai/datasets.html) for easy download. We can see them by checking the docs for URLs (remember ?? is a helpful command):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23JWxgxlcxkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#?? URLs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAXjj6uwdS-Z",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "It is always good to start working on a sample of your data before you use the full dataset-- this allows for quicker computations as you debug and get your code working. For IMDB, there is a sample dataset already available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEXG308DdCpi",
        "colab_type": "code",
        "outputId": "78f6220f-a30c-49c0-f625-890943784ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.fastai/data/imdb_sample')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9DwRL8tdg6J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We are not going to use this dataframe, but are just loading it to get a sense of what our data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ICUDKKdZe_",
        "colab_type": "code",
        "outputId": "3cbc82b0-90ae-41b3-ce61-ad0b0fae9780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>Every once in a long while a movie will come a...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>Name just says it all. I watched this movie wi...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>This movie succeeds at being one of the most u...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               text  is_valid\n",
              "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
              "1  positive  This is a extremely well-made film. The acting...     False\n",
              "2  negative  Every once in a long while a movie will come a...     False\n",
              "3  positive  Name just says it all. I watched this movie wi...     False\n",
              "4  negative  This movie succeeds at being one of the most u...     False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM1xA9oSe7vN",
        "colab_type": "text"
      },
      "source": [
        "We will be using TextList from the fastai library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3i8uzAGdmJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews = (TextList.from_csv(path,'texts.csv',cols='text')\n",
        "                         .split_from_df(col=2)\n",
        "                         .label_from_df(cols=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcGcOqozfps9",
        "colab_type": "text"
      },
      "source": [
        "## Exploring what our data looks like\n",
        "\n",
        "A good first step for any data problem is to explore the data and get a sense of what it looks like. In this case we are looking at movie reviews, which have been labeled as \"positive\" or \"negative\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYaQyJ2MfRBf",
        "colab_type": "code",
        "outputId": "e0f0ce1f-7768-4a11-ca9c-9d7dbe5a0fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "movie_reviews.valid.x[0],movie_reviews.valid.y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos xxmaj this very funny xxmaj british comedy shows what might happen if a section of xxmaj london , in this case xxmaj xxunk , were to xxunk itself independent from the rest of the xxup uk and its laws , xxunk & post - war xxunk . xxmaj merry xxunk is what would happen . \n",
              "  \n",
              "   xxmaj the explosion of a wartime bomb leads to the xxunk of ancient xxunk which show that xxmaj xxunk was xxunk to the xxmaj xxunk of xxmaj xxunk xxunk ago , a small historical xxunk long since forgotten . xxmaj to the new xxmaj xxunk , however , this is an unexpected opportunity to live as they please , free from any xxunk from xxmaj xxunk . \n",
              "  \n",
              "   xxmaj stanley xxmaj xxunk is excellent as the minor city xxunk who suddenly finds himself leading one of the world 's xxunk xxunk . xxmaj xxunk xxmaj margaret xxmaj xxunk is a delight as the history professor who sides with xxmaj xxunk . xxmaj others in the stand - out cast include xxmaj xxunk xxmaj xxunk , xxmaj paul xxmaj xxunk , xxmaj xxunk xxmaj xxunk , xxmaj xxunk xxmaj xxunk & xxmaj sir xxmaj michael xxmaj xxunk . \n",
              "  \n",
              "   xxmaj welcome to xxmaj xxunk !, Category positive)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6BgrXigKsf",
        "colab_type": "text"
      },
      "source": [
        "In NLP, a **token** is the basic unit of processing (what the tokens are depends on the application and your choices). Here, the tokens mostly correspond to words or punctuation, as well as several special tokens, corresponding to unknown words, capitalization, etc.\n",
        "\n",
        "All those tokens starting with \"xx\" are fastai special tokens. You can see the list of all of them and their meanings ([in the fastai docs](https://docs.fast.ai/text.transform.html)):\n",
        "\n",
        "here is the meaning of the special tokens:\n",
        "\n",
        "- UNK (xxunk) is for an unknown word (one that isn't present in the current vocabulary)\n",
        "- PAD (xxpad) is the token used for padding, if we need to regroup several texts of different lengths in a batch\n",
        "- BOS (xxbos) represents the beginning of a text in your dataset\n",
        "- FLD (xxfld) is used if you set mark_fields=True in your TokenizeProcessor to separate the different fields of texts (if your texts are loaded from several columns in a dataframe)\n",
        "- TK_MAJ (xxmaj) is used to indicate the next word begins with a capital in the original text\n",
        "- TK_UP (xxup) is used to indicate the next word is written in all caps in the original text\n",
        "- TK_REP (xxrep) is used to indicate the next character is repeated n times in the original text (usage xxrep n {char})\n",
        "- TK_WREP(xxwrep) is used to indicate the next word is repeated n times in the original text (usage xxwrep n {word})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ5VW1Ndf06I",
        "colab_type": "code",
        "outputId": "2352a701-6b10-416b-d603-3085172893ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(movie_reviews.train.x), len(movie_reviews.valid.x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAgYxSb_hxU8",
        "colab_type": "text"
      },
      "source": [
        "Notice that **ints-to-string** and **string-to-ints** have different lengths. Think for a moment about why this is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqsQc7fJiEwH",
        "colab_type": "code",
        "outputId": "8adc328d-9a2a-48fd-aa71-0066aa73e2e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(movie_reviews.vocab.itos),len(movie_reviews.vocab.stoi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6008, 19161)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXlzDGQjEY6",
        "colab_type": "code",
        "outputId": "3c51b153-d07e-4bf9-886e-f718336a9ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "movie_reviews.vocab.itos[220:230]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bit',\n",
              " 'might',\n",
              " 'things',\n",
              " 'horror',\n",
              " 'us',\n",
              " 'almost',\n",
              " 'may',\n",
              " 'right',\n",
              " 'must',\n",
              " 'away']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Nr4gX2iECQ",
        "colab_type": "text"
      },
      "source": [
        "You have integer and you want to know the vocabulary word does this correspond with. In others meaning, we are going to end up representing our reviews as an array of numbers, but if you want to go from those numbers back to words, you would need to use this into string\n",
        "\n",
        "The word order is based on frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDQ2Q1zFhuDb",
        "colab_type": "code",
        "outputId": "2011fbd4-a69e-4d01-a93c-496e17a94995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.vocab.stoi['language']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duksaJ8akXcd",
        "colab_type": "text"
      },
      "source": [
        "Now we are taking our words to map them to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArEXsDj6kSZD",
        "colab_type": "code",
        "outputId": "d8492a21-7140-430d-cb60-18cb65df6349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.vocab.itos[917]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ya_JqzUkzxP",
        "colab_type": "text"
      },
      "source": [
        "Back to our question: why itos and stoi have different lengths. Because 'language' can be shown many times in text but the '917' is only one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAVOwYxwkm40",
        "colab_type": "code",
        "outputId": "d7a1399e-7097-4cd7-b5a1-ce6a529ef3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "movie_reviews.vocab.stoi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'xxunk': 0,\n",
              "             'xxpad': 1,\n",
              "             'xxbos': 2,\n",
              "             'xxeos': 3,\n",
              "             'xxfld': 4,\n",
              "             'xxmaj': 5,\n",
              "             'xxup': 6,\n",
              "             'xxrep': 7,\n",
              "             'xxwrep': 8,\n",
              "             'the': 9,\n",
              "             '.': 10,\n",
              "             ',': 11,\n",
              "             'and': 12,\n",
              "             'a': 13,\n",
              "             'of': 14,\n",
              "             'to': 15,\n",
              "             'is': 16,\n",
              "             'it': 17,\n",
              "             'in': 18,\n",
              "             'i': 19,\n",
              "             'that': 20,\n",
              "             'this': 21,\n",
              "             '\"': 22,\n",
              "             \"'s\": 23,\n",
              "             '\\n \\n ': 24,\n",
              "             '-': 25,\n",
              "             'was': 26,\n",
              "             'as': 27,\n",
              "             'for': 28,\n",
              "             'movie': 29,\n",
              "             'with': 30,\n",
              "             'but': 31,\n",
              "             'film': 32,\n",
              "             'you': 33,\n",
              "             ')': 34,\n",
              "             'on': 35,\n",
              "             '(': 36,\n",
              "             \"n't\": 37,\n",
              "             'are': 38,\n",
              "             'he': 39,\n",
              "             'his': 40,\n",
              "             'not': 41,\n",
              "             'have': 42,\n",
              "             'be': 43,\n",
              "             'one': 44,\n",
              "             'they': 45,\n",
              "             'all': 46,\n",
              "             'at': 47,\n",
              "             'by': 48,\n",
              "             'an': 49,\n",
              "             'from': 50,\n",
              "             'like': 51,\n",
              "             '!': 52,\n",
              "             'so': 53,\n",
              "             'who': 54,\n",
              "             'there': 55,\n",
              "             'about': 56,\n",
              "             'just': 57,\n",
              "             'out': 58,\n",
              "             'if': 59,\n",
              "             'or': 60,\n",
              "             'do': 61,\n",
              "             \"'\": 62,\n",
              "             'what': 63,\n",
              "             'her': 64,\n",
              "             'has': 65,\n",
              "             'some': 66,\n",
              "             'more': 67,\n",
              "             'good': 68,\n",
              "             'when': 69,\n",
              "             'up': 70,\n",
              "             'very': 71,\n",
              "             '?': 72,\n",
              "             'she': 73,\n",
              "             'would': 74,\n",
              "             'no': 75,\n",
              "             'really': 76,\n",
              "             'were': 77,\n",
              "             'their': 78,\n",
              "             'my': 79,\n",
              "             'had': 80,\n",
              "             'time': 81,\n",
              "             'can': 82,\n",
              "             'only': 83,\n",
              "             'which': 84,\n",
              "             'even': 85,\n",
              "             'see': 86,\n",
              "             'story': 87,\n",
              "             'me': 88,\n",
              "             'into': 89,\n",
              "             'did': 90,\n",
              "             ':': 91,\n",
              "             'well': 92,\n",
              "             'we': 93,\n",
              "             'will': 94,\n",
              "             'does': 95,\n",
              "             'than': 96,\n",
              "             'also': 97,\n",
              "             'get': 98,\n",
              "             '...': 99,\n",
              "             'people': 100,\n",
              "             'other': 101,\n",
              "             'bad': 102,\n",
              "             'been': 103,\n",
              "             'could': 104,\n",
              "             'first': 105,\n",
              "             'much': 106,\n",
              "             'how': 107,\n",
              "             'most': 108,\n",
              "             'any': 109,\n",
              "             'because': 110,\n",
              "             'two': 111,\n",
              "             'then': 112,\n",
              "             'great': 113,\n",
              "             'him': 114,\n",
              "             'its': 115,\n",
              "             'too': 116,\n",
              "             'made': 117,\n",
              "             'them': 118,\n",
              "             'after': 119,\n",
              "             'movies': 120,\n",
              "             'make': 121,\n",
              "             '/': 122,\n",
              "             'way': 123,\n",
              "             'think': 124,\n",
              "             'never': 125,\n",
              "             'watch': 126,\n",
              "             'acting': 127,\n",
              "             'seen': 128,\n",
              "             ';': 129,\n",
              "             'films': 130,\n",
              "             'plot': 131,\n",
              "             'being': 132,\n",
              "             'many': 133,\n",
              "             'over': 134,\n",
              "             'where': 135,\n",
              "             'character': 136,\n",
              "             'man': 137,\n",
              "             'little': 138,\n",
              "             'better': 139,\n",
              "             'life': 140,\n",
              "             'characters': 141,\n",
              "             'love': 142,\n",
              "             'your': 143,\n",
              "             'here': 144,\n",
              "             'know': 145,\n",
              "             'scenes': 146,\n",
              "             'best': 147,\n",
              "             'end': 148,\n",
              "             'show': 149,\n",
              "             'while': 150,\n",
              "             'through': 151,\n",
              "             'should': 152,\n",
              "             'off': 153,\n",
              "             'ever': 154,\n",
              "             'these': 155,\n",
              "             'go': 156,\n",
              "             'such': 157,\n",
              "             'say': 158,\n",
              "             '--': 159,\n",
              "             'something': 160,\n",
              "             'scene': 161,\n",
              "             'still': 162,\n",
              "             'before': 163,\n",
              "             'though': 164,\n",
              "             'watching': 165,\n",
              "             'between': 166,\n",
              "             'actually': 167,\n",
              "             'old': 168,\n",
              "             '10': 169,\n",
              "             'find': 170,\n",
              "             'back': 171,\n",
              "             'now': 172,\n",
              "             'why': 173,\n",
              "             'years': 174,\n",
              "             \"'ve\": 175,\n",
              "             'actors': 176,\n",
              "             'fact': 177,\n",
              "             'those': 178,\n",
              "             \"'m\": 179,\n",
              "             'thing': 180,\n",
              "             'pretty': 181,\n",
              "             'quite': 182,\n",
              "             'part': 183,\n",
              "             'going': 184,\n",
              "             'same': 185,\n",
              "             'real': 186,\n",
              "             'another': 187,\n",
              "             'down': 188,\n",
              "             'funny': 189,\n",
              "             'nothing': 190,\n",
              "             'look': 191,\n",
              "             'makes': 192,\n",
              "             '*': 193,\n",
              "             'new': 194,\n",
              "             'want': 195,\n",
              "             'action': 196,\n",
              "             '&': 197,\n",
              "             'director': 198,\n",
              "             'work': 199,\n",
              "             'few': 200,\n",
              "             \"'re\": 201,\n",
              "             'seems': 202,\n",
              "             'around': 203,\n",
              "             'world': 204,\n",
              "             'point': 205,\n",
              "             'without': 206,\n",
              "             'cast': 207,\n",
              "             'again': 208,\n",
              "             'own': 209,\n",
              "             'both': 210,\n",
              "             'lot': 211,\n",
              "             'enough': 212,\n",
              "             'every': 213,\n",
              "             'family': 214,\n",
              "             'got': 215,\n",
              "             'ca': 216,\n",
              "             \"'ll\": 217,\n",
              "             'probably': 218,\n",
              "             'big': 219,\n",
              "             'bit': 220,\n",
              "             'might': 221,\n",
              "             'things': 222,\n",
              "             'horror': 223,\n",
              "             'us': 224,\n",
              "             'almost': 225,\n",
              "             'may': 226,\n",
              "             'right': 227,\n",
              "             'must': 228,\n",
              "             'away': 229,\n",
              "             'thought': 230,\n",
              "             'interesting': 231,\n",
              "             'least': 232,\n",
              "             'whole': 233,\n",
              "             'series': 234,\n",
              "             'gets': 235,\n",
              "             'each': 236,\n",
              "             'give': 237,\n",
              "             'young': 238,\n",
              "             'however': 239,\n",
              "             'making': 240,\n",
              "             'day': 241,\n",
              "             'fun': 242,\n",
              "             'anything': 243,\n",
              "             'minutes': 244,\n",
              "             'kind': 245,\n",
              "             'come': 246,\n",
              "             'girl': 247,\n",
              "             'saw': 248,\n",
              "             'script': 249,\n",
              "             'take': 250,\n",
              "             'long': 251,\n",
              "             'times': 252,\n",
              "             'someone': 253,\n",
              "             'found': 254,\n",
              "             'done': 255,\n",
              "             'feel': 256,\n",
              "             'far': 257,\n",
              "             'since': 258,\n",
              "             'role': 259,\n",
              "             'original': 260,\n",
              "             'course': 261,\n",
              "             'goes': 262,\n",
              "             'last': 263,\n",
              "             'true': 264,\n",
              "             'simply': 265,\n",
              "             'always': 266,\n",
              "             \"'d\": 267,\n",
              "             'tv': 268,\n",
              "             'hard': 269,\n",
              "             'place': 270,\n",
              "             'set': 271,\n",
              "             'trying': 272,\n",
              "             'believe': 273,\n",
              "             'shot': 274,\n",
              "             'comes': 275,\n",
              "             'actor': 276,\n",
              "             'yet': 277,\n",
              "             '4': 278,\n",
              "             'having': 279,\n",
              "             'book': 280,\n",
              "             'looks': 281,\n",
              "             'guy': 282,\n",
              "             'screen': 283,\n",
              "             'later': 284,\n",
              "             'shows': 285,\n",
              "             'performance': 286,\n",
              "             'worth': 287,\n",
              "             'comedy': 288,\n",
              "             'sure': 289,\n",
              "             'looking': 290,\n",
              "             'sense': 291,\n",
              "             'star': 292,\n",
              "             'effects': 293,\n",
              "             'read': 294,\n",
              "             'takes': 295,\n",
              "             'although': 296,\n",
              "             'audience': 297,\n",
              "             'ending': 298,\n",
              "             'john': 299,\n",
              "             'anyone': 300,\n",
              "             'worst': 301,\n",
              "             'american': 302,\n",
              "             'year': 303,\n",
              "             'especially': 304,\n",
              "             'women': 305,\n",
              "             'together': 306,\n",
              "             'dvd': 307,\n",
              "             'instead': 308,\n",
              "             'different': 309,\n",
              "             'am': 310,\n",
              "             'woman': 311,\n",
              "             'men': 312,\n",
              "             '2': 313,\n",
              "             'our': 314,\n",
              "             'played': 315,\n",
              "             'music': 316,\n",
              "             'special': 317,\n",
              "             'three': 318,\n",
              "             'rest': 319,\n",
              "             'put': 320,\n",
              "             'maybe': 321,\n",
              "             'wife': 322,\n",
              "             'kids': 323,\n",
              "             'war': 324,\n",
              "             'left': 325,\n",
              "             'black': 326,\n",
              "             'once': 327,\n",
              "             'second': 328,\n",
              "             'watched': 329,\n",
              "             'next': 330,\n",
              "             'friends': 331,\n",
              "             'rather': 332,\n",
              "             'let': 333,\n",
              "             '\\x96': 334,\n",
              "             'job': 335,\n",
              "             'start': 336,\n",
              "             'others': 337,\n",
              "             'budget': 338,\n",
              "             'need': 339,\n",
              "             'mind': 340,\n",
              "             'said': 341,\n",
              "             'main': 342,\n",
              "             'else': 343,\n",
              "             'wrong': 344,\n",
              "             'beautiful': 345,\n",
              "             'half': 346,\n",
              "             'high': 347,\n",
              "             'idea': 348,\n",
              "             'death': 349,\n",
              "             'tell': 350,\n",
              "             'help': 351,\n",
              "             'nice': 352,\n",
              "             'seem': 353,\n",
              "             'perhaps': 354,\n",
              "             'hollywood': 355,\n",
              "             'everyone': 356,\n",
              "             'play': 357,\n",
              "             'case': 358,\n",
              "             'production': 359,\n",
              "             'piece': 360,\n",
              "             'episode': 361,\n",
              "             'camera': 362,\n",
              "             'low': 363,\n",
              "             'already': 364,\n",
              "             'top': 365,\n",
              "             'poor': 366,\n",
              "             'during': 367,\n",
              "             '3': 368,\n",
              "             'stars': 369,\n",
              "             'house': 370,\n",
              "             '..': 371,\n",
              "             'couple': 372,\n",
              "             'boring': 373,\n",
              "             'reason': 374,\n",
              "             'try': 375,\n",
              "             'along': 376,\n",
              "             'name': 377,\n",
              "             'small': 378,\n",
              "             'plays': 379,\n",
              "             'father': 380,\n",
              "             'everything': 381,\n",
              "             'used': 382,\n",
              "             'video': 383,\n",
              "             'getting': 384,\n",
              "             'money': 385,\n",
              "             'full': 386,\n",
              "             'less': 387,\n",
              "             'performances': 388,\n",
              "             'often': 389,\n",
              "             'liked': 390,\n",
              "             'came': 391,\n",
              "             '1': 392,\n",
              "             'robert': 393,\n",
              "             'either': 394,\n",
              "             'fan': 395,\n",
              "             'given': 396,\n",
              "             'hand': 397,\n",
              "             'kill': 398,\n",
              "             'felt': 399,\n",
              "             'yes': 400,\n",
              "             'completely': 401,\n",
              "             'night': 402,\n",
              "             'children': 403,\n",
              "             'himself': 404,\n",
              "             'girls': 405,\n",
              "             'early': 406,\n",
              "             'awful': 407,\n",
              "             'oh': 408,\n",
              "             'live': 409,\n",
              "             'picture': 410,\n",
              "             'parts': 411,\n",
              "             'throughout': 412,\n",
              "             'until': 413,\n",
              "             'become': 414,\n",
              "             'town': 415,\n",
              "             'written': 416,\n",
              "             'terrible': 417,\n",
              "             'turn': 418,\n",
              "             'child': 419,\n",
              "             'despite': 420,\n",
              "             'moments': 421,\n",
              "             'boy': 422,\n",
              "             'problem': 423,\n",
              "             'head': 424,\n",
              "             'stupid': 425,\n",
              "             'beginning': 426,\n",
              "             'home': 427,\n",
              "             'version': 428,\n",
              "             'able': 429,\n",
              "             'excellent': 430,\n",
              "             'sometimes': 431,\n",
              "             'overall': 432,\n",
              "             'recommend': 433,\n",
              "             'sex': 434,\n",
              "             'keep': 435,\n",
              "             'human': 436,\n",
              "             'drama': 437,\n",
              "             'hero': 438,\n",
              "             'supposed': 439,\n",
              "             'seemed': 440,\n",
              "             'use': 441,\n",
              "             'writing': 442,\n",
              "             'wo': 443,\n",
              "             'remember': 444,\n",
              "             'went': 445,\n",
              "             'enjoy': 446,\n",
              "             'classic': 447,\n",
              "             'person': 448,\n",
              "             'killer': 449,\n",
              "             'lost': 450,\n",
              "             'late': 451,\n",
              "             '5': 452,\n",
              "             'title': 453,\n",
              "             'king': 454,\n",
              "             'entire': 455,\n",
              "             'history': 456,\n",
              "             'son': 457,\n",
              "             'school': 458,\n",
              "             'lead': 459,\n",
              "             'english': 460,\n",
              "             'sound': 461,\n",
              "             'cinema': 462,\n",
              "             'seeing': 463,\n",
              "             'unfortunately': 464,\n",
              "             'genre': 465,\n",
              "             'sort': 466,\n",
              "             'mean': 467,\n",
              "             'friend': 468,\n",
              "             'fans': 469,\n",
              "             'close': 470,\n",
              "             'quality': 471,\n",
              "             'definitely': 472,\n",
              "             'james': 473,\n",
              "             'worse': 474,\n",
              "             'says': 475,\n",
              "             'except': 476,\n",
              "             'doing': 477,\n",
              "             'itself': 478,\n",
              "             'past': 479,\n",
              "             'certainly': 480,\n",
              "             'days': 481,\n",
              "             'five': 482,\n",
              "             'dialogue': 483,\n",
              "             'line': 484,\n",
              "             'anyway': 485,\n",
              "             'under': 486,\n",
              "             'tries': 487,\n",
              "             'called': 488,\n",
              "             'fine': 489,\n",
              "             'guys': 490,\n",
              "             'care': 491,\n",
              "             'style': 492,\n",
              "             'hope': 493,\n",
              "             'short': 494,\n",
              "             'lines': 495,\n",
              "             'told': 496,\n",
              "             'car': 497,\n",
              "             'decent': 498,\n",
              "             'brother': 499,\n",
              "             'killed': 500,\n",
              "             'wanted': 501,\n",
              "             'entertaining': 502,\n",
              "             'based': 503,\n",
              "             'absolutely': 504,\n",
              "             'feeling': 505,\n",
              "             'truly': 506,\n",
              "             'etc': 507,\n",
              "             'heard': 508,\n",
              "             'serious': 509,\n",
              "             'run': 510,\n",
              "             'wonderful': 511,\n",
              "             'lives': 512,\n",
              "             'gives': 513,\n",
              "             'moment': 514,\n",
              "             'game': 515,\n",
              "             'documentary': 516,\n",
              "             'self': 517,\n",
              "             'several': 518,\n",
              "             'waste': 519,\n",
              "             'dead': 520,\n",
              "             'blood': 521,\n",
              "             'matter': 522,\n",
              "             'wonder': 523,\n",
              "             'humor': 524,\n",
              "             'thinking': 525,\n",
              "             'against': 526,\n",
              "             'white': 527,\n",
              "             'side': 528,\n",
              "             'works': 529,\n",
              "             'mother': 530,\n",
              "             'flick': 531,\n",
              "             'stuff': 532,\n",
              "             'turns': 533,\n",
              "             'finally': 534,\n",
              "             'loved': 535,\n",
              "             'group': 536,\n",
              "             'wants': 537,\n",
              "             'face': 538,\n",
              "             'guess': 539,\n",
              "             'dark': 540,\n",
              "             'city': 541,\n",
              "             'events': 542,\n",
              "             'starts': 543,\n",
              "             'hour': 544,\n",
              "             'took': 545,\n",
              "             'george': 546,\n",
              "             'themselves': 547,\n",
              "             'red': 548,\n",
              "             'behind': 549,\n",
              "             'talking': 550,\n",
              "             'hit': 551,\n",
              "             'eyes': 552,\n",
              "             'attempt': 553,\n",
              "             'direction': 554,\n",
              "             'novel': 555,\n",
              "             'saying': 556,\n",
              "             'word': 557,\n",
              "             'dull': 558,\n",
              "             'light': 559,\n",
              "             'view': 560,\n",
              "             'playing': 561,\n",
              "             'opinion': 562,\n",
              "             'expect': 563,\n",
              "             'evil': 564,\n",
              "             'ten': 565,\n",
              "             'violence': 566,\n",
              "             'local': 567,\n",
              "             'final': 568,\n",
              "             'gave': 569,\n",
              "             'leave': 570,\n",
              "             'paul': 571,\n",
              "             'crap': 572,\n",
              "             'happens': 573,\n",
              "             'knows': 574,\n",
              "             'problems': 575,\n",
              "             'example': 576,\n",
              "             'relationship': 577,\n",
              "             'non': 578,\n",
              "             'michael': 579,\n",
              "             'victor': 580,\n",
              "             'ridiculous': 581,\n",
              "             'god': 582,\n",
              "             'similar': 583,\n",
              "             'general': 584,\n",
              "             'major': 585,\n",
              "             'bunch': 586,\n",
              "             'sister': 587,\n",
              "             'oscar': 588,\n",
              "             'turned': 589,\n",
              "             'brilliant': 590,\n",
              "             'highly': 591,\n",
              "             'nearly': 592,\n",
              "             'de': 593,\n",
              "             'please': 594,\n",
              "             'romance': 595,\n",
              "             'body': 596,\n",
              "             'extremely': 597,\n",
              "             'mr.': 598,\n",
              "             'soon': 599,\n",
              "             'yourself': 600,\n",
              "             'known': 601,\n",
              "             'lack': 602,\n",
              "             'age': 603,\n",
              "             'interest': 604,\n",
              "             'ago': 605,\n",
              "             'stories': 606,\n",
              "             'exactly': 607,\n",
              "             'finds': 608,\n",
              "             'modern': 609,\n",
              "             'voice': 610,\n",
              "             'perfect': 611,\n",
              "             'heart': 612,\n",
              "             'alone': 613,\n",
              "             'tells': 614,\n",
              "             'daughter': 615,\n",
              "             'directed': 616,\n",
              "             'needs': 617,\n",
              "             'kid': 618,\n",
              "             'lady': 619,\n",
              "             'sad': 620,\n",
              "             'fight': 621,\n",
              "             'happened': 622,\n",
              "             'eye': 623,\n",
              "             'favorite': 624,\n",
              "             'using': 625,\n",
              "             'upon': 626,\n",
              "             'ben': 627,\n",
              "             'none': 628,\n",
              "             'beyond': 629,\n",
              "             'nature': 630,\n",
              "             'change': 631,\n",
              "             'save': 632,\n",
              "             'shots': 633,\n",
              "             'country': 634,\n",
              "             'number': 635,\n",
              "             'shown': 636,\n",
              "             'surprised': 637,\n",
              "             'romantic': 638,\n",
              "             'huge': 639,\n",
              "             'murder': 640,\n",
              "             'steve': 641,\n",
              "             'slow': 642,\n",
              "             'myself': 643,\n",
              "             'woods': 644,\n",
              "             'apparently': 645,\n",
              "             'lake': 646,\n",
              "             'cheap': 647,\n",
              "             'involved': 648,\n",
              "             'roles': 649,\n",
              "             '6': 650,\n",
              "             'gore': 651,\n",
              "             'obviously': 652,\n",
              "             'knew': 653,\n",
              "             'level': 654,\n",
              "             '8': 655,\n",
              "             'experience': 656,\n",
              "             'became': 657,\n",
              "             'gone': 658,\n",
              "             'cover': 659,\n",
              "             'amazing': 660,\n",
              "             'create': 661,\n",
              "             'living': 662,\n",
              "             'usually': 663,\n",
              "             'order': 664,\n",
              "             'monster': 665,\n",
              "             'happen': 666,\n",
              "             'list': 667,\n",
              "             'clearly': 668,\n",
              "             'power': 669,\n",
              "             'features': 670,\n",
              "             're': 671,\n",
              "             'subject': 672,\n",
              "             'across': 673,\n",
              "             'parents': 674,\n",
              "             'seriously': 675,\n",
              "             'ways': 676,\n",
              "             'room': 677,\n",
              "             'filmed': 678,\n",
              "             'cheesy': 679,\n",
              "             'disappointed': 680,\n",
              "             'important': 681,\n",
              "             'plenty': 682,\n",
              "             '7': 683,\n",
              "             'particular': 684,\n",
              "             'started': 685,\n",
              "             'today': 686,\n",
              "             'enjoyed': 687,\n",
              "             'cinematography': 688,\n",
              "             'annoying': 689,\n",
              "             'looked': 690,\n",
              "             'supporting': 691,\n",
              "             'mostly': 692,\n",
              "             'message': 693,\n",
              "             'somewhat': 694,\n",
              "             'viewer': 695,\n",
              "             'type': 696,\n",
              "             'certain': 697,\n",
              "             'release': 698,\n",
              "             'effort': 699,\n",
              "             'possible': 700,\n",
              "             'add': 701,\n",
              "             'figure': 702,\n",
              "             'named': 703,\n",
              "             'wish': 704,\n",
              "             'difficult': 705,\n",
              "             'falls': 706,\n",
              "             'four': 707,\n",
              "             'husband': 708,\n",
              "             'score': 709,\n",
              "             'leads': 710,\n",
              "             'form': 711,\n",
              "             'working': 712,\n",
              "             'writer': 713,\n",
              "             'sets': 714,\n",
              "             'including': 715,\n",
              "             'enjoyable': 716,\n",
              "             'ok': 717,\n",
              "             'note': 718,\n",
              "             'spent': 719,\n",
              "             'review': 720,\n",
              "             'art': 721,\n",
              "             'police': 722,\n",
              "             'sit': 723,\n",
              "             'horrible': 724,\n",
              "             'actress': 725,\n",
              "             'ones': 726,\n",
              "             'bring': 727,\n",
              "             'greatest': 728,\n",
              "             'dance': 729,\n",
              "             'earth': 730,\n",
              "             'becomes': 731,\n",
              "             'happy': 732,\n",
              "             'cut': 733,\n",
              "             'straight': 734,\n",
              "             'soundtrack': 735,\n",
              "             'leading': 736,\n",
              "             'laugh': 737,\n",
              "             'strange': 738,\n",
              "             'space': 739,\n",
              "             'b': 740,\n",
              "             'tale': 741,\n",
              "             'comic': 742,\n",
              "             'near': 743,\n",
              "             'due': 744,\n",
              "             'weak': 745,\n",
              "             'earlier': 746,\n",
              "             'follow': 747,\n",
              "             'british': 748,\n",
              "             'ends': 749,\n",
              "             'typical': 750,\n",
              "             'attention': 751,\n",
              "             'points': 752,\n",
              "             'talent': 753,\n",
              "             'tom': 754,\n",
              "             'female': 755,\n",
              "             'future': 756,\n",
              "             'fall': 757,\n",
              "             'laughs': 758,\n",
              "             'stop': 759,\n",
              "             'easy': 760,\n",
              "             'moving': 761,\n",
              "             'apart': 762,\n",
              "             'chance': 763,\n",
              "             'running': 764,\n",
              "             'york': 765,\n",
              "             'particularly': 766,\n",
              "             'luke': 767,\n",
              "             'bill': 768,\n",
              "             'forced': 769,\n",
              "             'theme': 770,\n",
              "             'rating': 771,\n",
              "             'coming': 772,\n",
              "             'davis': 773,\n",
              "             'totally': 774,\n",
              "             'realistic': 775,\n",
              "             'simple': 776,\n",
              "             'hours': 777,\n",
              "             'taken': 778,\n",
              "             'indeed': 779,\n",
              "             'released': 780,\n",
              "             'sexual': 781,\n",
              "             'feels': 782,\n",
              "             'french': 783,\n",
              "             'screenplay': 784,\n",
              "             'la': 785,\n",
              "             'jokes': 786,\n",
              "             'sequences': 787,\n",
              "             'chase': 788,\n",
              "             'portrayed': 789,\n",
              "             'dramatic': 790,\n",
              "             'mention': 791,\n",
              "             'talk': 792,\n",
              "             'gun': 793,\n",
              "             'thriller': 794,\n",
              "             'jimmy': 795,\n",
              "             'career': 796,\n",
              "             'reality': 797,\n",
              "             'incredibly': 798,\n",
              "             'whether': 799,\n",
              "             'towards': 800,\n",
              "             'easily': 801,\n",
              "             'entertainment': 802,\n",
              "             'feature': 803,\n",
              "             'western': 804,\n",
              "             'dialog': 805,\n",
              "             'business': 806,\n",
              "             'suspense': 807,\n",
              "             'focus': 808,\n",
              "             'doubt': 809,\n",
              "             'possibly': 810,\n",
              "             'water': 811,\n",
              "             'gay': 812,\n",
              "             'blob': 813,\n",
              "             'comments': 814,\n",
              "             'brothers': 815,\n",
              "             'clear': 816,\n",
              "             'agree': 817,\n",
              "             'allen': 818,\n",
              "             'door': 819,\n",
              "             'editing': 820,\n",
              "             'third': 821,\n",
              "             'deserves': 822,\n",
              "             'silly': 823,\n",
              "             'fantastic': 824,\n",
              "             'convincing': 825,\n",
              "             'hardly': 826,\n",
              "             'lame': 827,\n",
              "             'act': 828,\n",
              "             'former': 829,\n",
              "             'material': 830,\n",
              "             'appears': 831,\n",
              "             'understand': 832,\n",
              "             'twist': 833,\n",
              "             'episodes': 834,\n",
              "             'buy': 835,\n",
              "             'secret': 836,\n",
              "             'richard': 837,\n",
              "             'south': 838,\n",
              "             'bourne': 839,\n",
              "             'deal': 840,\n",
              "             'musical': 841,\n",
              "             'words': 842,\n",
              "             'unique': 843,\n",
              "             'mess': 844,\n",
              "             'opening': 845,\n",
              "             'society': 846,\n",
              "             'avoid': 847,\n",
              "             'footage': 848,\n",
              "             'joe': 849,\n",
              "             'free': 850,\n",
              "             'forget': 851,\n",
              "             'herself': 852,\n",
              "             'appear': 853,\n",
              "             'obvious': 854,\n",
              "             'box': 855,\n",
              "             'single': 856,\n",
              "             'average': 857,\n",
              "             'indian': 858,\n",
              "             'rent': 859,\n",
              "             'okay': 860,\n",
              "             'scary': 861,\n",
              "             'within': 862,\n",
              "             'office': 863,\n",
              "             'crime': 864,\n",
              "             'science': 865,\n",
              "             '80': 866,\n",
              "             'believable': 867,\n",
              "             'period': 868,\n",
              "             'showing': 869,\n",
              "             'call': 870,\n",
              "             'return': 871,\n",
              "             'keeps': 872,\n",
              "             'lee': 873,\n",
              "             'expected': 874,\n",
              "             'stay': 875,\n",
              "             'middle': 876,\n",
              "             'jack': 877,\n",
              "             'hands': 878,\n",
              "             'david': 879,\n",
              "             'attempts': 880,\n",
              "             'strong': 881,\n",
              "             'tension': 882,\n",
              "             'crew': 883,\n",
              "             'hilarious': 884,\n",
              "             'grade': 885,\n",
              "             'outside': 886,\n",
              "             'means': 887,\n",
              "             'viewing': 888,\n",
              "             'sadly': 889,\n",
              "             'hell': 890,\n",
              "             'whatever': 891,\n",
              "             'sorry': 892,\n",
              "             'recently': 893,\n",
              "             'stage': 894,\n",
              "             'decides': 895,\n",
              "             'hear': 896,\n",
              "             'team': 897,\n",
              "             'learn': 898,\n",
              "             'nor': 899,\n",
              "             'open': 900,\n",
              "             'break': 901,\n",
              "             'question': 902,\n",
              "             'remake': 903,\n",
              "             'porn': 904,\n",
              "             'pain': 905,\n",
              "             'imagine': 906,\n",
              "             'deep': 907,\n",
              "             'zombie': 908,\n",
              "             'basically': 909,\n",
              "             'killing': 910,\n",
              "             'company': 911,\n",
              "             'poorly': 912,\n",
              "             'dr.': 913,\n",
              "             'predictable': 914,\n",
              "             'taking': 915,\n",
              "             'large': 916,\n",
              "             'language': 917,\n",
              "             'giving': 918,\n",
              "             'public': 919,\n",
              "             'audiences': 920,\n",
              "             'ask': 921,\n",
              "             'cool': 922,\n",
              "             'america': 923,\n",
              "             'slasher': 924,\n",
              "             'west': 925,\n",
              "             'mentioned': 926,\n",
              "             'die': 927,\n",
              "             'christmas': 928,\n",
              "             'complete': 929,\n",
              "             'needed': 930,\n",
              "             'martin': 931,\n",
              "             'cgi': 932,\n",
              "             'boys': 933,\n",
              "             'vargas': 934,\n",
              "             'usual': 935,\n",
              "             'begin': 936,\n",
              "             'dad': 937,\n",
              "             'total': 938,\n",
              "             'somehow': 939,\n",
              "             'stick': 940,\n",
              "             'shame': 941,\n",
              "             'successful': 942,\n",
              "             'sitting': 943,\n",
              "             'fred': 944,\n",
              "             'meets': 945,\n",
              "             'unless': 946,\n",
              "             'dancing': 947,\n",
              "             'sounds': 948,\n",
              "             'above': 949,\n",
              "             'elements': 950,\n",
              "             'whose': 951,\n",
              "             'german': 952,\n",
              "             'considering': 953,\n",
              "             'caught': 954,\n",
              "             'credit': 955,\n",
              "             'interested': 956,\n",
              "             'move': 957,\n",
              "             'filming': 958,\n",
              "             'truth': 959,\n",
              "             'eventually': 960,\n",
              "             'share': 961,\n",
              "             'ability': 962,\n",
              "             'meaning': 963,\n",
              "             'agent': 964,\n",
              "             'fast': 965,\n",
              "             'stand': 966,\n",
              "             'onto': 967,\n",
              "             'plain': 968,\n",
              "             'comment': 969,\n",
              "             'kept': 970,\n",
              "             'situation': 971,\n",
              "             'setting': 972,\n",
              "             'value': 973,\n",
              "             'willing': 974,\n",
              "             'realize': 975,\n",
              "             'acted': 976,\n",
              "             'weird': 977,\n",
              "             'alive': 978,\n",
              "             'fairly': 979,\n",
              "             'dream': 980,\n",
              "             'building': 981,\n",
              "             'hair': 982,\n",
              "             'bored': 983,\n",
              "             'minute': 984,\n",
              "             'emotional': 985,\n",
              "             'directing': 986,\n",
              "             'theatrical': 987,\n",
              "             'famous': 988,\n",
              "             'begins': 989,\n",
              "             'front': 990,\n",
              "             'catch': 991,\n",
              "             'sequence': 992,\n",
              "             'runs': 993,\n",
              "             'follows': 994,\n",
              "             'song': 995,\n",
              "             'government': 996,\n",
              "             'miss': 997,\n",
              "             'actual': 998,\n",
              "             'makers': 999,\n",
              "             ...})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510SJZ2fmNXG",
        "colab_type": "text"
      },
      "source": [
        "Let's test that a non-word maps to xxunk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAOPvp4fl_LQ",
        "colab_type": "code",
        "outputId": "4f21359f-7aa5-41aa-b733-11073f2a171f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.vocab.itos[movie_reviews.vocab.stoi['rrachell']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxunk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn9zKrBrmXeS",
        "colab_type": "code",
        "outputId": "83e0f43b-caf4-481e-a85a-572c4ce2b1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.vocab.itos[movie_reviews.vocab.stoi['language']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6_pb7Wzmd16",
        "colab_type": "code",
        "outputId": "4e78bd6d-2506-41dd-fb22-4b4617a4c9cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "t= movie_reviews.train[0][0]\n",
        "t.data[:30]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,    5, 4619,   25,    0,   25,  867,   52,    5, 3776,    5, 1800,   95,   37,   85,  191,   64,  935,\n",
              "          0, 2738,  517,   18,   21,   11,   84, 2417,  192,   88, 3777,   64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzG-NKakpVdC",
        "colab_type": "text"
      },
      "source": [
        "# Creating our term-document matrix\n",
        "\n",
        "As we covered in the last lesson, a term-document matrix represents a document as a \"bag of words\", that is, we don't keep track of the order the words are in, just which words occur (and how often).\n",
        "\n",
        "In the previous lesson, we used [sklearn's CountVectorizer](https://github.com/scikit-learn/scikit-learn/blob/55bf5d9/sklearn/feature_extraction/text.py#L940). Today we will create our own (similar) version. This is for two reasons:\n",
        "\n",
        "- to understand what sklearn is doing underneath the hood\n",
        "- to create something that will work with a fastai TextList\n",
        "\n",
        "To create our term-document matrix, we first need to learn about **counters** and **sparse matrices**.\n",
        "\n",
        "**Counters**\n",
        "\n",
        "Counters are a useful Python object. If you aren't familar with them, here is how they work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25BTYQS8ml17",
        "colab_type": "code",
        "outputId": "84df51a9-0a3d-46d5-fc89-544674f2f4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c = Counter([4,2,8,8,4,8])\n",
        "c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({2: 1, 4: 2, 8: 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPI5lbbaq21d",
        "colab_type": "code",
        "outputId": "fcf56c41-2f07-4a3e-e921-c11f195c2e5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c.values()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([2, 1, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDTSapazq6le",
        "colab_type": "code",
        "outputId": "9b1d8342-5344-456b-ac53-b711f2e665a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([4, 2, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlt1TJ8xtjQ2",
        "colab_type": "text"
      },
      "source": [
        "## Sparse Matrices (in Scipy)\n",
        "\n",
        "Even though we've reduced over 19,000 words down to 6,000, that is still a lot! Most tokens don't appear in most reviews. We want to take advantage of this by storing our data as a **sparse matrix**.\n",
        "\n",
        "A matrix with lots of zeros is called **sparse** (the opposite of sparse is **dense**). For sparse matrices, you can save a lot of memory by only storing the non-zero values.\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/sparse.png)\n",
        "\n",
        "Another example of a large, sparse matrix:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/Finite_element_sparse_matrix.png)\n",
        "\n",
        "[Source](https://commons.wikimedia.org/w/index.php?curid=2245335)\n",
        "\n",
        "There are the most common sparse storage formats:\n",
        "\n",
        "- coordinate-wise (scipy calls COO)\n",
        "- compressed sparse row (CSR)\n",
        "- compressed sparse column (CSC)\n",
        "\n",
        "Let's walk through [these examples](http://www.mathcs.emory.edu/~cheung/Courses/561/Syllabus/3-C/sparse.html) --> [32:00](https://youtu.be/hp2ipC5pW4I?t=1978)\n",
        "\n",
        "There are actually [many more formats](http://www.cs.colostate.edu/~mcrob/toolbox/c++/sparseMatrix/sparse_matrix_compression.html) as well.\n",
        "\n",
        "A class of matrices (e.g, diagonal) is generally called sparse if the number of non-zero elements is proportional to the number of rows (or columns) instead of being proportional to the product rows x columns.\n",
        "\n",
        "**Scipy Implementation**\n",
        "\n",
        "From the [Scipy Sparse Matrix Documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)\n",
        "\n",
        "To construct a matrix efficiently, use either dok_matrix or lil_matrix. The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays. As illustrated below, the COO format may also be used to efficiently construct matrices\n",
        "To perform manipulations such as multiplication or inversion, first convert the matrix to either CSC or CSR format.\n",
        "All conversions among the CSR, CSC, and COO formats are efficient, linear-time operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qb-IFpBKdl4",
        "colab_type": "text"
      },
      "source": [
        "## Our version of CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-ybf_THq8lG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_term_doc_matrix(label_list,vocab_len):\n",
        "    j_indices = []\n",
        "    indptr = []\n",
        "    values = []\n",
        "    indptr.append(0)\n",
        "    \n",
        "    for i, doc in enumerate(label_list):\n",
        "        feature_counter = Counter(doc.data)\n",
        "        j_indices.extend(feature_counter.keys())\n",
        "        values.extend(feature_counter.values())\n",
        "        indptr.append(len(j_indices))\n",
        "        \n",
        "    # return(values,j_indices,indptr)\n",
        "    return scipy.sparse.csr_matrix((values,j_indices,indptr),\n",
        "                                   shape = (len(indptr)-1,vocab_len),\n",
        "                                   dtype = int)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ds-zvs7MByr",
        "colab_type": "text"
      },
      "source": [
        "### feature_counter = Counter(doc.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4p9yyg8MRfx",
        "colab_type": "code",
        "outputId": "74da6488-addc-4805-ca9b-71141f609e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "movie_reviews.train.x[0] \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj un - xxunk - believable ! xxmaj meg xxmaj ryan does n't even look her usual xxunk lovable self in this , which normally makes me forgive her shallow xxunk acting xxunk . xxmaj hard to believe she was the producer on this dog . xxmaj plus xxmaj kevin xxmaj kline : what kind of suicide trip has his career been on ? xxmaj xxunk ... xxmaj xxunk ! ! ! xxmaj finally this was directed by the guy who did xxmaj big xxmaj xxunk ? xxmaj must be a replay of xxmaj jonestown - hollywood style . xxmaj xxunk !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvlgIhPsMa8W",
        "colab_type": "code",
        "outputId": "8a4bbfd2-4442-4078-b81b-796c166b2f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.train.x[0].data "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2,    5, 4619,   25, ...,   10,    5,    0,   52])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4ofNBwCNAqx",
        "colab_type": "text"
      },
      "source": [
        "taking the frequency of different words and using the Counter to get dictionary of frequencies, getting the key and values ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeycvuGVMyK8",
        "colab_type": "code",
        "outputId": "c658645f-c946-4cdf-c0ce-c4c2d7824bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Counter(movie_reviews.train.x[0].data )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 8,\n",
              "         2: 1,\n",
              "         5: 15,\n",
              "         9: 2,\n",
              "         10: 3,\n",
              "         11: 1,\n",
              "         13: 1,\n",
              "         14: 2,\n",
              "         15: 1,\n",
              "         18: 1,\n",
              "         21: 3,\n",
              "         25: 3,\n",
              "         26: 2,\n",
              "         35: 2,\n",
              "         37: 1,\n",
              "         40: 1,\n",
              "         43: 1,\n",
              "         48: 1,\n",
              "         52: 5,\n",
              "         54: 1,\n",
              "         63: 1,\n",
              "         64: 2,\n",
              "         65: 1,\n",
              "         72: 2,\n",
              "         73: 1,\n",
              "         84: 1,\n",
              "         85: 1,\n",
              "         88: 1,\n",
              "         90: 1,\n",
              "         91: 1,\n",
              "         95: 1,\n",
              "         99: 1,\n",
              "         103: 1,\n",
              "         127: 1,\n",
              "         191: 1,\n",
              "         192: 1,\n",
              "         219: 1,\n",
              "         228: 1,\n",
              "         245: 1,\n",
              "         269: 1,\n",
              "         273: 1,\n",
              "         282: 1,\n",
              "         355: 1,\n",
              "         492: 1,\n",
              "         517: 1,\n",
              "         534: 1,\n",
              "         616: 1,\n",
              "         796: 1,\n",
              "         867: 1,\n",
              "         935: 1,\n",
              "         1144: 1,\n",
              "         1145: 1,\n",
              "         1213: 1,\n",
              "         1360: 1,\n",
              "         1361: 1,\n",
              "         1447: 1,\n",
              "         1800: 1,\n",
              "         1801: 1,\n",
              "         2417: 1,\n",
              "         2418: 1,\n",
              "         2738: 1,\n",
              "         3776: 1,\n",
              "         3777: 1,\n",
              "         3778: 1,\n",
              "         3779: 1,\n",
              "         4619: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8GKTcprNvmY",
        "colab_type": "text"
      },
      "source": [
        "j_indices.extend(feature_counter.keys())\n",
        "\n",
        "values.extend(feature_counter.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM-aSxImNbRQ",
        "colab_type": "text"
      },
      "source": [
        "The adding them on to kind of coordinates that we want to store.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUqwd6OYM16f",
        "colab_type": "code",
        "outputId": "3af2d545-3ca5-4ead-b48a-94c6468fca51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "val_term_doc = get_term_doc_matrix(movie_reviews.valid.x,len(movie_reviews.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 51 ms, sys: 8.76 ms, total: 59.8 ms\n",
            "Wall time: 55.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LJvDRyBQjj2",
        "colab_type": "code",
        "outputId": "7dd66b52-db41-44e4-92a8-923b9cc5d507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "trn_term_doc = get_term_doc_matrix(movie_reviews.train.x,len(movie_reviews.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 212 ms, sys: 20.9 ms, total: 233 ms\n",
            "Wall time: 214 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8-ZCAtSQrhG",
        "colab_type": "code",
        "outputId": "fede1587-ca9c-431b-d6f7-d24602907aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "trn_term_doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x6008 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 112405 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WXzpfhBQ8Le",
        "colab_type": "code",
        "outputId": "06cc4274-dc0c-4174-f412-8476de7fc7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "trn_term_doc[:10,:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<10x10 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 45 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhgmnN27RVsM",
        "colab_type": "text"
      },
      "source": [
        "In 10x10 section of the matrix, there are 55 spot are 0 while 45 spot have values . If we want to see it in full, we can convert sparse matrix to dense matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGtNoXJdRSLv",
        "colab_type": "code",
        "outputId": "b0a19f3a-caad-4457-da49-1f7b174fe7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "trn_term_doc.todense()[:10,:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 8,  0,  1,  0, ...,  0,  0,  0,  2],\n",
              "        [22,  0,  1,  0, ...,  2,  0,  0, 27],\n",
              "        [ 4,  0,  1,  0, ...,  2,  0,  0,  5],\n",
              "        [13,  0,  1,  0, ...,  0,  0,  0, 16],\n",
              "        ...,\n",
              "        [ 4,  0,  1,  0, ...,  0,  0,  0, 19],\n",
              "        [42,  0,  1,  0, ..., 14,  0,  0, 30],\n",
              "        [18,  0,  1,  0, ...,  0,  0,  0, 15],\n",
              "        [20,  0,  1,  0, ...,  1,  0,  0, 10]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Vtr0P6R_CG",
        "colab_type": "code",
        "outputId": "9ece9569-4896-47de-8d35-99eff91b0e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "trn_term_doc[:10,-10:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<10x10 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 0 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41MXexfsScQ-",
        "colab_type": "text"
      },
      "source": [
        "In this case, there are zero elements stored in the first 10 entries but the last ten vocabulary words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn57e3feSWdo",
        "colab_type": "code",
        "outputId": "bc7e1e97-33c6-4418-8b82-245d52314f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_term_doc.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 6008)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aguf8C2STWd8",
        "colab_type": "text"
      },
      "source": [
        "### More data exploration\n",
        "\n",
        "We could convert our sparse matrix to a dense matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrK71EzUTVHQ",
        "colab_type": "code",
        "outputId": "7965fc45-47de-4f60-e676-aa5edc758261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.vocab.itos[:4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk', 'xxpad', 'xxbos', 'xxeos']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eyEOpCdTna_",
        "colab_type": "code",
        "outputId": "6049cee0-5433-4cdf-9916-809bac45ba1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "val_term_doc.todense()[:10,:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[32,  0,  1,  0, ...,  1,  0,  0, 10],\n",
              "        [ 9,  0,  1,  0, ...,  1,  0,  0,  7],\n",
              "        [ 6,  0,  1,  0, ...,  0,  0,  0, 12],\n",
              "        [78,  0,  1,  0, ...,  0,  0,  0, 44],\n",
              "        ...,\n",
              "        [ 8,  0,  1,  0, ...,  0,  0,  0,  8],\n",
              "        [43,  0,  1,  0, ...,  8,  1,  0, 25],\n",
              "        [ 7,  0,  1,  0, ...,  1,  0,  0,  9],\n",
              "        [19,  0,  1,  0, ...,  2,  0,  0,  5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgIz4thBU1cj",
        "colab_type": "text"
      },
      "source": [
        "### Excercise:\n",
        "\n",
        "[Video 5](https://youtu.be/dt7sArnLo1g?t=487)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2pKcFUQU5QB",
        "colab_type": "code",
        "outputId": "67860425-d875-4c4a-c98e-2a6da6130b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "review = movie_reviews.valid.x[1]; review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos i saw this movie once as a kid on the late - late show and fell in love with it . \n",
              " \n",
              "  xxmaj it took 30 + years , but i recently did find it on xxup dvd - it was n't cheap , either - in a xxunk that xxunk in war movies . xxmaj we watched it last night for the first time . xxmaj the audio was good , however it was grainy and had the trailers between xxunk . xxmaj even so , it was better than i remembered it . i was also impressed at how true it was to the play . \n",
              " \n",
              "  xxmaj the xxunk is around here xxunk . xxmaj if you 're xxunk in finding it , fire me a xxunk and i 'll see if i can get you the xxunk . xxunk"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWhn8kQ-UyKR",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Since the word \"late\" shows up twice in this review (\"...as a kid on the late - late show...\"), confirm that a value of 2 is stored in the term-document matrix, for the row corresponding to this review and the column corresponding to the word \"late\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbnepB1eTz0b",
        "colab_type": "code",
        "outputId": "0bda13ad-fc54-495f-b818-69b763bb897c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#first, look up the integer of 'late'\n",
        "print(movie_reviews.vocab.stoi['late'])\n",
        "\n",
        "# then use term doc to find the values\n",
        "\n",
        "val_term_doc[1,451]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQcFGBrkYudJ",
        "colab_type": "text"
      },
      "source": [
        "This review has total 144 tokens and it has 81 distinct tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCBiyNQNYKaT",
        "colab_type": "code",
        "outputId": "f2402443-944f-429c-d042-96dd3ca6995c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "val_term_doc[1].sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KazMVl4YtvX",
        "colab_type": "code",
        "outputId": "f35105e3-4ad4-4a09-bbdb-c0b01d7428da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "val_term_doc[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x6008 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 81 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH0A32F8ZLvs",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** How could you convert review.data back to text (without just using review.text)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9qHVukLY_WQ",
        "colab_type": "code",
        "outputId": "0c4ef92b-0999-4ae3-f9ee-beaa31c45386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "review.data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2,  19, 248,  21, ...,   9,   0,  10,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr-urH6OZO1N",
        "colab_type": "code",
        "outputId": "09780602-ffbb-49a7-8dd4-c10d93c473c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# use the itos\n",
        "\n",
        "[movie_reviews.vocab.itos[a] for a in review.data]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxbos',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'this',\n",
              " 'movie',\n",
              " 'once',\n",
              " 'as',\n",
              " 'a',\n",
              " 'kid',\n",
              " 'on',\n",
              " 'the',\n",
              " 'late',\n",
              " '-',\n",
              " 'late',\n",
              " 'show',\n",
              " 'and',\n",
              " 'fell',\n",
              " 'in',\n",
              " 'love',\n",
              " 'with',\n",
              " 'it',\n",
              " '.',\n",
              " '\\n \\n ',\n",
              " 'xxmaj',\n",
              " 'it',\n",
              " 'took',\n",
              " '30',\n",
              " '+',\n",
              " 'years',\n",
              " ',',\n",
              " 'but',\n",
              " 'i',\n",
              " 'recently',\n",
              " 'did',\n",
              " 'find',\n",
              " 'it',\n",
              " 'on',\n",
              " 'xxup',\n",
              " 'dvd',\n",
              " '-',\n",
              " 'it',\n",
              " 'was',\n",
              " \"n't\",\n",
              " 'cheap',\n",
              " ',',\n",
              " 'either',\n",
              " '-',\n",
              " 'in',\n",
              " 'a',\n",
              " 'xxunk',\n",
              " 'that',\n",
              " 'xxunk',\n",
              " 'in',\n",
              " 'war',\n",
              " 'movies',\n",
              " '.',\n",
              " 'xxmaj',\n",
              " 'we',\n",
              " 'watched',\n",
              " 'it',\n",
              " 'last',\n",
              " 'night',\n",
              " 'for',\n",
              " 'the',\n",
              " 'first',\n",
              " 'time',\n",
              " '.',\n",
              " 'xxmaj',\n",
              " 'the',\n",
              " 'audio',\n",
              " 'was',\n",
              " 'good',\n",
              " ',',\n",
              " 'however',\n",
              " 'it',\n",
              " 'was',\n",
              " 'grainy',\n",
              " 'and',\n",
              " 'had',\n",
              " 'the',\n",
              " 'trailers',\n",
              " 'between',\n",
              " 'xxunk',\n",
              " '.',\n",
              " 'xxmaj',\n",
              " 'even',\n",
              " 'so',\n",
              " ',',\n",
              " 'it',\n",
              " 'was',\n",
              " 'better',\n",
              " 'than',\n",
              " 'i',\n",
              " 'remembered',\n",
              " 'it',\n",
              " '.',\n",
              " 'i',\n",
              " 'was',\n",
              " 'also',\n",
              " 'impressed',\n",
              " 'at',\n",
              " 'how',\n",
              " 'true',\n",
              " 'it',\n",
              " 'was',\n",
              " 'to',\n",
              " 'the',\n",
              " 'play',\n",
              " '.',\n",
              " '\\n \\n ',\n",
              " 'xxmaj',\n",
              " 'the',\n",
              " 'xxunk',\n",
              " 'is',\n",
              " 'around',\n",
              " 'here',\n",
              " 'xxunk',\n",
              " '.',\n",
              " 'xxmaj',\n",
              " 'if',\n",
              " 'you',\n",
              " \"'re\",\n",
              " 'xxunk',\n",
              " 'in',\n",
              " 'finding',\n",
              " 'it',\n",
              " ',',\n",
              " 'fire',\n",
              " 'me',\n",
              " 'a',\n",
              " 'xxunk',\n",
              " 'and',\n",
              " 'i',\n",
              " \"'ll\",\n",
              " 'see',\n",
              " 'if',\n",
              " 'i',\n",
              " 'can',\n",
              " 'get',\n",
              " 'you',\n",
              " 'the',\n",
              " 'xxunk',\n",
              " '.',\n",
              " 'xxunk']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3DgdFYrZ0dh",
        "colab_type": "text"
      },
      "source": [
        "**Exercise:** Confirm that review has 81 distinct tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dGoeQxoZuOX",
        "colab_type": "code",
        "outputId": "40574f55-68f4-4f28-e75f-5759f6da1bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(set(review.data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQV3tX1WaMFI",
        "colab_type": "text"
      },
      "source": [
        "## stoi (string-to-int) is larger than itos (int-to-string) \n",
        "\n",
        "since many words are mapping to unknown. We will figure out that what words were getting mapped to unknown. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4-bOBnBaB-I",
        "colab_type": "code",
        "outputId": "a984ee41-e0dc-4e1c-dc12-bf2b1bb1e3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(movie_reviews.vocab.stoi) - len(movie_reviews.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0joQgoHiasja",
        "colab_type": "text"
      },
      "source": [
        "We write a little loop to go through the item and see if they map to unknown and then put them in a list "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH6oBzSXa4u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk = []\n",
        "for word, num in movie_reviews.vocab.stoi.items():\n",
        "    if num == 0:\n",
        "        unk.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTiLKUKKbLVz",
        "colab_type": "code",
        "outputId": "8d0d2ec2-0490-416b-d49e-42371f9bfceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(unk)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEtqZau6bNrj",
        "colab_type": "code",
        "outputId": "7b243006-cb65-420b-b075-56b69fb7f9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unk[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xxunk',\n",
              " 'bleeping',\n",
              " 'pert',\n",
              " 'ticky',\n",
              " 'schtick',\n",
              " 'whoosh',\n",
              " 'banzai',\n",
              " 'chill',\n",
              " 'wooofff',\n",
              " 'cheery',\n",
              " 'superstars',\n",
              " 'fashionable',\n",
              " 'cruelly',\n",
              " 'separating',\n",
              " 'mistreat',\n",
              " 'tensions',\n",
              " 'religions',\n",
              " 'baseness',\n",
              " 'nobility',\n",
              " 'puro',\n",
              " 'disowned',\n",
              " 'option',\n",
              " 'faults',\n",
              " 'dignified',\n",
              " 'realisation',\n",
              " 'reconciliation',\n",
              " 'mrs',\n",
              " 'iyer',\n",
              " 'heartbreaking',\n",
              " 'histories',\n",
              " 'frankness',\n",
              " 'starters',\n",
              " 'montage',\n",
              " 'swearing',\n",
              " 'halestorm',\n",
              " 'korea',\n",
              " 'concentrate',\n",
              " 'pic',\n",
              " 'elude',\n",
              " 'characteristics',\n",
              " 'blathered',\n",
              " 'brassed',\n",
              " 'declaration',\n",
              " 'peck',\n",
              " 'garnered',\n",
              " 'fearless',\n",
              " 'tempered',\n",
              " 'humane',\n",
              " 'tails',\n",
              " 'slighted',\n",
              " 'slater',\n",
              " 'barrage',\n",
              " 'underway',\n",
              " 'operating',\n",
              " 'tag',\n",
              " 'dorff',\n",
              " 'reid',\n",
              " 'continually',\n",
              " 'revel',\n",
              " 'nra',\n",
              " 'benton',\n",
              " 'slate',\n",
              " 'penal',\n",
              " 'vengeful',\n",
              " 'seed',\n",
              " 'backbone',\n",
              " 'dismal',\n",
              " 'fortunate',\n",
              " 'ds',\n",
              " 'tmob',\n",
              " 'autographed',\n",
              " 'intercepted',\n",
              " 'lectured',\n",
              " 'reprints',\n",
              " 'comicon',\n",
              " 'attendees',\n",
              " 'blackhawk',\n",
              " 'insisted',\n",
              " 'jumped',\n",
              " 'apologized',\n",
              " 'wishing',\n",
              " 'seller',\n",
              " 'abomination',\n",
              " 'crib',\n",
              " 'seriousness',\n",
              " 'reclaim',\n",
              " 'sidenotes',\n",
              " 'archenemy',\n",
              " 'simultaneous',\n",
              " 'sheet',\n",
              " 'ely',\n",
              " 'leaping',\n",
              " 'brick',\n",
              " 'blasting',\n",
              " 'pistol',\n",
              " 'duster',\n",
              " 'postscript',\n",
              " 'accompanied',\n",
              " '1975',\n",
              " 'smack']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrZ7qW0ekNZx",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "We define the **log-count ratio** $r$ for each word $f$:\n",
        "\n",
        "$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n",
        "\n",
        "where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWb1EAH7bPJk",
        "colab_type": "code",
        "outputId": "9d7c0ac5-7f33-490c-a7b4-4d48c63c432a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "movie_reviews.y.classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative', 'positive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAfNcO-JknH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc\n",
        "y = movie_reviews.train.y\n",
        "val_y = movie_reviews.valid.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpY2J-ZUk-Ak",
        "colab_type": "code",
        "outputId": "bb7713e4-64dd-40cf-b930-bb12247cd957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "positive = y.c2i['positive']\n",
        "positive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqeRusYulGZR",
        "colab_type": "code",
        "outputId": "c83997f1-cf83-4bc1-989b-a286f24e8d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "negative = y.c2i['negative']\n",
        "negative"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHY_MiCmlLJU",
        "colab_type": "code",
        "outputId": "35a63ceb-f8bf-4cfd-8b32-7e6e8ee00a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7154,    0,  417,    0, ...,    0,    3,    3,    3], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KswBgppglWKo",
        "colab_type": "code",
        "outputId": "147a2b0d-b341-4370-924e-f2aee8425f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.squeeze(x[y.items==positive].sum(0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[6471,    0,  383,    0, ...,    3,    0,    0,    0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLTmdu7inNZE",
        "colab_type": "code",
        "outputId": "39aafb9d-3dfe-49e8-98a2-08ecfd8e83f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6471,    0,  383,    0, ...,    3,    0,    0,    0], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2eMihpnpMA",
        "colab_type": "text"
      },
      "source": [
        "For each word in our vocabulary, we are summing up how many positive reviews it is in, and how many negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izzqm1y3ng1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahxjN6uCn-JW",
        "colab_type": "code",
        "outputId": "ce1edd6a-57de-469d-cf17-a93cfd2df217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p1[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6471,     0,   383,     0,     0, 10267,   674,    57,     0,  5260], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKF-JzkAn_Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = movie_reviews.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyasYcRloDSL",
        "colab_type": "code",
        "outputId": "1bae61f9-6907-468b-998e-468ba5c51ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(p1),len(p0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6008, 6008)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGVmXwhUoGQo",
        "colab_type": "code",
        "outputId": "e462727a-1594-42ed-d2d5-f07482539283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v.itos[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxunk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7W3RG31oMh0",
        "colab_type": "text"
      },
      "source": [
        "### Using our ratios for even more data exploration\n",
        "\n",
        "We can use p0 and p1 to do some more data exploration!\n",
        "\n",
        "**Exercise:** compare how often \"loved\" appears in positive reviews vs. negative reviews. How about \"hate\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USbKF4FRoIIn",
        "colab_type": "code",
        "outputId": "cbb7d696-690c-4efd-c7cd-262810f06425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Exercise: How often does the word \"loved\" appear in neg vs. pos reviews?\n",
        "p0[v.stoi['loved']],p1[v.stoi['loved']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 29)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYJ7kICso_K2",
        "colab_type": "text"
      },
      "source": [
        "\"loved\" is shown more often in postive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkv_xlvbom4_",
        "colab_type": "code",
        "outputId": "19c025bf-f8fb-448d-9490-8898739de33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Exercise: How often does the word \"hated\" appear in neg vs. pos reviews?\n",
        "p0[v.stoi['hated']],p1[v.stoi['hated']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg9lLLzLpFvw",
        "colab_type": "text"
      },
      "source": [
        "\"hated\" is shown more often in negative review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHV_5sCIpUPk",
        "colab_type": "text"
      },
      "source": [
        "#### Let's figure out why positive reviews with the word \"hated\"\n",
        "\n",
        "I was curious to look at an example of a postive review with the word \"hated\" in it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie3w8SLzowXM",
        "colab_type": "code",
        "outputId": "e1505452-4aa0-45d8-b9aa-b19c68b633a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v.stoi['hated']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdowJkQFp1Hw",
        "colab_type": "text"
      },
      "source": [
        "We use **np.argwhere** to find which reviews have a nonzero value for that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT4LqvXWpgw9",
        "colab_type": "code",
        "outputId": "b87a4582-1cfa-4f05-e276-990aedca0367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = np.argwhere((x[:,1977]>0))[:,0];a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15,  49, 304, 351, 393, 612, 695, 773], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqUBQ23-qGin",
        "colab_type": "text"
      },
      "source": [
        "Then I get all the postive reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSX2FRjwqFk2",
        "colab_type": "code",
        "outputId": "89a51019-be2c-4d98-8fae-02d46e638de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = np.argwhere(y.items==positive)[:,0];b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   3,  10,  11, ..., 787, 789, 790, 797])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o7nqE95qYbr",
        "colab_type": "text"
      },
      "source": [
        "Now we take a set intersection of those to fit things. So we get all reviews that have the word \"hated\". So \"hated\" appears in 3 reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XamcBma3qWbM",
        "colab_type": "code",
        "outputId": "5434dcbc-6a2b-4453-a164-a8804de2aeb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set(a).intersection(set(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{393, 612, 695}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Z3DPoxqvki",
        "colab_type": "code",
        "outputId": "c3b51957-8289-40a4-ffd1-6f01e47065a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "review = movie_reviews.train.x[695]\n",
        "review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj xxunk , yeah this episode is extremely underrated . \n",
              " \n",
              "  xxmaj even though there is a xxup lot of bad writing and acting at parts . i think the good over wins the bad . \n",
              " \n",
              "  i love the xxunk parts and the big ' twist ' at the end . i absolutely love that scene when xxmaj michelle xxunk xxmaj tony . xxmaj it 's actually one of my favorite scenes of xxmaj season 1 . \n",
              " \n",
              "  xxmaj for some reason , people have always hated the xxmaj xxunk episodes , yet i have always liked them . xxmaj they 're not the best , in terms of writing . but the theme really does interest me , \n",
              " \n",
              "  i 'm gon na give it a xxup three star , but if the writing were a little more consistent i 'd give it xxup four ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnUctkf3rA3X",
        "colab_type": "text"
      },
      "source": [
        "---->`people have always hated the xxmaj xxunk episodes`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-aYNGlKrYon",
        "colab_type": "text"
      },
      "source": [
        "#### negative reviews with the word \"loved\"\n",
        "\n",
        "Now, let's look at an example of a negative review that contains the word \"loved\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9dSzyRSq1dp",
        "colab_type": "code",
        "outputId": "4ec0d399-f17d-4826-bcc9-5adbe52535f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v.stoi['loved']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "535"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNZyrHezreRv",
        "colab_type": "code",
        "outputId": "ea154917-392f-4008-ef4a-ab2e6cd5ca0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "a = np.argwhere((x[:,534]>0))[:,0];a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,  19,  24,  51,  61,  70,  81, 110, 123, 155, 175, 193, 221, 265, 274, 279, 284, 290, 295, 304, 360, 384,\n",
              "       421, 465, 516, 520, 548, 569, 588, 604, 620, 631, 661, 672, 679, 702, 709, 759, 764, 792], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ3bEXwnruFs",
        "colab_type": "code",
        "outputId": "098222e4-1d05-4247-e996-d434c614ca28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = np.argwhere(y.items==negative)[:,0];b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   2,   4,   5, ..., 795, 796, 798, 799])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6edHrAW-r3Il",
        "colab_type": "code",
        "outputId": "e1010a18-ac17-4652-9bf1-776cf3995a82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        }
      },
      "source": [
        "set(a).intersection(set(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0,\n",
              " 24,\n",
              " 51,\n",
              " 70,\n",
              " 81,\n",
              " 123,\n",
              " 155,\n",
              " 193,\n",
              " 221,\n",
              " 274,\n",
              " 279,\n",
              " 284,\n",
              " 290,\n",
              " 295,\n",
              " 304,\n",
              " 421,\n",
              " 516,\n",
              " 548,\n",
              " 604,\n",
              " 620,\n",
              " 631,\n",
              " 672,\n",
              " 679,\n",
              " 709,\n",
              " 759,\n",
              " 764,\n",
              " 792}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGzraZm9r8Jx",
        "colab_type": "code",
        "outputId": "0d542109-81bb-4ff7-8de8-498d3493d8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "review = movie_reviews.train.x[792]\n",
        "review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj this is not really a zombie film , if we 're xxunk zombies as the dead walking around . xxmaj here the protagonist , xxmaj xxunk xxmaj louque ( played by an xxunk young xxmaj dean xxmaj xxunk ) , xxunk control of a method to create zombies , though in fact , his ' method ' is to mentally project his thoughts and control other living people 's minds turning them into xxunk slaves . xxmaj this is an interesting concept for a movie , and was done much more effectively by xxmaj xxunk xxmaj lang in his series of ' xxmaj dr. xxmaj mabuse ' films , including ' xxmaj dr. xxmaj mabuse the xxmaj xxunk ' ( xxunk ) and ' xxmaj the xxmaj testament of xxmaj dr. xxmaj mabuse ' ( 1933 ) . xxmaj here it is unfortunately xxunk to his quest to regain the love of his former fiancée , xxmaj claire xxmaj duvall ( played by the xxmaj anne xxmaj xxunk look alike with a bad xxunk , xxmaj dorothy xxmaj stone ) which is really the major theme . \n",
              " \n",
              "  xxmaj the movie has an intriguing beginning , as xxmaj louque is sent on a military xxunk expedition to xxmaj xxunk to end the cult of zombies that came from there . xxmaj at some type of compound ( where we get great 30s sets and clothes ) he xxunk his xxunk to xxmaj claire , and then barely five minutes later , she gives him back his ring xxunk her love for his pal , xxmaj xxunk xxmaj greyson ( xxmaj robert xxmaj xxunk ) . xxmaj it 's unintentionally funny the way they talk to each other without making eye contact . xxmaj this would have been a great movie for ' xxmaj mystery xxmaj science xxmaj theater xxunk ' , if they had n't already xxunk it . \n",
              " \n",
              "  xxmaj it 's never shown how xxmaj louque actually learns the ' xxunk ' secret , but he then uses it to kill his enemies , create a giant army of xxunk carrying soldiers and body guards . xxmaj we wo n't see such sheer force of will until xxmaj john xxmaj xxunk in ' xxmaj the xxmaj brain xxmaj from xxmaj planet xxmaj xxunk ' ( xxunk ) . \n",
              " \n",
              "  xxmaj finally xxmaj claire xxunk to marry him if he will let xxmaj greyson live and return to xxmaj america . xxmaj louque agrees , but actually turns him into one of his xxunk slaves . xxmaj on their wedding night he realizes that xxmaj claire will only begin to love him if he gives up his ' powers . ' xxmaj to gain her love , he does so , causing the ' revolt ' of the title , in which all his slaves xxunk and attack his compound and kill him . xxmaj greyson xxunk xxmaj claire , and we seem to be at the end of a parable : \" xxmaj whom the xxunk would destroy , they first make mad . \" \n",
              " \n",
              "  xxmaj so really then , it 's not that bad of a film , despite the low imdb rating it currently has . xxmaj on repeated viewings ( ? ) one can see the xxunk in the well formed script ! xxmaj dean xxmaj xxunk had yet to develop into a good actor , and is almost unrecognizable in his xxunk -- is that really his own hair ? xxmaj we remember him more for his xxunk , old man roles in ' xxmaj white xxmaj christmas ' ( xxunk ) , ' x xxmaj the xxmaj unknown ' ( 1956 ) and ' xxmaj king xxmaj xxunk ' ( 1958 ) . xxmaj the story xxunk a lot of its basic themes from the xxmaj xxunk brothers better , earlier film ' xxmaj white xxmaj zombie ' ( xxunk ) in which xxunk xxmaj robert xxmaj xxunk ( as xxmaj charles xxmaj xxunk ) uses ' xxunk ' to win the love of xxmaj xxunk xxmaj xxunk ( as xxmaj xxunk xxmaj parker ) . \n",
              " \n",
              "  xxmaj if you want real zombie movies ( of which there are hundreds ! ) i 'd start with ' xxmaj white xxmaj zombie ' ( xxunk ) , ' xxmaj king of the xxmaj zombies ' ( xxunk ) , ' i xxmaj walked with a xxmaj zombie ' ( xxunk ) , ' xxmaj night of the xxmaj living xxmaj dead ' ( xxunk ) , ' xxmaj the xxmaj last xxmaj man on xxmaj earth ' ( 1964 ) and its two xxunk . xxmaj in the modern era of classy films , there are ' xxmaj horror xxmaj express ' ( 1972 ) , ' xxmaj the xxmaj xxunk and the xxmaj xxunk ' ( xxunk ) , ' 28 xxmaj days xxmaj later ' ( 2002 ) and its sequel , as well as many , many , others too numerous to mention . \n",
              " \n",
              "  xxmaj this one is not really a zombie film . xxmaj judging this movie on its own terms , it 's more of a semi - xxmaj gothic romance . xxmaj as such it ranks a little below some of xxmaj universal 's bottom billed b horror movies of the late 30s and early xxunk . xxmaj so i 'll give it a 5 ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmXBitCs_ET",
        "colab_type": "text"
      },
      "source": [
        "### Applying Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf79Gw4dsDq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTdbGktOwLG_",
        "colab_type": "text"
      },
      "source": [
        "We do averaging over the number of positive reviews (pr1), averaging over the number of negative reviews(pr0) . The reason why we add 1 to numerator and denominator is to help with numerical stability for these formulars"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbfCH0DytRF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1)/((y.items==positive).sum()+1)\n",
        "pr0 = (p0+1)/((y.items==negative).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc-0jrD9w5Lw",
        "colab_type": "text"
      },
      "source": [
        "We take log of the ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og7EbslutgLi",
        "colab_type": "code",
        "outputId": "b5629a83-9f56-40bc-bc0d-9e8bfc5f6ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r = np.log(pr1/pr0) ; r"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.015487,  0.084839,  0.      ,  0.084839, ...,  1.471133, -1.301455, -1.301455, -1.301455])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA5rBRs1t0JC",
        "colab_type": "text"
      },
      "source": [
        "#### Vocab most likely associated with positive/negative reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EReOEbWhxJ6H",
        "colab_type": "text"
      },
      "source": [
        "the np.argpartition is not perfectly sort things but it still can give you the number you request . Create a partition so we want to get the top 10, the 10 biggest things and the 10 smallest things.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIoYP0rstx5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biggest = np.argpartition(r,-10)[-10:]\n",
        "smallest = np.argpartition(r,10)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYoliAtixnV1",
        "colab_type": "text"
      },
      "source": [
        "These are the words that most kind that a review is positive or that a review is negative \n",
        "\n",
        "**Most positive words:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi9fsPAJxmQD",
        "colab_type": "code",
        "outputId": "f0bb2c0d-963e-4bf3-a7ed-1a4330399e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "[v.itos[k] for k in biggest]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sport',\n",
              " 'davies',\n",
              " 'gilliam',\n",
              " 'fanfan',\n",
              " 'biko',\n",
              " 'felix',\n",
              " 'noir',\n",
              " 'jabba',\n",
              " 'astaire',\n",
              " 'jimmy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DclDXJXLx8VG",
        "colab_type": "text"
      },
      "source": [
        "Then I look up \"biko\" which is a positive word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbFmIn1tx4XX",
        "colab_type": "code",
        "outputId": "982bc878-e15e-4871-935e-336aaf40e3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(trn_term_doc[:,v.stoi['biko']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf5d80c6yLTP",
        "colab_type": "code",
        "outputId": "f7814cfc-35f3-4c92-e80a-2014a4876c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "movie_reviews.train.x[515]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos \" xxmaj the xxmaj true xxmaj story xxmaj of xxmaj the xxmaj friendship xxmaj that xxmaj shook xxmaj south xxmaj africa xxmaj and xxmaj xxunk xxmaj the xxmaj world . \" \n",
              " \n",
              "  xxmaj richard xxmaj attenborough , who directed \" a xxmaj bridge xxmaj too xxmaj far \" and \" xxmaj gandhi \" , wanted to bring the story of xxmaj steve xxmaj biko to life , and the journey and trouble that xxunk xxmaj donald xxmaj woods went through in order to get his story told . xxmaj the films uses xxmaj wood 's two books for it 's information and basis - \" xxmaj biko \" and \" xxmaj asking for xxmaj trouble \" . \n",
              " \n",
              "  xxmaj the film takes place in the late 1970 's , in xxmaj south xxmaj africa . xxmaj south xxmaj africa is in the grip of the terrible apartheid , which keeps the blacks separated from the whites and xxunk the whites as the superior race . xxmaj the blacks are forced to live in xxunk on the xxunk of the cities and xxunk , and they come under frequent xxunk by the police and the army . xxmaj we are shown a dawn xxunk on a xxunk , as xxunk and armed police force their way through the camp beating and even killing the inhabitants . xxmaj then we are introduced to xxmaj donald xxmaj woods ( xxmaj kevin xxmaj kline ) , who is the editor of a popular newspaper . xxmaj after xxunk a negative story about black xxunk xxmaj steve xxmaj biko ( xxmaj denzel xxmaj washington ) , xxmaj woods goes to meet with him . xxmaj the two are xxunk of each other at first , but they soon become good friends and xxmaj biko shows the horrors of the apartheid system from a black persons point of view to xxmaj woods . xxmaj this xxunk xxmaj woods to speak out against what 's happening around him , and makes him desperate to bring xxmaj steve xxmaj biko 's story out of the xxunk of the white man 's xxmaj south xxmaj africa and to the world . xxmaj soon , xxmaj steve xxmaj biko is arrested and is killed in prison . xxmaj now xxmaj woods and his family are daring to escape from xxmaj south xxmaj africa to xxmaj england , where xxmaj woods can xxunk his book about xxmaj steve xxmaj biko and the apartheid . \n",
              " \n",
              "  xxmaj when i first heard of \" xxmaj cry xxmaj freedom \" , i was under the impression that it was a movie completely dedicated to the life of xxmaj steve xxmaj biko . i had never actually heard of xxmaj steve xxmaj biko before i seen this film , as the events in this film were really before my time . xxmaj but it 's more about the story of xxmaj donald xxmaj woods and his journey across the border into xxmaj xxunk as he tried to xxunk the xxmaj south xxmaj african xxunk . xxmaj woods was put on a five year type house xxunk after xxmaj steve xxmaj biko was killed . xxmaj so in order to xxunk his xxunk on xxmaj steve xxmaj biko , he had to escape . xxmaj because the xxunk would be considered xxunk in xxmaj south xxmaj africa and that could have resulted in xxmaj woods meeting a fate similar to that of xxmaj biko 's . xxmaj the real xxmaj donald xxmaj woods and his wife acted as xxunk to this film . \n",
              " \n",
              "  xxmaj denzel xxmaj washington is only in the film for the first hour , and i was disappointed with that as i was expecting to see him for the entire movie . xxmaj but he was amazing as xxmaj steve xxmaj biko , and captured his personality from what i 've read really well and his accent sounded perfect . xxmaj his performance earned him an xxmaj oscar nomination for xxmaj best xxmaj supporting xxmaj actor . xxmaj kevin xxmaj kline delivers a excellent and thought - xxunk performance as xxmaj donald xxmaj woods , and xxmaj penelope xxmaj xxunk is excellent as his wife xxmaj xxunk . \n",
              " \n",
              "  xxmaj filming took place in xxmaj xxunk , as needless to say problems xxunk when they tried to film it in xxmaj south xxmaj africa . xxmaj while in xxmaj south xxmaj africa , the xxmaj south xxmaj african xxunk followed the film crew everywhere , so they got the bad xxunk and they pulled out and went to xxunk xxmaj xxunk instead . xxmaj despite everything , and the fact that the apartheid did n't end ' xxunk seven years later , \" xxmaj cry xxmaj freedom \" was n't xxunk in xxmaj south xxmaj africa . xxmaj but xxunk showing the movie received bomb threats . \n",
              " \n",
              "  xxmaj richard xxmaj attenborough brings the horrors of the apartheid to the screen with extreme force and determination . xxmaj he does n't hold back at the end of the movie when showing what was supposed to be a xxunk xxunk by students in a xxunk , turns into a massacre when police open fire on them . xxmaj the film ends with the names of all the anti - apartheid xxunk who died in prison , and the explanations for their deaths . xxmaj many had \" xxmaj no xxmaj explanation \" . xxmaj quite a few were \" xxmaj xxunk \" , which is hard to believe , and many more either fell from the top of the xxunk or were \" xxmaj suicide from xxmaj hanging \" . xxmaj no one will ever know what really happened to them , but i think it 's fair to say that none of these men died at their own hands , but at the hands of others ; or to be more xxunk , at the hands of the police . \n",
              " \n",
              "  \" xxmaj cry xxmaj freedom \" is a must - see movie for it 's portrayal and story of xxmaj steve xxmaj biko . xxmaj it 's also a xxunk and xxunk portrayal of a beautiful land divided and in the xxunk grips of racial xxunk and violence ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtroOUbrywPy",
        "colab_type": "text"
      },
      "source": [
        "**Most negative words:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz-J80UlyXNw",
        "colab_type": "code",
        "outputId": "6233f426-31b6-4201-cdcf-9ae2239f8ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "[v.itos[k] for k in smallest]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['worst',\n",
              " 'crap',\n",
              " 'crater',\n",
              " 'porn',\n",
              " 'disappointment',\n",
              " 'dog',\n",
              " 'vargas',\n",
              " 'naschy',\n",
              " 'fuqua',\n",
              " 'soderbergh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24HQbeAy2gM",
        "colab_type": "code",
        "outputId": "832ef7e6-aab3-4780-fb3f-d18e39de68d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.argmax(trn_term_doc[:,v.stoi['soderbergh']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "434"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmNQ-KI_zIUl",
        "colab_type": "code",
        "outputId": "f2fc28da-c316-4a00-cd44-aa7add5a997f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        }
      },
      "source": [
        "movie_reviews.train.x[434]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \n",
              " \n",
              "  xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after xxunk years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \n",
              " \n",
              "  xxmaj none of this excuses him this present , almost diabolical failure . xxmaj as xxmaj david xxmaj xxunk xxunk , \" two parts of xxmaj che do n't ( even ) make a whole \" . \n",
              " \n",
              "  xxmaj epic xxunk in name only , xxmaj che(2008 ) barely qualifies as a feature film ! xxmaj it certainly has no legs , xxunk as except for its xxunk ultimate resolution forced upon it by history , xxmaj soderbergh 's xxunk - long xxunk just goes nowhere . \n",
              " \n",
              "  xxmaj even xxmaj margaret xxmaj xxunk , the more xxunk of xxmaj australia 's xxmaj at xxmaj the xxmaj movies duo , noted about xxmaj soderbergh 's xxunk waste of ( xxup xxunk digital xxunk ) : \" you 're in the woods ... xxunk in the woods ... xxunk in the woods ... \" . i too am surprised xxmaj soderbergh did n't give us another xxunk of xxup that somewhere between his xxunk two xxmaj parts , because he still left out massive xxunk of xxmaj che 's \" xxunk \" life ! \n",
              " \n",
              "  xxmaj for a xxunk of an important but infamous historical figure , xxmaj soderbergh xxunk xxunk , if not deliberately insults , his audiences by \n",
              " \n",
              "  1 . never providing most of xxmaj che 's story ; \n",
              " \n",
              "  2 . xxunk xxunk film xxunk with mere xxunk xxunk ; \n",
              " \n",
              "  3 . xxunk both true xxunk and a narrative of events ; \n",
              " \n",
              "  4 . barely developing an idea , or a character ; \n",
              " \n",
              "  5 . remaining xxunk episodic ; \n",
              " \n",
              "  6 . xxunk proper context for scenes --- whatever we do get is xxunk in xxunk xxunk ; \n",
              " \n",
              "  7 . xxunk xxunk all audiences ( even xxmaj spanish - xxunk will be confused by the xxunk xxunk in xxmaj english ) ; and \n",
              " \n",
              "  8 . xxunk xxunk his main subject into one dimension . xxmaj why , at xxup this late stage ? xxmaj the t - shirt franchise has been a success ! \n",
              " \n",
              "  xxmaj our sense of xxunk is surely due to xxmaj peter xxmaj xxunk and xxmaj benjamin xxunk xxmaj xxunk xxunk their screenplay solely on xxmaj xxunk 's memoirs . xxmaj so , like a poor student who has read only xxup one of his xxunk xxunk for his xxunk , xxmaj soderbergh 's product is xxunk limited in perspective . \n",
              " \n",
              "  xxmaj the audience is held captive within the same xxunk knowledge , scenery and circumstances of the \" revolutionaries \" , but that does n't xxunk our sympathy . xxmaj instead , it xxunk on us that \" xxmaj ah , xxmaj soderbergh 's trying to xxunk his audiences the same as the xxmaj latino peasants were at the time \" . xxmaj but these are the xxup same illiterate xxmaj latino peasants who xxunk out the good doctor to his enemies . xxmaj why does xxmaj soderbergh feel the need to xxunk us with them , and keep us equally mentally captive ? xxmaj such audience xxunk must have a purpose . \n",
              " \n",
              "  xxmaj part2 is more xxunk than xxmaj part1 , but it 's literally mind - numbing with its repetitive bush - bashing , misery of xxunk , and lack of variety or character xxunk . deltoro 's xxmaj che has no opportunity to grow as a person while he struggles to xxunk his own ill - xxunk troops . xxmaj the only xxunk is the humour as xxmaj che deals with his sometimes deeply ignorant \" revolutionaries \" , some of whom xxunk lack self - control around local peasants or food . xxmaj we certainly get no insight into what caused the conditions , nor any xxunk xxunk of their xxunk xxunk , such as it was . \n",
              " \n",
              "  xxmaj part2 's xxunk xxunk remains xxunk episodic : again , nothing is telegraphed or xxunk . xxmaj thus even the scenes with xxmaj xxunk xxmaj xxunk ( xxmaj xxunk xxmaj xxunk ) are unexpected and disconcerting . xxmaj any xxunk events are portrayed xxunk and xxmaj latino - xxunk , with xxmaj part1 's interviews xxunk by time - xxunk xxunk between the corrupt xxmaj xxunk president ( xxmaj xxunk de xxmaj xxunk ) and xxup us xxmaj government xxunk promising xxup cia xxunk ( ! ) . \n",
              " \n",
              "  xxmaj the rest of xxmaj part2 's \" woods \" and day - for - night blue xxunk just xxunk the audience until they 're xxunk the xxunk . \n",
              " \n",
              "  xxmaj perhaps deltoro felt too xxunk the frustration of many non - xxmaj american xxmaj latinos about never getting a truthful , xxunk history of xxmaj che 's xxunk within their own countries . xxmaj when foreign xxunk still wo n't deliver a free press to their people -- for whatever reason -- then one can see how a popular xxmaj american indie producer might set out to xxunk the not - so - well - read ( \" i may not be able to read or write , but i 'm xxup not xxunk . xxmaj the xxmaj inspector xxmaj xxunk ) ) out to their own local xxunk . xxmaj the film 's obvious xxunk and gross over - xxunk hint very strongly that it 's aiming only at the xxunk of the less - informed xxup who xxup still xxup speak xxup little xxmaj english . xxmaj if they did , they 'd have read xxunk on the subject already , and xxunk the relevant social issues amongst themselves -- learning the lessons of history as they should . \n",
              " \n",
              "  xxmaj such insights are precisely what societies still need -- and not just the remaining illiterate xxmaj latinos of xxmaj central and xxmaj south xxmaj america -- yet it 's what xxmaj che(2008 ) xxunk fails to deliver . xxmaj soderbergh xxunk his lead because he 's weak on narrative . i am xxunk why xxmaj xxunk deltoro deliberately chose xxmaj soderbergh for this project if he knew this . xxmaj it 's been xxunk , xxunk about xxmaj xxunk was xxunk wanted : it 's what i went to see this film for , but the director xxunk robs us of that . \n",
              " \n",
              "  xxmaj david xxmaj xxunk , writing in xxmaj the xxmaj australian ( xxunk ) observed that while xxmaj part1 was \" uneven \" , xxmaj part2 actually \" goes rapidly downhill \" from there , \" xxunk xxmaj che 's final xxunk in xxmaj xxunk in xxunk detail \" , which \" ... feels almost unbearably slow and turgid \" . \n",
              " \n",
              "  xxmaj che : xxmaj the xxmaj xxunk aka xxmaj part2 is certainly no xxunk for xxmaj xxunk , painting it a picture of misery and xxunk . xxmaj the entire second half is only xxunk by the aforementioned humour , and the dramatic -- yet tragic -- capture and execution of the film 's subject . \n",
              " \n",
              "  xxmaj the rest of this xxunk cinema xxunk is just confusing , irritating misery -- xxunk , for a xxmaj soderbergh film , to be avoided at all costs . xxmaj it is bound to break the hearts of all who know even just a xxunk about the xxunk / 10 )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNxNMQ9_zLI4",
        "colab_type": "code",
        "outputId": "7906cecb-54f7-44e0-ec89-4a1281a9a7ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "trn_term_doc[:,v.stoi[\"soderbergh\"]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x1 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p_ri2HVzck9",
        "colab_type": "text"
      },
      "source": [
        "as find out, \"Soderbergh\" is only showing up in one of the reviews so it is probably not more informative to do on our full dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ESD1050e9W",
        "colab_type": "text"
      },
      "source": [
        "### Continuing with Naive Bayes¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8_0qMLhzayz",
        "colab_type": "code",
        "outputId": "48c13fb0-6efb-407b-8f0a-df39c8113902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(y.items == positive).mean(),(y.items==negative).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.47875, 0.52125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKvClmuu1Acf",
        "colab_type": "text"
      },
      "source": [
        "b is the percentage of the reviews are positive versus negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDmzZg_G0o5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items ==positive).mean()/(y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwqiPVxQ1Hs-",
        "colab_type": "text"
      },
      "source": [
        "Now we get our predictions by taking the validation term document x r +b . The reason it is greater than 0 because it's predicting positive less than 0 predicting negative . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3IqGGkz006g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = (val_term_doc @ r +b)>0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5D4-xY-06B7",
        "colab_type": "code",
        "outputId": "1fee48d8-771c-44ea-bd1b-1d75cfcfe807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(preds == val_y.items).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQKNdOBM1lbo",
        "colab_type": "text"
      },
      "source": [
        "we got 64% accuracy which is reasonably good given that this was on a sample of our data using a pretty simple technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_olCphuiDLgD",
        "colab_type": "text"
      },
      "source": [
        "### Switching to full data set\n",
        "\n",
        "Now that we have our approach working on a smaller sample of the data, we can try using it on the full dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z1OHYOdDQsn",
        "colab_type": "text"
      },
      "source": [
        "#### Download data and process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s010G2y1kUk",
        "colab_type": "code",
        "outputId": "65404784-1d03-47e6-e82d-64699bc0b67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/imdb/test'),\n",
              " PosixPath('/root/.fastai/data/imdb/tmp_lm'),\n",
              " PosixPath('/root/.fastai/data/imdb/README'),\n",
              " PosixPath('/root/.fastai/data/imdb/unsup'),\n",
              " PosixPath('/root/.fastai/data/imdb/imdb.vocab'),\n",
              " PosixPath('/root/.fastai/data/imdb/tmp_clas'),\n",
              " PosixPath('/root/.fastai/data/imdb/train')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBR-kH7DDfHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_full = (TextList.from_folder(path)\n",
        "                .split_by_folder(valid='test')\n",
        "                .label_from_folder(classes=['neg','pos']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fQTPw1BD-DJ",
        "colab_type": "code",
        "outputId": "ec8e8a3e-2f6c-4590-d5d7-f36cf54aaae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(reviews_full.train),len(reviews_full.valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekJdqBs_EDNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = reviews_full.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPgcn8CjFehM",
        "colab_type": "code",
        "outputId": "356d49e7-b9f5-48d7-9463-228300562542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "v.itos[100:110]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bad',\n",
              " 'people',\n",
              " 'will',\n",
              " 'other',\n",
              " 'also',\n",
              " 'into',\n",
              " 'first',\n",
              " 'because',\n",
              " 'great',\n",
              " 'how']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5T9W7LGFgxj",
        "colab_type": "code",
        "outputId": "d87b1678-8143-4459-f19d-9a82ab04445f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time \n",
        "val_term_doc = get_term_doc_matrix(reviews_full.valid.x,len(reviews_full.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.8 s, sys: 250 ms, total: 6.05 s\n",
            "Wall time: 5.78 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVSP-VCFzG7",
        "colab_type": "code",
        "outputId": "d84ba3a2-a626-4fda-cca3-023edef21f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "trn_term_doc = get_term_doc_matrix(reviews_full.train.x,len(reviews_full.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.75 s, sys: 222 ms, total: 5.97 s\n",
            "Wall time: 5.75 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmvihKGNKXoN",
        "colab_type": "text"
      },
      "source": [
        "**Save data**\n",
        "\n",
        "That was slow. Let's save our matrices for faster loading next time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tTEDodGGDtG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz('trn_term_doc.npz',trn_term_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XsGX__bGFi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz('val_term_doc.npz',val_term_doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIysMVQrKtDV",
        "colab_type": "text"
      },
      "source": [
        "When storing data like this, always make sure it's included in your .gitignore file\n",
        "\n",
        "In the future, we'll just be able to load our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMAZmgtPKqja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = scipy.sparse.load_npz('trn_term_doc.npz')\n",
        "val_term_doc = scipy.sparse.load_npz('val_term_doc.npz')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmLsZbX-QWLx",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Bayes on full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq1xif_YQKdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc\n",
        "y = reviews_full.train.y\n",
        "\n",
        "val_y = reviews_full.valid.y.items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi-Qn6JaQjVM",
        "colab_type": "code",
        "outputId": "e8b0a982-3033-4c8d-eb36-1b95fd6d2ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x38456 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 3716267 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knkeAA6QQjz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))\n",
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi9_cS1KQyMq",
        "colab_type": "code",
        "outputId": "517bd503-98f5-4740-9da4-d3134794f7ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "p1[:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 28449,      0,  12500,      0,      0, 342619,  20464,   1338,      7, 173122, 138001, 143763,  89570,  83404,\n",
              "        76828,  66715,  58510,  47896,  50177,  40451], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-3YlAeQ6LV",
        "colab_type": "text"
      },
      "source": [
        "#### Data exploration: negative to positive ratios\n",
        "\n",
        "I was curious about the ratio of times a given word appears in negative reviews to times it occurs in positive reviews. Bigger ratios (> 1) mean the word is indicative of a negative review, and smaller ratios (< 1) mean it is indicative of a positive review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFp5cfTXQzYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neg_pos_given_word(word):\n",
        "    print(p0[v.stoi[word]]/p1[v.stoi[word]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_EnWpfBR-LB",
        "colab_type": "code",
        "outputId": "76342c8f-c23e-4a14-d0f0-acf47c225852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_pos_given_word('hated')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.051546391752577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rKfODbaSDnm",
        "colab_type": "code",
        "outputId": "19284990-7c3c-4123-bf0f-aad07b052990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_pos_given_word('liked')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6424702058504875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ZeLShcSRAs",
        "colab_type": "code",
        "outputId": "aa1551b4-41d2-4a92-fadb-01ee348f9d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "neg_pos_given_word('worst')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.837301587301587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT8Dk_4xSVlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1)/((y.items==positive).sum()+1)\n",
        "pr0 = (p0+1)/((y.items==negative).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z5QDClMTGrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceJh5T90TJOZ",
        "colab_type": "code",
        "outputId": "8d8a877c-3512-4d8a-bc38-0cbebb0005e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r[v.stoi['hated']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7133498878774648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY4e2N8kTfW0",
        "colab_type": "code",
        "outputId": "c7a7ee79-b0ec-4f70-aeba-398a38042e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r[v.stoi['loved']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1563661500586044"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dWGlcdSTjbS",
        "colab_type": "code",
        "outputId": "c02eb1f5-3515-47f4-8738-d182542fa463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r[v.stoi['worst']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.2826243504315076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTi7xxmdTni9",
        "colab_type": "code",
        "outputId": "1b36221e-d4ab-4484-d65f-2e094959945a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r[v.stoi['best']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7225576052173609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlLvr4EqU2e6",
        "colab_type": "text"
      },
      "source": [
        "#### Back to Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRooq-FUTsNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative = y.c2i['neg']\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq2IR01aVW4S",
        "colab_type": "text"
      },
      "source": [
        "Since we have equal numbers of positive and negative reviews in this data set, b is 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCxPCYt4VWCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJb3m8EGVee4",
        "colab_type": "code",
        "outputId": "c208794d-7035-4abe-cc7a-208269d34310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = np.log((y.items==positive).mean()/(y.items==negative).mean());b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx6CBF0pVom6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = (val_term_doc @ r + b) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLO2OUO4Vv0F",
        "colab_type": "text"
      },
      "source": [
        "Our accuracy is 80% for the full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuJA3copVvJf",
        "colab_type": "code",
        "outputId": "d2f44794-890e-43f9-9c33-03a2d83d3ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(preds == val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHus3EJNXQ1W",
        "colab_type": "text"
      },
      "source": [
        "#### Binarized Naive Bayes\n",
        "\n",
        "Another variation of Naive Bayes is Binarized Naive Bayes which maybe it only matters whether a word is in a review or not ( not the frequency of the word ). Previous we are looking at the counts of how often a word appear in a review and maybe that's not important. Instead of keeping track of \"loved\" shown ten times in a review, now it's just going to be yes or no is loved  in the review.\n",
        "\n",
        "To do that we just convert our term document matrix using **.sign()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCDwz0PwV0me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc.sign()\n",
        "y = reviews_full.train.y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zp_Dlw_bRCL",
        "colab_type": "text"
      },
      "source": [
        "Using **c2i** to convert a class into integer. In this case, the classes are \"neg\" and \"pos\" and this let us know if that's 1 or 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ahw0w-bZFyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative = y.c2i['neg']\n",
        "positive = y.c2i['pos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ2JY9UwYzU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p1 = np.squeeze(np.asarray(x[y.items==positive].sum(0)))\n",
        "p0 = np.squeeze(np.asarray(x[y.items==negative].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IPSPujWZOer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1)/((y.items==positive).sum()+1)\n",
        "pr0 = (p0+1)/((y.items==negative).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5mlXVO4ZuAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)\n",
        "b = np.log((y.items==positive).mean()/(y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IUQwMYiaRBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = (val_term_doc.sign() @ r +b) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P50tbRCDaZiq",
        "colab_type": "code",
        "outputId": "c1d0a714-6aa7-4b6a-9efb-8b0e9f055f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y9uUh3MaiPw",
        "colab_type": "text"
      },
      "source": [
        "We got 82% accuracy.\n",
        "\n",
        "Futhermore, in x , we only see 1 and 0 telling you whether or not that word in a review or not "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhpS6O5fadj2",
        "colab_type": "code",
        "outputId": "5ceecccd-54cf-40d5-d248-f1d12237caf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "x.todense()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0],\n",
              "        [1, 0, 1, 0, ..., 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyus81FXbqbG",
        "colab_type": "text"
      },
      "source": [
        "**Question** : When to use binarized vs regular naive bayes?\n",
        "\n",
        "Let's try both "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETGXmWWFb9u2",
        "colab_type": "text"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "Here is how we can fit logistic regression where the features are the **unigrams**.\n",
        "\n",
        "**ngrams** is refered to sequence of n words. Here n is 1 , it's just a single word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyLhdUFxa9l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJX-IzQBc32D",
        "colab_type": "text"
      },
      "source": [
        "to use Logistic regression, we convert y values to be integer - **y.items.astype(int)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YppAm8Rd2_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc\n",
        "y = reviews_full.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHc6MrB9cv9j",
        "colab_type": "code",
        "outputId": "eac6dbbd-ddb9-4cfc-c341-2893fcc52f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1,dual=True)\n",
        "m.fit(x,y.items.astype(int))\n",
        "preds = m.predict(val_term_doc)\n",
        "(preds==val_y).mean()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxEZRDBddjtV",
        "colab_type": "text"
      },
      "source": [
        "Now we have better accuracy with 88.2%\n",
        "\n",
        "**and with binarized version:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMW27ttRdTWz",
        "colab_type": "code",
        "outputId": "74f1544e-ac14-41ed-daf7-da214c73e652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(trn_term_doc.sign(),y.items.astype(int))\n",
        "preds = m.predict(val_term_doc.sign())\n",
        "(preds==val_y).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88536"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUZ4w96veh6_",
        "colab_type": "text"
      },
      "source": [
        "the accuracy improve to 88.5% by switching to binarized version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAF7zk6beu_-",
        "colab_type": "text"
      },
      "source": [
        "## Trigram with NB features\n",
        "\n",
        "Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwQBMTYJfXsT",
        "colab_type": "text"
      },
      "source": [
        "### ngrams\n",
        "\n",
        "An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words). A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
        "\n",
        "Here, we are referring to sequences of words. So examples of bigrams include \"the dog\", \"said that\", and \"can't you\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICCnK2UleYZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjgAAjTpfusB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews = (TextList.from_csv(path,'texts.csv',cols='text')\n",
        "                    .split_from_df(col=2)\n",
        "                    .label_from_df(cols=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf5Su7yHgBKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = movie_reviews.vocab.itos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukMMOsa-gHG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_len = len(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9X-z45KgShb",
        "colab_type": "text"
      },
      "source": [
        "###Our data\n",
        "\n",
        "Create train matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbjdQQvDgQdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_n = 1\n",
        "max_n = 3\n",
        "\n",
        "j_indices=[]\n",
        "indptr=[]\n",
        "values=[]\n",
        "indptr.append(0)\n",
        "num_tokens = vocab_len\n",
        "\n",
        "itongram = dict()\n",
        "ngramtoi = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmrx8ZuNhBIs",
        "colab_type": "text"
      },
      "source": [
        "We will iterate through the sequences of words to create our n-grams. That we have to go each review to get what are all the sequences of two words that show up. And we'll do this with nested loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgBnGPS6g-7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, doc in enumerate(movie_reviews.train.x):\n",
        "    feature_counter = Counter(doc.data)\n",
        "    j_indices.extend(feature_counter.keys())\n",
        "    values.extend(feature_counter.values())\n",
        "    this_doc_ngrams = list()\n",
        "\n",
        "    m = 0\n",
        "    for n in range(min_n, max_n + 1):\n",
        "        for k in range(vocab_len - n + 1):\n",
        "            ngram = doc.data[k: k + n]\n",
        "            if str(ngram) not in ngramtoi:\n",
        "                if len(ngram)==1:\n",
        "                    num = ngram[0]\n",
        "                    ngramtoi[str(ngram)] = num\n",
        "                    itongram[num] = ngram\n",
        "                else:\n",
        "                    ngramtoi[str(ngram)] = num_tokens\n",
        "                    itongram[num_tokens] = ngram\n",
        "                    num_tokens += 1\n",
        "            this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
        "            m += 1\n",
        "\n",
        "    ngram_counter = Counter(this_doc_ngrams)\n",
        "    j_indices.extend(ngram_counter.keys())\n",
        "    values.extend(ngram_counter.values())\n",
        "    indptr.append(len(j_indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD4eeidBiZFH",
        "colab_type": "text"
      },
      "source": [
        "Using dictionaries to convert between indices and strings (in this case, our n-grams) is a common & useful approach! Here, we have **itongram** (index to n-gram) and **ngramtoi** (n-gram to index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgRRl_B-hs6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ngram_doc_matrix = scipy.sparse.csr_matrix((values,j_indices,indptr),\n",
        "                                                shape=(len(indptr)-1,len(ngramtoi)),\n",
        "                                                dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ukB7Vpi4z1",
        "colab_type": "code",
        "outputId": "f0b40717-54b8-465a-db75-c90ded76f0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_ngram_doc_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x260374 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 678885 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmsrqD-RjiE0",
        "colab_type": "text"
      },
      "source": [
        "before we were saying term document matrix when our term are just unigram and now we got ngram document matrix for the training set . Here we note that this got 260374 tokens. There are lot of tokens because this can grow conceivably in like **n_factorial** , you know the fact that you can take all these different combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE0DpFCxkf82",
        "colab_type": "text"
      },
      "source": [
        "### Looking at our data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIvjf2NJi9QU",
        "colab_type": "code",
        "outputId": "0eb0355a-8d4d-46a0-8ca6-7f65158bf2b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ngramtoi),len(itongram)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260374, 260374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deah6iwtklpM",
        "colab_type": "code",
        "outputId": "a5213b96-36c0-45b4-9044-02d22aa7f186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "itongram[20005]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15,   9, 710])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L7ydPBDkrMu",
        "colab_type": "code",
        "outputId": "08c812cf-0d64-45e1-8307-15f3f11d1046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ngramtoi[str(np.array([15,9,710]))]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQwaet8Ykz61",
        "colab_type": "code",
        "outputId": "047bdded-26fd-495a-f92c-94f338d54500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "itongram[100000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1140,   33])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q36osDdDlDm3",
        "colab_type": "text"
      },
      "source": [
        "We randomly curious about 100000th element. It says the ngrams looks like this array has two things. So this must be bigram with two words in 1140 and 33. And the words are corresponding to \"hate\" and \"you\". These two words are next to each others. So this is one of our bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VqBC_X_lCWm",
        "colab_type": "code",
        "outputId": "5e2a635b-e85a-4680-dedf-ab80b7aaa05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v[1140],v[33]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hate', 'you')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hVWgmf5lsd1",
        "colab_type": "code",
        "outputId": "d7f0b52e-9b16-45ac-ef37-d6099aebbaef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "itongram[100010]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5430,   10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFkSIdVqmDIK",
        "colab_type": "code",
        "outputId": "d09be284-101f-4261-e83e-d6dca9f11085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v[5430],v[10]    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('photographer', '.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRthXvG9mTXn",
        "colab_type": "text"
      },
      "source": [
        "Here we find a trigram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOn3NewBmIKV",
        "colab_type": "code",
        "outputId": "52d3255a-d780-4b54-d7ba-55b7ea5ba0af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "itongram[6116]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 85, 191,  64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tIAiEumR56",
        "colab_type": "code",
        "outputId": "eadc6152-b8c9-4a58-e243-d77d534e7d0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "v[85],v[191],v[64]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('even', 'look', 'her')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HViai_Twmjxb",
        "colab_type": "text"
      },
      "source": [
        "All of these give us a sense of kind of what is this matrix that we've found. This matrix has 800 reviews and it has 260374 ngrams which are combination of unigram, some bigram and some trigram. \n",
        "\n",
        "They are recorded as these arrays of one, two or three things but they correspond to set of tupples of words with one, two, or three words in them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zewaziDXp-OB",
        "colab_type": "text"
      },
      "source": [
        "### Create valid matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esX-Rh54mbUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j_indices = []\n",
        "indptr = []\n",
        "values = []\n",
        "indptr.append(0)\n",
        "\n",
        "for i, doc in enumerate(movie_reviews.valid.x):\n",
        "    feature_counter = Counter(doc.data)\n",
        "    j_indices.extend(feature_counter.keys())\n",
        "    values.extend(feature_counter.values())\n",
        "    this_doc_ngrams = list()\n",
        "\n",
        "    m = 0\n",
        "    for n in range(min_n, max_n + 1):\n",
        "        for k in range(vocab_len - n + 1):\n",
        "            ngram = doc.data[k: k + n]\n",
        "            if str(ngram) in ngramtoi:\n",
        "                this_doc_ngrams.append(ngramtoi[str(ngram)])\n",
        "            m += 1\n",
        "\n",
        "    ngram_counter = Counter(this_doc_ngrams)\n",
        "    j_indices.extend(ngram_counter.keys())\n",
        "    values.extend(ngram_counter.values())\n",
        "    indptr.append(len(j_indices))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQyagIMpqAxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_ngram_doc_matrix = scipy.sparse.csr_matrix((values,j_indices,indptr),\n",
        "                                                shape=(len(indptr)-1,len(ngramtoi)),\n",
        "                                                dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys2F-wX_qYSc",
        "colab_type": "code",
        "outputId": "8797f725-89bf-40a1-e9bf-179d165bb4ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "valid_ngram_doc_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<200x260374 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 121600 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcqdSrrDqaPI",
        "colab_type": "code",
        "outputId": "0711af0d-9392-4d90-de75-8a4bf4a2a244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_ngram_doc_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x260374 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 678885 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHrZJG-2qgcW",
        "colab_type": "text"
      },
      "source": [
        "###Save data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrmZBvH-qc7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scipy.sparse.save_npz('train_ngram_matrix.npz',train_ngram_doc_matrix)\n",
        "scipy.sparse.save_npz('valid_ngram_matrix.npz',valid_ngram_doc_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9fx8d8vrXW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('itongram.pickle','wb') as handle:\n",
        "       pickle.dump(itongram,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "with open('ngramtoi.pickle','wb') as handle:\n",
        "    pickle.dump(ngramtoi,handle,protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou1EmPMSsQHD",
        "colab_type": "text"
      },
      "source": [
        "###Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqSmTAe2sN5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ngram_doc_matrix = scipy.sparse.load_npz(\"train_ngram_matrix.npz\")\n",
        "valid_ngram_doc_matrix = scipy.sparse.load_npz(\"valid_ngram_matrix.npz\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12p5EvMsSwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('itongram.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)\n",
        "    \n",
        "with open('ngramtoi.pickle', 'rb') as handle:\n",
        "    b = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04tkoW59sYme",
        "colab_type": "text"
      },
      "source": [
        "###Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rriijsGVsU-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train_ngram_doc_matrix\n",
        "y = movie_reviews.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lr4qxgTsjWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = y.c2i['positive']\n",
        "negative = y.c2i['negative']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvRgB6k-spaI",
        "colab_type": "code",
        "outputId": "bdc374bc-03ca-47f0-b5cb-3dcae55d58be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x260374 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 566480 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-gLiegsuB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 260374"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAwtvhM4sy87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = (y.items==positive)[:k]\n",
        "neg = (y.items==negative)[:k]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqgHxGuptOrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = x[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9nA7cD8tQdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels = [o==positive for o in movie_reviews.valid.y.items]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoElVS8suLE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFuQ0jiSuWj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1)/((y.items==positive).sum()+1)\n",
        "pr0 = (p0+1)/((y.items==negative).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3biZG28uoRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc7IVXC-usvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items==positive).mean()/(y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ApI8CsXu3OG",
        "colab_type": "code",
        "outputId": "1f4b1b52-d530-4d25-9d60-5e55bbca9b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.08505123261815539"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiCeiXzpu4Em",
        "colab_type": "code",
        "outputId": "aa712ee2-5c2c-414c-cbb1-2fc691857272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(y.items==positive).mean(),(y.items==negative).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.47875, 0.52125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwDeMLFmu_Xj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pre_preds = valid_ngram_doc_matrix @ r.T + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdA0lHpKvGwZ",
        "colab_type": "code",
        "outputId": "bda6977d-61da-4798-f2a9-03496082661f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pre_preds"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 111.020137,   39.70486 ,    1.3799  ,   14.603319, ...,   81.603164,   -5.777097, -152.193113,  120.194783])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQxVG6q3vHtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = pre_preds.T >0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeoq1DfBvM_l",
        "colab_type": "code",
        "outputId": "901964e2-9d8f-4fe4-ae35-0204f64a6610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True, False,  True,  True, False,  True, False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvE0S7D0vOW_",
        "colab_type": "code",
        "outputId": "83d0a2db-7cc6-4d97-e2dc-3e3e5545d95b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(preds==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7gJLSIxvbbp",
        "colab_type": "text"
      },
      "source": [
        "### Binarized Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Mbz-K2vYN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_x_ngram_sgn = train_ngram_doc_matrix.sign()\n",
        "val_x_ngram_sgn = valid_ngram_doc_matrix.sign()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWHY9Cn2vw6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = trn_x_ngram_sgn[:k]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HTdvXpv1nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N237xoswB8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1)/((y.items==positive).sum()+1)\n",
        "pr0 = (p0+1)/((y.items==negative).sum()+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw0ENXZ9wTbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)\n",
        "b = np.log((y.items==positive).mean()/(y.items==negative).mean())\n",
        "\n",
        "pre_preds = val_x_ngram_sgn @ r.T +b\n",
        "preds = pre_preds.T >0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MXCtq9AwuMS",
        "colab_type": "code",
        "outputId": "0a8e1be7-6a75-4e2a-d00d-f5d18709eb43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(preds==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.735"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jEHf3fx5I6",
        "colab_type": "text"
      },
      "source": [
        "###Logistic Regression\n",
        "\n",
        "Here we fit regularized logistic regression where the features are the trigrams.\n",
        "\n",
        "**use CountVectorizer to compare**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZbvepoLxFVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bd58K77yHqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veczr = CountVectorizer(ngram_range=(1,3),preprocessor=noop,tokenizer=noop,max_features=800000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xej7VxaybLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = movie_reviews.train.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQAbD52Pyfr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.train.x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMwnX_KyyNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_words = [[docs.vocab.itos[o] for o in doc.data] for doc in movie_reviews.valid.x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxzpEKsZy8DS",
        "colab_type": "code",
        "outputId": "1729f6f1-5a3e-460b-b313-3249bd3ec98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "train_ngram_doc = veczr.fit_transform(train_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.56 s, sys: 7.82 ms, total: 1.57 s\n",
            "Wall time: 1.57 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC3bdNaNzZj2",
        "colab_type": "code",
        "outputId": "1047302b-0e63-4168-817f-dbb1134c7c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_ngram_doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<800x260373 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 565680 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehp0pG7uzc-Z",
        "colab_type": "code",
        "outputId": "6af1ed92-dce5-426f-85c4-d4aae1226b05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "veczr.vocabulary_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'xxbos': 235215,\n",
              " 'xxmaj': 235589,\n",
              " 'un': 217511,\n",
              " '-': 14660,\n",
              " 'xxunk': 247952,\n",
              " 'believable': 50420,\n",
              " '!': 593,\n",
              " 'meg': 134438,\n",
              " 'ryan': 171955,\n",
              " 'does': 72621,\n",
              " \"n't\": 141191,\n",
              " 'even': 78281,\n",
              " 'look': 129009,\n",
              " 'her': 101674,\n",
              " 'usual': 219401,\n",
              " 'lovable': 129862,\n",
              " 'self': 175864,\n",
              " 'in': 110056,\n",
              " 'this': 206619,\n",
              " ',': 8800,\n",
              " 'which': 228198,\n",
              " 'normally': 145182,\n",
              " 'makes': 131546,\n",
              " 'me': 133646,\n",
              " 'forgive': 88662,\n",
              " 'shallow': 177158,\n",
              " 'acting': 27682,\n",
              " '.': 16836,\n",
              " 'hard': 97855,\n",
              " 'to': 210364,\n",
              " 'believe': 50457,\n",
              " 'she': 177306,\n",
              " 'was': 222309,\n",
              " 'the': 193798,\n",
              " 'producer': 164483,\n",
              " 'on': 152319,\n",
              " 'dog': 72926,\n",
              " 'plus': 162127,\n",
              " 'kevin': 122604,\n",
              " 'kline': 123461,\n",
              " ':': 20359,\n",
              " 'what': 226855,\n",
              " 'kind': 123231,\n",
              " 'of': 147512,\n",
              " 'suicide': 188353,\n",
              " 'trip': 215808,\n",
              " 'has': 98123,\n",
              " 'his': 103526,\n",
              " 'career': 58691,\n",
              " 'been': 49234,\n",
              " '?': 20987,\n",
              " '...': 18342,\n",
              " 'finally': 85368,\n",
              " 'directed': 71212,\n",
              " 'by': 56556,\n",
              " 'guy': 96410,\n",
              " 'who': 229023,\n",
              " 'did': 70468,\n",
              " 'big': 51595,\n",
              " 'must': 140407,\n",
              " 'be': 47412,\n",
              " 'a': 21275,\n",
              " 'replay': 169611,\n",
              " 'jonestown': 121174,\n",
              " 'hollywood': 105183,\n",
              " 'style': 187604,\n",
              " 'xxbos xxmaj': 235357,\n",
              " 'xxmaj un': 246383,\n",
              " 'un -': 217512,\n",
              " '- xxunk': 16422,\n",
              " 'xxunk -': 248697,\n",
              " '- believable': 14848,\n",
              " 'believable !': 50421,\n",
              " '! xxmaj': 756,\n",
              " 'xxmaj meg': 242183,\n",
              " 'meg xxmaj': 134443,\n",
              " 'xxmaj ryan': 244138,\n",
              " 'ryan does': 171964,\n",
              " \"does n't\": 72753,\n",
              " \"n't even\": 141520,\n",
              " 'even look': 78521,\n",
              " 'look her': 129094,\n",
              " 'her usual': 102320,\n",
              " 'usual xxunk': 219433,\n",
              " 'xxunk lovable': 252438,\n",
              " 'lovable self': 129871,\n",
              " 'self in': 175889,\n",
              " 'in this': 111780,\n",
              " 'this ,': 206638,\n",
              " ', which': 13810,\n",
              " 'which normally': 228470,\n",
              " 'normally makes': 145185,\n",
              " 'makes me': 131625,\n",
              " 'me forgive': 133781,\n",
              " 'forgive her': 88663,\n",
              " 'her shallow': 102226,\n",
              " 'shallow xxunk': 177173,\n",
              " 'xxunk acting': 249119,\n",
              " 'acting xxunk': 27924,\n",
              " 'xxunk .': 248797,\n",
              " '. xxmaj': 17369,\n",
              " 'xxmaj hard': 239904,\n",
              " 'hard to': 97911,\n",
              " 'to believe': 210897,\n",
              " 'believe she': 50526,\n",
              " 'she was': 177750,\n",
              " 'was the': 223683,\n",
              " 'the producer': 199538,\n",
              " 'producer on': 164495,\n",
              " 'on this': 153091,\n",
              " 'this dog': 206958,\n",
              " 'dog .': 72927,\n",
              " 'xxmaj plus': 243513,\n",
              " 'plus xxmaj': 162147,\n",
              " 'xxmaj kevin': 241336,\n",
              " 'kevin xxmaj': 122609,\n",
              " 'xxmaj kline': 241432,\n",
              " 'kline :': 123467,\n",
              " ': what': 20589,\n",
              " 'what kind': 227108,\n",
              " 'kind of': 123254,\n",
              " 'of suicide': 149874,\n",
              " 'suicide trip': 188375,\n",
              " 'trip has': 215819,\n",
              " 'has his': 98393,\n",
              " 'his career': 103704,\n",
              " 'career been': 58707,\n",
              " 'been on': 49445,\n",
              " 'on ?': 152368,\n",
              " '? xxmaj': 21111,\n",
              " 'xxmaj xxunk': 247309,\n",
              " 'xxunk ...': 248843,\n",
              " '... xxmaj': 18626,\n",
              " 'xxunk !': 247957,\n",
              " '! !': 608,\n",
              " 'xxmaj finally': 239054,\n",
              " 'finally this': 85430,\n",
              " 'this was': 208162,\n",
              " 'was directed': 222764,\n",
              " 'directed by': 71225,\n",
              " 'by the': 56975,\n",
              " 'the guy': 197076,\n",
              " 'guy who': 96501,\n",
              " 'who did': 229220,\n",
              " 'did xxmaj': 70743,\n",
              " 'xxmaj big': 236722,\n",
              " 'big xxmaj': 51745,\n",
              " 'xxunk ?': 248967,\n",
              " 'xxmaj must': 242513,\n",
              " 'must be': 140420,\n",
              " 'be a': 47457,\n",
              " 'a replay': 24646,\n",
              " 'replay of': 169614,\n",
              " 'of xxmaj': 150909,\n",
              " 'xxmaj jonestown': 241138,\n",
              " 'jonestown -': 121175,\n",
              " '- hollywood': 15339,\n",
              " 'hollywood style': 105256,\n",
              " 'style .': 187615,\n",
              " 'xxbos xxmaj un': 235543,\n",
              " 'xxmaj un -': 246384,\n",
              " 'un - xxunk': 217515,\n",
              " '- xxunk -': 16428,\n",
              " 'xxunk - believable': 248707,\n",
              " '- believable !': 14849,\n",
              " 'believable ! xxmaj': 50422,\n",
              " '! xxmaj meg': 813,\n",
              " 'xxmaj meg xxmaj': 242186,\n",
              " 'meg xxmaj ryan': 134445,\n",
              " 'xxmaj ryan does': 244143,\n",
              " \"ryan does n't\": 171965,\n",
              " \"does n't even\": 72768,\n",
              " \"n't even look\": 141536,\n",
              " 'even look her': 78523,\n",
              " 'look her usual': 129095,\n",
              " 'her usual xxunk': 102322,\n",
              " 'usual xxunk lovable': 219436,\n",
              " 'xxunk lovable self': 252439,\n",
              " 'lovable self in': 129872,\n",
              " 'self in this': 175890,\n",
              " 'in this ,': 111782,\n",
              " 'this , which': 206664,\n",
              " ', which normally': 13842,\n",
              " 'which normally makes': 228471,\n",
              " 'normally makes me': 145186,\n",
              " 'makes me forgive': 131629,\n",
              " 'me forgive her': 133782,\n",
              " 'forgive her shallow': 88664,\n",
              " 'her shallow xxunk': 102227,\n",
              " 'shallow xxunk acting': 177174,\n",
              " 'xxunk acting xxunk': 249121,\n",
              " 'acting xxunk .': 27925,\n",
              " 'xxunk . xxmaj': 248831,\n",
              " '. xxmaj hard': 17667,\n",
              " 'xxmaj hard to': 239905,\n",
              " 'hard to believe': 97918,\n",
              " 'to believe she': 210904,\n",
              " 'believe she was': 50527,\n",
              " 'she was the': 177783,\n",
              " 'was the producer': 223725,\n",
              " 'the producer on': 199540,\n",
              " 'producer on this': 164496,\n",
              " 'on this dog': 153096,\n",
              " 'this dog .': 206959,\n",
              " 'dog . xxmaj': 72929,\n",
              " '. xxmaj plus': 17911,\n",
              " 'xxmaj plus xxmaj': 243517,\n",
              " 'plus xxmaj kevin': 162150,\n",
              " 'xxmaj kevin xxmaj': 241339,\n",
              " 'kevin xxmaj kline': 122611,\n",
              " 'xxmaj kline :': 241435,\n",
              " 'kline : what': 123468,\n",
              " ': what kind': 20590,\n",
              " 'what kind of': 227109,\n",
              " 'kind of suicide': 123303,\n",
              " 'of suicide trip': 149876,\n",
              " 'suicide trip has': 188376,\n",
              " 'trip has his': 215820,\n",
              " 'has his career': 98395,\n",
              " 'his career been': 103708,\n",
              " 'career been on': 58708,\n",
              " 'been on ?': 49446,\n",
              " 'on ? xxmaj': 152369,\n",
              " '? xxmaj xxunk': 21210,\n",
              " 'xxmaj xxunk ...': 247323,\n",
              " 'xxunk ... xxmaj': 248862,\n",
              " '... xxmaj xxunk': 18661,\n",
              " 'xxmaj xxunk !': 247310,\n",
              " 'xxunk ! !': 247959,\n",
              " '! ! !': 610,\n",
              " '! ! xxmaj': 615,\n",
              " '! xxmaj finally': 793,\n",
              " 'xxmaj finally this': 239059,\n",
              " 'finally this was': 85431,\n",
              " 'this was directed': 208170,\n",
              " 'was directed by': 222765,\n",
              " 'directed by the': 71226,\n",
              " 'by the guy': 57017,\n",
              " 'the guy who': 197081,\n",
              " 'guy who did': 96505,\n",
              " 'who did xxmaj': 229223,\n",
              " 'did xxmaj big': 70745,\n",
              " 'xxmaj big xxmaj': 236727,\n",
              " 'big xxmaj xxunk': 51752,\n",
              " 'xxmaj xxunk ?': 247337,\n",
              " 'xxunk ? xxmaj': 248978,\n",
              " '? xxmaj must': 21162,\n",
              " 'xxmaj must be': 242514,\n",
              " 'must be a': 140422,\n",
              " 'be a replay': 47502,\n",
              " 'a replay of': 24647,\n",
              " 'replay of xxmaj': 169616,\n",
              " 'of xxmaj jonestown': 151052,\n",
              " 'xxmaj jonestown -': 241139,\n",
              " 'jonestown - hollywood': 121176,\n",
              " '- hollywood style': 15340,\n",
              " 'hollywood style .': 105257,\n",
              " 'style . xxmaj': 187616,\n",
              " '. xxmaj xxunk': 18188,\n",
              " 'is': 114366,\n",
              " 'extremely': 80761,\n",
              " 'well': 225716,\n",
              " 'made': 130644,\n",
              " 'film': 83820,\n",
              " 'script': 174274,\n",
              " 'and': 33946,\n",
              " 'camera': 57721,\n",
              " 'work': 233483,\n",
              " 'are': 41012,\n",
              " 'all': 30461,\n",
              " 'first': 85890,\n",
              " 'rate': 166609,\n",
              " 'music': 140241,\n",
              " 'good': 94145,\n",
              " 'too': 214407,\n",
              " 'though': 208610,\n",
              " 'it': 117551,\n",
              " 'mostly': 137844,\n",
              " 'early': 74979,\n",
              " 'when': 227457,\n",
              " 'things': 206075,\n",
              " 'still': 185848,\n",
              " 'relatively': 168961,\n",
              " 'there': 204250,\n",
              " 'no': 144363,\n",
              " 'really': 167508,\n",
              " 'cast': 59075,\n",
              " 'several': 176859,\n",
              " 'faces': 81047,\n",
              " 'will': 230377,\n",
              " 'familiar': 81626,\n",
              " 'entire': 77477,\n",
              " 'an': 33087,\n",
              " 'excellent': 79796,\n",
              " 'job': 120747,\n",
              " 'with': 231170,\n",
              " '\\n \\n ': 0,\n",
              " 'but': 55216,\n",
              " 'watch': 224112,\n",
              " 'because': 48810,\n",
              " 'end': 76332,\n",
              " 'situation': 180238,\n",
              " 'like': 126902,\n",
              " 'one': 153397,\n",
              " 'presented': 163551,\n",
              " 'now': 146734,\n",
              " 'blame': 52345,\n",
              " 'british': 54342,\n",
              " 'for': 87090,\n",
              " 'setting': 176784,\n",
              " 'hindus': 103470,\n",
              " 'muslims': 140396,\n",
              " 'against': 29861,\n",
              " 'each': 74783,\n",
              " 'other': 156101,\n",
              " 'then': 203872,\n",
              " 'them': 203443,\n",
              " 'into': 113684,\n",
              " 'two': 216847,\n",
              " 'countries': 66006,\n",
              " 'some': 182067,\n",
              " 'merit': 134926,\n",
              " 'view': 220873,\n",
              " \"'s\": 4211,\n",
              " 'also': 31789,\n",
              " 'true': 215922,\n",
              " 'that': 191516,\n",
              " 'forced': 88464,\n",
              " 'region': 168789,\n",
              " 'as': 42798,\n",
              " 'they': 204938,\n",
              " 'around': 42440,\n",
              " 'time': 209681,\n",
              " 'partition': 158797,\n",
              " 'seems': 175459,\n",
              " 'more': 136835,\n",
              " 'likely': 127636,\n",
              " 'simply': 179651,\n",
              " 'saw': 172784,\n",
              " 'between': 51328,\n",
              " 'were': 226122,\n",
              " 'clever': 62384,\n",
              " 'enough': 77130,\n",
              " 'exploit': 80552,\n",
              " 'their': 202830,\n",
              " 'own': 157760,\n",
              " 'ends': 76690,\n",
              " 'result': 169915,\n",
              " 'much': 139717,\n",
              " 'cruelty': 67238,\n",
              " 'inhumanity': 112819,\n",
              " 'very': 219998,\n",
              " 'unpleasant': 218249,\n",
              " 'remember': 169283,\n",
              " 'see': 174814,\n",
              " 'screen': 174075,\n",
              " 'never': 143353,\n",
              " 'painted': 158224,\n",
              " 'black': 52194,\n",
              " 'white': 228937,\n",
              " 'case': 58970,\n",
              " 'both': 53375,\n",
              " 'sides': 179375,\n",
              " 'hope': 105512,\n",
              " 'change': 59918,\n",
              " 'younger': 259666,\n",
              " 'generation': 91755,\n",
              " 'redemption': 168583,\n",
              " 'sort': 183596,\n",
              " 'make': 131230,\n",
              " 'choice': 61337,\n",
              " 'man': 131946,\n",
              " 'ruined': 171644,\n",
              " 'life': 126517,\n",
              " 'truly': 216065,\n",
              " 'loved': 130082,\n",
              " 'family': 81657,\n",
              " 'later': 124768,\n",
              " 'come': 63097,\n",
              " 'looking': 129227,\n",
              " 'point': 162183,\n",
              " 'without': 232553,\n",
              " 'great': 95354,\n",
              " 'pain': 158128,\n",
              " 'carries': 58854,\n",
              " 'message': 134989,\n",
              " 'have': 98825,\n",
              " 'grave': 95320,\n",
              " 'can': 57887,\n",
              " 'caring': 58767,\n",
              " 'people': 159376,\n",
              " 'reality': 167409,\n",
              " 'wrenching': 234800,\n",
              " 'since': 179821,\n",
              " 'real': 167120,\n",
              " 'across': 27566,\n",
              " 'india': 112483,\n",
              " '/': 18685,\n",
              " 'pakistan': 158273,\n",
              " 'border': 53135,\n",
              " 'sense': 175979,\n",
              " 'similar': 179525,\n",
              " '\"': 885,\n",
              " 'mr': 139630,\n",
              " '&': 2764,\n",
              " 'we': 225002,\n",
              " 'glad': 93381,\n",
              " 'seen': 175586,\n",
              " 'resolution': 169763,\n",
              " 'if': 109091,\n",
              " 'xxup': 256420,\n",
              " 'uk': 217428,\n",
              " 'us': 218989,\n",
              " 'could': 65629,\n",
              " 'deal': 68601,\n",
              " 'racism': 166226,\n",
              " 'would': 234302,\n",
              " 'certainly': 59722,\n",
              " 'better': 51084,\n",
              " 'off': 151359,\n",
              " 'xxmaj this': 245963,\n",
              " 'this is': 207315,\n",
              " 'is a': 114454,\n",
              " 'a extremely': 22460,\n",
              " 'extremely well': 80814,\n",
              " 'well -': 225761,\n",
              " '- made': 15548,\n",
              " 'made film': 130753,\n",
              " 'film .': 83976,\n",
              " 'xxmaj the': 245396,\n",
              " 'the acting': 194047,\n",
              " 'acting ,': 27699,\n",
              " ', script': 12374,\n",
              " 'script and': 174304,\n",
              " 'and camera': 34620,\n",
              " 'camera -': 57732,\n",
              " '- work': 16324,\n",
              " 'work are': 233518,\n",
              " 'are all': 41095,\n",
              " 'all first': 30706,\n",
              " 'first -': 85912,\n",
              " '- rate': 15849,\n",
              " 'rate .': 166614,\n",
              " 'the music': 198546,\n",
              " 'music is': 140305,\n",
              " 'is good': 115451,\n",
              " 'good ,': 94164,\n",
              " ', too': 13451,\n",
              " 'too ,': 214410,\n",
              " ', though': 13364,\n",
              " 'though it': 208705,\n",
              " 'it is': 118571,\n",
              " 'is mostly': 115889,\n",
              " 'mostly early': 137871,\n",
              " 'early in': 75023,\n",
              " 'in the': 111407,\n",
              " 'the film': 196370,\n",
              " 'film ,': 83895,\n",
              " ', when': 13757,\n",
              " 'when things': 227777,\n",
              " 'things are': 206105,\n",
              " 'are still': 41990,\n",
              " 'still relatively': 186037,\n",
              " 'relatively xxunk': 168972,\n",
              " 'xxmaj there': 245855,\n",
              " 'there are': 204333,\n",
              " 'are no': 41690,\n",
              " 'no really': 144751,\n",
              " 'really xxunk': 168047,\n",
              " 'xxunk in': 251851,\n",
              " 'the cast': 194866,\n",
              " 'cast ,': 59084,\n",
              " 'though several': 208725,\n",
              " 'several faces': 176868,\n",
              " 'faces will': 81063,\n",
              " 'will be': 230412,\n",
              " 'be familiar': 47777,\n",
              " 'familiar .': 81630,\n",
              " 'the entire': 196050,\n",
              " 'entire cast': 77482,\n",
              " 'cast does': 59134,\n",
              " 'does an': 72651,\n",
              " 'an excellent': 33466,\n",
              " 'excellent job': 79841,\n",
              " 'job with': 120821,\n",
              " 'with the': 232077,\n",
              " 'the script': 200171,\n",
              " 'script .': 174296,\n",
              " '. \\n \\n ': 16837,\n",
              " '\\n \\n  xxmaj': 201,\n",
              " 'xxmaj but': 237126,\n",
              " 'but it': 55695,\n",
              " 'is hard': 115484,\n",
              " 'to watch': 213667,\n",
              " 'watch ,': 224118,\n",
              " ', because': 9675,\n",
              " 'because there': 48980,\n",
              " 'there is': 204441,\n",
              " 'is no': 116005,\n",
              " 'no good': 144551,\n",
              " 'good end': 94326,\n",
              " 'end to': 76473,\n",
              " 'to a': 210430,\n",
              " 'a situation': 24996,\n",
              " 'situation like': 180259,\n",
              " 'like the': 127322,\n",
              " 'the one': 198830,\n",
              " 'one presented': 153897,\n",
              " 'presented .': 163554,\n",
              " 'xxmaj it': 240727,\n",
              " 'is now': 116121,\n",
              " 'now xxunk': 146917,\n",
              " 'xxunk to': 254850,\n",
              " 'to blame': 210923,\n",
              " 'blame the': 52354,\n",
              " 'the xxmaj': 201947,\n",
              " 'xxmaj british': 236999,\n",
              " 'british for': 54363,\n",
              " 'for setting': 87889,\n",
              " 'setting xxmaj': 176810,\n",
              " 'xxmaj hindus': 240185,\n",
              " 'hindus and': 103471,\n",
              " 'and xxmaj': 38613,\n",
              " 'xxmaj muslims': 242508,\n",
              " 'muslims against': 140400,\n",
              " 'against each': 29874,\n",
              " 'each other': 74849,\n",
              " 'other ,': 156112,\n",
              " ', and': 9210,\n",
              " 'and then': 37926,\n",
              " 'then xxunk': 204208,\n",
              " 'xxunk xxunk': 256113,\n",
              " 'xxunk them': 254689,\n",
              " 'them into': 203601,\n",
              " 'into two': 114005,\n",
              " 'two countries': 216919,\n",
              " 'countries .': 66007,\n",
              " 'is some': 116649,\n",
              " 'some merit': 182377,\n",
              " 'merit in': 134929,\n",
              " 'this view': 208150,\n",
              " 'view ,': 220878,\n",
              " ', but': 9816,\n",
              " \"it 's\": 117588,\n",
              " \"'s also\": 4421,\n",
              " 'also true': 32183,\n",
              " 'true that': 216022,\n",
              " 'that no': 192771,\n",
              " 'no one': 144674,\n",
              " 'one forced': 153656,\n",
              " 'forced xxmaj': 88496,\n",
              " 'muslims in': 140404,\n",
              " 'the region': 199760,\n",
              " 'region to': 168794,\n",
              " 'to xxunk': 213906,\n",
              " 'xxunk each': 250833,\n",
              " 'other as': 156152,\n",
              " 'as they': 43777,\n",
              " 'they did': 205222,\n",
              " 'did around': 70505,\n",
              " 'around the': 42544,\n",
              " 'the time': 201139,\n",
              " 'time of': 209909,\n",
              " 'of partition': 149424,\n",
              " 'partition .': 158798,\n",
              " 'it seems': 119090,\n",
              " 'seems more': 175506,\n",
              " 'more likely': 137145,\n",
              " 'likely that': 127641,\n",
              " 'that the': 193156,\n",
              " 'british simply': 54371,\n",
              " 'simply saw': 179762,\n",
              " 'saw the': 172841,\n",
              " 'the xxunk': 202328,\n",
              " 'xxunk between': 250006,\n",
              " 'between the': 51405,\n",
              " 'xxunk and': 249286,\n",
              " 'and were': 38443,\n",
              " 'were clever': 226238,\n",
              " 'clever enough': 62404,\n",
              " 'enough to': 77231,\n",
              " 'to exploit': 211447,\n",
              " 'exploit them': 80555,\n",
              " 'them to': 203679,\n",
              " 'to their': 213475,\n",
              " 'their own': 203196,\n",
              " 'own ends': 157823,\n",
              " 'ends .': 76695,\n",
              " 'the result': 199836,\n",
              " 'result is': 169924,\n",
              " 'is that': 116796,\n",
              " 'that there': 193275,\n",
              " 'is much': 115900,\n",
              " 'much cruelty': 139809,\n",
              " 'cruelty and': 67241,\n",
              " 'and inhumanity': 35995,\n",
              " 'inhumanity in': 112822,\n",
              " 'the situation': 200461,\n",
              " 'situation and': 180247,\n",
              " 'and this': 38036,\n",
              " 'is very': 117137,\n",
              " 'very unpleasant': 220509,\n",
              " 'unpleasant to': 218256,\n",
              " 'to remember': 212696,\n",
              " 'remember and': 169296,\n",
              " 'and to': 38111,\n",
              " 'to see': 212854,\n",
              " 'see on': 175040,\n",
              " 'on the': 152902,\n",
              " 'the screen': 200136,\n",
              " 'screen .': 174091,\n",
              " 'is never': 115991,\n",
              " 'never painted': 143568,\n",
              " 'painted as': 158229,\n",
              " 'as a': 42823,\n",
              " 'a black': 21626,\n",
              " 'black -': 52203,\n",
              " '- and': 14760,\n",
              " 'and -': 33996,\n",
              " '- white': 16291,\n",
              " 'white case': 228952,\n",
              " 'case .': 58987,\n",
              " 'is xxunk': 117365,\n",
              " 'and xxunk': 38888,\n",
              " 'xxunk on': 253072,\n",
              " 'on both': 152517,\n",
              " 'both sides': 53502,\n",
              " 'sides ,': 179376,\n",
              " 'and also': 34248,\n",
              " 'also the': 32154,\n",
              " 'the hope': 197241,\n",
              " 'hope for': 105530,\n",
              " 'for change': 87359,\n",
              " 'change in': 59935,\n",
              " 'the younger': 202749,\n",
              " 'younger generation': 259673,\n",
              " 'generation .': 91761,\n",
              " 'is redemption': 116416,\n",
              " 'redemption of': 168590,\n",
              " 'of a': 147610,\n",
              " 'a sort': 25099,\n",
              " 'sort ,': 183597,\n",
              " ', in': 11120,\n",
              " 'the end': 195972,\n",
              " 'end ,': 76339,\n",
              " 'when xxmaj': 227801,\n",
              " 'xxunk has': 251511,\n",
              " 'has to': 98634,\n",
              " 'to make': 212189,\n",
              " 'make a': 131241,\n",
              " 'a hard': 23137,\n",
              " 'hard choice': 97881,\n",
              " 'choice between': 61345,\n",
              " 'between a': 51341,\n",
              " 'a man': 23685,\n",
              " 'man who': 132150,\n",
              " 'who has': 229325,\n",
              " 'has ruined': 98557,\n",
              " 'ruined her': 171652,\n",
              " 'her life': 102060,\n",
              " 'life ,': 126534,\n",
              " 'but also': 55298,\n",
              " 'also truly': 32185,\n",
              " 'truly loved': 216110,\n",
              " 'loved her': 130091,\n",
              " 'her ,': 101692,\n",
              " 'and her': 35712,\n",
              " 'her family': 101932,\n",
              " 'family which': 81804,\n",
              " 'which has': 228319,\n",
              " 'has xxunk': 98680,\n",
              " 'xxunk her': 251582,\n",
              " ', then': 13173,\n",
              " 'then later': 204072,\n",
              " 'later come': 124810,\n",
              " 'come looking': 63158,\n",
              " 'looking for': 129260,\n",
              " 'for her': 87496,\n",
              " 'her .': 101710,\n",
              " 'but by': 55370,\n",
              " 'by that': 56972,\n",
              " 'that point': 192893,\n",
              " 'point ,': 162186,\n",
              " ', she': 12445,\n",
              " 'she has': 177504,\n",
              " 'has no': 98482,\n",
              " 'no xxunk': 144880,\n",
              " 'xxunk that': 254369,\n",
              " 'that is': 192435,\n",
              " 'is without': 117270,\n",
              " 'without great': 232630,\n",
              " 'great pain': 95591,\n",
              " 'pain for': 158144,\n",
              " 'this film': 207025,\n",
              " 'film carries': 84143,\n",
              " 'carries the': 58858,\n",
              " 'the message': 198158,\n",
              " 'message that': 135026,\n",
              " 'that both': 191862,\n",
              " 'both xxmaj': 53538,\n",
              " 'muslims and': 140402,\n",
              " 'hindus have': 103473,\n",
              " 'have their': 99557,\n",
              " 'their grave': 203063,\n",
              " 'grave xxunk': 95335,\n",
              " 'xxunk ,': 248396,\n",
              " 'also that': 32151,\n",
              " 'both can': 53406,\n",
              " 'can be': 57938,\n",
              " 'be xxunk': 48415,\n",
              " 'and caring': 34640,\n",
              " 'caring people': 58777,\n",
              " 'people .': 159412,\n",
              " 'the reality': 199706,\n",
              " 'reality of': 167436,\n",
              " 'partition makes': 158802,\n",
              " 'makes that': 131661,\n",
              " 'that xxunk': 193698,\n",
              " 'xxunk all': 249208,\n",
              " 'all the': 30990,\n",
              " 'the more': 198263,\n",
              " 'more wrenching': 137388,\n",
              " 'wrenching ,': 234801,\n",
              " ', since': 12528,\n",
              " 'since there': 179899,\n",
              " 'there can': 204406,\n",
              " 'can never': 58108,\n",
              " 'never be': 143375,\n",
              " 'be real': 48100,\n",
              " 'real xxunk': 167321,\n",
              " 'xxunk across': 249113,\n",
              " 'across the': 27586,\n",
              " 'xxmaj india': 240590,\n",
              " 'india /': 112490,\n",
              " '/ xxmaj': 18907,\n",
              " 'xxmaj pakistan': 243182,\n",
              " 'pakistan border': 158276,\n",
              " 'border .': 53138,\n",
              " 'xxmaj in': 240533,\n",
              " 'in that': 111376,\n",
              " 'that sense': 193016,\n",
              " 'sense ,': 175985,\n",
              " ', it': 11287,\n",
              " 'is similar': 116578,\n",
              " 'similar to': 179557,\n",
              " 'to \"': 210368,\n",
              " '\" xxmaj': 2328,\n",
              " 'xxmaj mr': 242460,\n",
              " 'mr &': 139631,\n",
              " '& xxmaj': 2903,\n",
              " 'xxunk xxmaj': 255664,\n",
              " 'xxunk \"': 247972,\n",
              " '\" .': 1004,\n",
              " ', we': 13678,\n",
              " 'we were': 225329,\n",
              " 'were glad': 226337,\n",
              " 'glad to': 93394,\n",
              " 'to have': 211733,\n",
              " 'have seen': 99432,\n",
              " 'seen the': 175774,\n",
              " ', even': 10426,\n",
              " 'even though': 78667,\n",
              " 'though the': 208736,\n",
              " 'the resolution': 199820,\n",
              " 'resolution was': 169779,\n",
              " 'was xxunk': 223942,\n",
              " 'xxmaj if': 240474,\n",
              " 'if the': 109289,\n",
              " 'the xxup': 202653,\n",
              " 'xxup uk': 257596,\n",
              " 'uk and': 217435,\n",
              " 'and xxup': 39047,\n",
              " 'xxup us': 257609,\n",
              " 'us could': 219041,\n",
              " 'could deal': 65715,\n",
              " 'deal with': 68620,\n",
              " 'with their': 232212,\n",
              " 'own xxunk': 157951,\n",
              " 'xxunk of': 252885,\n",
              " 'of racism': 149553,\n",
              " 'racism with': 166246,\n",
              " 'with this': 232244,\n",
              " 'this kind': 207417,\n",
              " 'of xxunk': 151175,\n",
              " ', they': 13232,\n",
              " 'they would': 205812,\n",
              " 'would certainly': 234409,\n",
              " 'certainly be': 59727,\n",
              " 'be better': 47628,\n",
              " 'better off': 51219,\n",
              " 'off .': 151379,\n",
              " 'xxbos xxmaj this': 235538,\n",
              " 'xxmaj this is': 246003,\n",
              " 'this is a': 207319,\n",
              " 'is a extremely': 114503,\n",
              " 'a extremely well': 22461,\n",
              " 'extremely well -': 80815,\n",
              " 'well - made': 225771,\n",
              " '- made film': 15550,\n",
              " 'made film .': 130755,\n",
              " 'film . xxmaj': 83983,\n",
              " '. xxmaj the': 18069,\n",
              " 'xxmaj the acting': 245406,\n",
              " 'the acting ,': 194049,\n",
              " 'acting , script': 27720,\n",
              " ', script and': 12375,\n",
              " 'script and camera': 174306,\n",
              " 'and camera -': 34621,\n",
              " 'camera - work': 57733,\n",
              " '- work are': 16326,\n",
              " 'work are all': 233519,\n",
              " 'are all first': 41101,\n",
              " 'all first -': 30707,\n",
              " 'first - rate': 85915,\n",
              " '- rate .': 15850,\n",
              " 'rate . xxmaj': 166615,\n",
              " 'xxmaj the music': 245635,\n",
              " 'the music is': 198555,\n",
              " 'music is good': 140308,\n",
              " 'is good ,': 115454,\n",
              " 'good , too': 94181,\n",
              " ', too ,': 13453,\n",
              " 'too , though': 214421,\n",
              " ', though it': 13378,\n",
              " 'though it is': 208709,\n",
              " 'it is mostly': 118641,\n",
              " 'is mostly early': 115894,\n",
              " 'mostly early in': 137872,\n",
              " 'early in the': 75024,\n",
              " 'in the film': 111525,\n",
              " 'the film ,': 196376,\n",
              " 'film , when': 83948,\n",
              " ', when things': 13778,\n",
              " 'when things are': 227778,\n",
              " 'things are still': 206109,\n",
              " 'are still relatively': 41993,\n",
              " 'still relatively xxunk': 186038,\n",
              " 'relatively xxunk .': 168973,\n",
              " '. xxmaj there': 18072,\n",
              " 'xxmaj there are': 245860,\n",
              " 'there are no': 204367,\n",
              " 'are no really': 41701,\n",
              " 'no really xxunk': 144752,\n",
              " 'really xxunk in': 168053,\n",
              " 'xxunk in the': 251923,\n",
              " 'in the cast': 111464,\n",
              " 'the cast ,': 194867,\n",
              " 'cast , though': 59096,\n",
              " ', though several': 13383,\n",
              " 'though several faces': 208726,\n",
              " 'several faces will': 176869,\n",
              " 'faces will be': 81064,\n",
              " 'will be familiar': 230428,\n",
              " 'be familiar .': 47778,\n",
              " 'familiar . xxmaj': 81631,\n",
              " 'xxmaj the entire': 245509,\n",
              " 'the entire cast': 196053,\n",
              " 'entire cast does': 77484,\n",
              " 'cast does an': 59135,\n",
              " 'does an excellent': 72652,\n",
              " 'an excellent job': 33471,\n",
              " 'excellent job with': 79844,\n",
              " 'job with the': 120822,\n",
              " 'with the script': 232189,\n",
              " 'the script .': 200175,\n",
              " 'script . \\n \\n ': 174297,\n",
              " '. \\n \\n  xxmaj': 16871,\n",
              " '\\n \\n  xxmaj but': 256,\n",
              " 'xxmaj but it': 237158,\n",
              " 'but it is': 55712,\n",
              " 'it is hard': 118625,\n",
              " 'is hard to': 115486,\n",
              " 'hard to watch': 97936,\n",
              " 'to watch ,': 213670,\n",
              " 'watch , because': 224119,\n",
              " ', because there': 9688,\n",
              " 'because there is': 48983,\n",
              " 'there is no': 204467,\n",
              " 'is no good': 116014,\n",
              " 'no good end': 144554,\n",
              " 'good end to': 94328,\n",
              " 'end to a': 76474,\n",
              " 'to a situation': 210484,\n",
              " 'a situation like': 24998,\n",
              " 'situation like the': 180260,\n",
              " 'like the one': 127348,\n",
              " 'the one presented': 198845,\n",
              " 'one presented .': 153898,\n",
              " 'presented . xxmaj': 163555,\n",
              " '. xxmaj it': 17714,\n",
              " 'xxmaj it is': 240769,\n",
              " 'it is now': 118650,\n",
              " 'is now xxunk': 116127,\n",
              " 'now xxunk to': 146921,\n",
              " 'xxunk to blame': 254877,\n",
              " 'to blame the': 210925,\n",
              " 'blame the xxmaj': 52357,\n",
              " 'the xxmaj british': 201983,\n",
              " 'xxmaj british for': 237009,\n",
              " 'british for setting': 54364,\n",
              " 'for setting xxmaj': 87890,\n",
              " 'setting xxmaj hindus': 176811,\n",
              " 'xxmaj hindus and': 240186,\n",
              " 'hindus and xxmaj': 103472,\n",
              " 'and xxmaj muslims': 38796,\n",
              " 'xxmaj muslims against': 242510,\n",
              " 'muslims against each': 140401,\n",
              " 'against each other': 29875,\n",
              " 'each other ,': 74851,\n",
              " 'other , and': 156113,\n",
              " ', and then': 9441,\n",
              " 'and then xxunk': 37967,\n",
              " 'then xxunk xxunk': 204219,\n",
              " 'xxunk xxunk them': 256260,\n",
              " 'xxunk them into': 254695,\n",
              " 'them into two': 203604,\n",
              " 'into two countries': 114006,\n",
              " 'two countries .': 216920,\n",
              " 'countries . xxmaj': 66008,\n",
              " 'xxmaj there is': 245863,\n",
              " 'there is some': 204478,\n",
              " 'is some merit': 116653,\n",
              " 'some merit in': 182378,\n",
              " 'merit in this': 134930,\n",
              " 'in this view': 111835,\n",
              " 'this view ,': 208151,\n",
              " 'view , but': 220880,\n",
              " ', but it': 9890,\n",
              " \"but it 's\": 55696,\n",
              " \"it 's also\": 117600,\n",
              " \"'s also true\": 4429,\n",
              " 'also true that': 32184,\n",
              " 'true that no': 216023,\n",
              " 'that no one': 192776,\n",
              " 'no one forced': 144682,\n",
              " 'one forced xxmaj': 153657,\n",
              " 'forced xxmaj hindus': 88497,\n",
              " 'xxmaj muslims in': 242512,\n",
              " 'muslims in the': 140405,\n",
              " 'in the region': 111634,\n",
              " 'the region to': 199761,\n",
              " 'region to xxunk': 168795,\n",
              " 'to xxunk each': 213937,\n",
              " 'xxunk each other': 250837,\n",
              " 'each other as': 74858,\n",
              " 'other as they': 156153,\n",
              " 'as they did': 43783,\n",
              " 'they did around': 205225,\n",
              " 'did around the': 70506,\n",
              " 'around the time': 42555,\n",
              " 'the time of': 201151,\n",
              " 'time of partition': 209911,\n",
              " 'of partition .': 149425,\n",
              " 'partition . xxmaj': 158799,\n",
              " 'xxmaj it seems': 240801,\n",
              " 'it seems more': 119098,\n",
              " 'seems more likely': 175507,\n",
              " 'more likely that': 137147,\n",
              " 'likely that the': 127642,\n",
              " 'that the xxmaj': 193265,\n",
              " 'xxmaj british simply': 237013,\n",
              " 'british simply saw': 54372,\n",
              " 'simply saw the': 179763,\n",
              " 'saw the xxunk': 172850,\n",
              " 'the xxunk between': 202374,\n",
              " 'xxunk between the': 250017,\n",
              " 'between the xxunk': 51431,\n",
              " 'the xxunk and': 202358,\n",
              " 'xxunk and were': 249580,\n",
              " 'and were clever': 38444,\n",
              " 'were clever enough': 226239,\n",
              " 'clever enough to': 62405,\n",
              " 'enough to exploit': 77237,\n",
              " 'to exploit them': 211449,\n",
              " 'exploit them to': 80556,\n",
              " 'them to their': 203691,\n",
              " 'to their own': 213485,\n",
              " 'their own ends': 203203,\n",
              " 'own ends .': 157824,\n",
              " 'ends . \\n \\n ': 76696,\n",
              " '\\n \\n  xxmaj the': 500,\n",
              " 'xxmaj the result': 245703,\n",
              " 'the result is': 199837,\n",
              " 'result is that': 169928,\n",
              " 'is that there': 116827,\n",
              " 'that there is': 193278,\n",
              " 'there is much': 204463,\n",
              " 'is much cruelty': 115902,\n",
              " 'much cruelty and': 139810,\n",
              " 'cruelty and inhumanity': 67242,\n",
              " 'and inhumanity in': 35997,\n",
              " 'inhumanity in the': 112823,\n",
              " 'in the situation': 111667,\n",
              " 'the situation and': 200465,\n",
              " 'situation and this': 180248,\n",
              " 'and this is': 38045,\n",
              " 'this is very': 207390,\n",
              " 'is very unpleasant': 117172,\n",
              " 'very unpleasant to': 220510,\n",
              " 'unpleasant to remember': 218257,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNq2J0__zH0B",
        "colab_type": "code",
        "outputId": "c8834eb5-ad9e-4327-d29a-2c8ce2f070f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "val_ngram_doc = veczr.transform(valid_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 176 ms, sys: 2.97 ms, total: 179 ms\n",
            "Wall time: 178 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDioBYkNzPwZ",
        "colab_type": "code",
        "outputId": "c535ac53-9e17-480c-fd22-348eaf8d390f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "val_ngram_doc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<200x260373 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 93552 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKSCEtJ0o5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = veczr.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUEyt4IF07ej",
        "colab_type": "code",
        "outputId": "ae7d79d8-0ff6-4d6a-f2f7-b8574167f746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "vocab[200000:200005]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the same cult',\n",
              " 'the same dance',\n",
              " 'the same date',\n",
              " 'the same day',\n",
              " 'the same diner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3-kGk-1GxE",
        "colab_type": "text"
      },
      "source": [
        "#### Binarized Naive Bayes, using ngrams from CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8srU9G0y0_dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = movie_reviews.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UarBb3Eo1VgC",
        "colab_type": "text"
      },
      "source": [
        "C is the inverse of regularization strength; smaller values specify stronger regularization. Regularized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0utdhoAA1UuY",
        "colab_type": "code",
        "outputId": "69db305a-5e13-4a50-a8c0-1f041cc9de6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "m = LogisticRegression(C = 0.1, dual=True);\n",
        "m.fit(train_ngram_doc.sign(),y.items);\n",
        "\n",
        "preds = m.predict(val_ngram_doc.sign());\n",
        "(preds.T == valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zLeLZah135X",
        "colab_type": "text"
      },
      "source": [
        "Not binarized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eLusQw1w4T",
        "colab_type": "code",
        "outputId": "12f7199c-5814-4cc6-ed85-2541369b107b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "m = LogisticRegression(C=0.1, dual=True)\n",
        "m.fit(train_ngram_doc, y.items);\n",
        "\n",
        "preds = m.predict(val_ngram_doc)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.78"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewWZyw5q2xc-",
        "colab_type": "text"
      },
      "source": [
        "**Using my ngrams, binarized:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYOMkaHt164d",
        "colab_type": "code",
        "outputId": "fe773bab-03f7-4215-edc8-1363aade27f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "source": [
        "m2 = LogisticRegression(C=0.1, dual=True)\n",
        "m2.fit(trn_x_ngram_sgn, y.items)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlcAkg_X3H0_",
        "colab_type": "code",
        "outputId": "6f21ca6a-b566-4727-87a0-a70f4af2aee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds = m2.predict(val_x_ngram_sgn)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzU64I6y3MwS",
        "colab_type": "text"
      },
      "source": [
        "Worse performance when not binarized. I manually tried several different C values, and this was the best I found:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZJz7023KHS",
        "colab_type": "code",
        "outputId": "085b9bc0-2d98-4953-f2e9-fd4ff634d2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "m2 = LogisticRegression(C=0.0001, dual=True, max_iter=50000)\n",
        "m2.fit(train_ngram_doc_matrix, y.items)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.0001, class_weight=None, dual=True, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=50000,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHmpYZvw3QLk",
        "colab_type": "code",
        "outputId": "4625210e-0ccc-4921-e0a5-344ac38c94f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds = m2.predict(valid_ngram_doc_matrix)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_oeFbPn3eG_",
        "colab_type": "text"
      },
      "source": [
        "#### Log-count ratio\n",
        "\n",
        "Here is the log-count ratio r."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf7yHO8z3RUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train_ngram_doc_matrix.sign()\n",
        "val_x = valid_ngram_doc_matrix.sign()\n",
        "y = movie_reviews.train.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttVrAmj73wQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive=y.c2i['positive']\n",
        "negative = y.c2i['negative']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwTlfHaB5Lmh",
        "colab_type": "code",
        "outputId": "9902a74a-4adc-4b50-c5ce-33826f735e15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 260374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAYol5_l4Fpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 260374"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD0heQhD4Jto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = (y.items==positive)[:k]\n",
        "neg = (y.items==negative)[:k]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7xZKvqQ4TCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx = x[:k]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMkF18Rg6DWp",
        "colab_type": "code",
        "outputId": "14eefc57-8487-4f51-93dd-851369920aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xx.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 260374)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p93MxqZ4UwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_labels = [ o ==positive for o in movie_reviews.valid.y.items]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaioIfwD4eXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p0 = np.squeeze(np.array(xx[neg].sum(0)))\n",
        "p1 = np.squeeze(np.array(xx[pos].sum(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1DcjWV24iSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pr1 = (p1+1) / ((y.items==positive).sum() + 1)\n",
        "pr0 = (p0+1) / ((y.items==negative).sum() + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y47jjW7Z4j_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.log(pr1/pr0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pYfoV1r44bo",
        "colab_type": "code",
        "outputId": "a97e9e8d-c79b-4b70-e26f-736acac84dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(260374,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWSGe_R54nUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.log((y.items==positive).mean() / (y.items==negative).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MawBsuqM4qvH",
        "colab_type": "code",
        "outputId": "5ee57e48-f6b4-4e0a-8559-7250b066bbc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.exp(r)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.994341, 1.088542, 1.      , 1.088542, ..., 0.544271, 0.544271, 0.544271, 0.544271])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 266
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDnwVI_i4uXG",
        "colab_type": "text"
      },
      "source": [
        "Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylC3l5bf4sXA",
        "colab_type": "code",
        "outputId": "f9be52ff-5fce-41cf-d877-2dffc48ffddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "x_nb = xx.multiply(r)\n",
        "m = LogisticRegression(dual=True, C=0.1)\n",
        "m.fit(x_nb, y.items);\n",
        "\n",
        "val_x_nb = val_x.multiply(r)\n",
        "preds = m.predict(val_x_nb)\n",
        "(preds.T==valid_labels).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-271-5c84aae37a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_nb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_x_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1532\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ -91.206826  403.62008  -294.555645  165.261428 ... -222.886389  125.698305 -327.813732 -251.578884].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "056oyjKK6UwE",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "\n",
        "Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFVbIxZCaSZw",
        "colab_type": "text"
      },
      "source": [
        "# Additionaly [Video 6](https://youtu.be/z8-Tbrg1-rE?t=56)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF5bKevkakk_",
        "colab_type": "text"
      },
      "source": [
        "## Trouble installing fastai library?\n",
        "\n",
        "Here is a [guide to troubleshooting](https://docs.fast.ai/troubleshoot.html) problems with fastai installation. By far, the most common problem is having fastai installed for a different environment/different Python installation than the one your Jupyter notebook is using (you can have Python installed in multiple places on your computer and not even realize it!). Or, you might have different versions of fastai installed in your different environments/different Python installations (and the one you are running in Jupyter notebook could be out of date, even if you installed version 1.0 somewhere else). For both of these problems, please [see this entry](https://docs.fast.ai/troubleshoot.html#modulenotfounderror-no-module-named-fastaivision)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocs40SR1bpK-",
        "colab_type": "text"
      },
      "source": [
        "## More detail about randomized SVD\n",
        "\n",
        "I didn't cover how randomized SVD worked, because we aren't going to learn about it in detail in this course. The main things I want you to know about randomized SVD are:\n",
        "\n",
        "- it is fast\n",
        "\n",
        "- it gives us a truncated SVD (whereas with traditional SVD, we are usually throwing away small singular values and their corresponding columns)\n",
        "\n",
        "If you were curious to know more, two keys are:\n",
        "\n",
        "- It is often useful to be able to reduce dimensionality of data in a way that preserves distances. The Johnson–Lindenstrauss lemma is a classic result of this type. [Johnson-Lindenstrauss Lemma](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma): a small set of points in a high-dimensional space can be embedded into a space of much lower dimension in such a way that distances between the points are nearly preserved (proof uses random projections).\n",
        "\n",
        "- We haven't found a better general SVD method, we'll just use the method we have on a smaller matrix.\n",
        "\n",
        "Below is an over-simplified version of randomized_svd (you wouldn't want to use this in practice, but it covers the core ideas). The main part to notice is that we multiply our original matrix by a smaller random matrix (M @ rand_matrix) to produce smaller_matrix, and then use our same np.linalg.svd as before:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def randomized_svd(M, k=10):\n",
        "    m, n = M.shape\n",
        "    transpose = False\n",
        "    if m < n:\n",
        "        transpose = True\n",
        "        M = M.T\n",
        "\n",
        "    rand_matrix = np.random.normal(size=(M.shape[1], k))  # short side by k\n",
        "    Q, _ = np.linalg.qr(M @ rand_matrix, mode='reduced')  # long side by k\n",
        "    smaller_matrix = Q.T @ M                              # k by short side\n",
        "    U_hat, s, V = np.linalg.svd(smaller_matrix, full_matrices=False)\n",
        "    U = Q @ U_hat\n",
        "\n",
        "    if transpose:\n",
        "        return V.T, s.T, U.T\n",
        "    else:\n",
        "        return U, s, V\n",
        "```\n",
        "\n",
        "This code snippet is from this [randomized-SVD jupyter notebook](https://github.com/fastai/randomized-SVD/blob/master/Randomized%20SVD.ipynb) which was the demo I used for my PyBay talk on [Using randomness to make code](https://www.youtube.com/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7) much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ25PHHvdlmZ",
        "colab_type": "text"
      },
      "source": [
        "## Bayes Theorem\n",
        "\n",
        "Ex: Physicist Leonard Mlodinow tested positive for HIV in 1989.\n",
        "His doctor said there was a 99.9% chance he had HIV.\n",
        "\n",
        "A = positive test results\n",
        "\n",
        "B = having HIV\n",
        "\n",
        "True positives:  $P(A|B) = 99.9\\%$\n",
        "\n",
        "Prevalence:  $P(B)= 0.01\\%$\n",
        "\n",
        "False positives:  $P(A|B^C) = 0.1\\%$\n",
        "\n",
        "Was his doctor correct?\n",
        "\n",
        "This example is from the book:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/drunkards-walk.jpg)\n",
        "\n",
        "Bayes Theorem (for conditional probabilities): \n",
        "\n",
        "P(A | B) P(B) = P(B | A) P(A)\n",
        "\n",
        "![link text](https://github.com/hduongck/AI-ML-Learning/blob/master/Pic/Capture.PNG?raw=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNiBgGCOe2U3",
        "colab_type": "text"
      },
      "source": [
        "Derivation of Naive Bayes\n",
        "We want to calculate the probability that the review \"I loved it\" is positive. Using Bayes Theorem, we can rewrite this:\n",
        "\n",
        "$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}, | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$\n",
        "The \"naive\" part of Naive Bayes is that we will assume that the probabilities of the different words are all independent.\n",
        "\n",
        "$$ P(\\text{pos} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{pos}) \\cdot P(\\text{\"loved\"} | \\text{pos}) \\cdot P(\\text{\"it\"} | \\text{pos}) \\cdot P(\\text{pos})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$\n",
        "We do the same calculation to see how likely it is the review is negative, and then choose whichever is larger.\n",
        "\n",
        "$$ P(\\text{neg} | \\text{\"I\"}, \\text{\"loved\"}, \\text{\"it\"}) = \\frac{P(\\text{\"I\"} | \\text{neg}) \\cdot P(\\text{\"loved\"} | \\text{neg}) \\cdot P(\\text{\"it\"} | \\text{neg}) \\cdot P(\\text{neg})}{P(\\text{\"I\"}, \\text{\"loved\"}, \\text{\"it})}$$\n",
        "We will add one to avoid dividing by zero (or something close to it). Similarly, we take logarithms to avoid multiplying by a lot of tiny values. For the reasons we want to avoid this, please see the next section on numerical stability:\n",
        "\n",
        "More reading: [Using log-probabilities for Naive Bayes](http://www.cs.rhodes.edu/~kirlinp/courses/ai/f18/projects/proj3/naive-bayes-log-probs.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec-3xMsYg-WC",
        "colab_type": "text"
      },
      "source": [
        "## Numerical Stability\n",
        "Exercise\n",
        "Take a moment to look at the function $f$ below. Before you try running it, write on paper what the output would be of $x_1 = f(\\frac{1}{10})$. Now, (still on paper) plug that back into $f$ and calculate $x_2 = f(x_1)$. Keep going for 10 iterations.\n",
        "\n",
        "This example is taken from page 107 of Numerical Methods, by Greenbaum and Chartier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2eoreYe4wrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    if x <= 1/2:\n",
        "        return 2 * x\n",
        "    if x > 1/2:\n",
        "        return 2 * x - 1\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cww3H4QtjbH3",
        "colab_type": "text"
      },
      "source": [
        "Only after you've written down what you think the answer should be, run the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYR54vVgjY-Y",
        "colab_type": "code",
        "outputId": "2664513f-4496-4686-9e64-df706d011cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x = 1/10\n",
        "for i in range(80):\n",
        "    print(x)\n",
        "    x = f(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n",
            "0.2\n",
            "0.4\n",
            "0.8\n",
            "0.6000000000000001\n",
            "0.20000000000000018\n",
            "0.40000000000000036\n",
            "0.8000000000000007\n",
            "0.6000000000000014\n",
            "0.20000000000000284\n",
            "0.4000000000000057\n",
            "0.8000000000000114\n",
            "0.6000000000000227\n",
            "0.20000000000004547\n",
            "0.40000000000009095\n",
            "0.8000000000001819\n",
            "0.6000000000003638\n",
            "0.2000000000007276\n",
            "0.4000000000014552\n",
            "0.8000000000029104\n",
            "0.6000000000058208\n",
            "0.20000000001164153\n",
            "0.40000000002328306\n",
            "0.8000000000465661\n",
            "0.6000000000931323\n",
            "0.20000000018626451\n",
            "0.40000000037252903\n",
            "0.8000000007450581\n",
            "0.6000000014901161\n",
            "0.20000000298023224\n",
            "0.4000000059604645\n",
            "0.800000011920929\n",
            "0.6000000238418579\n",
            "0.20000004768371582\n",
            "0.40000009536743164\n",
            "0.8000001907348633\n",
            "0.6000003814697266\n",
            "0.20000076293945312\n",
            "0.40000152587890625\n",
            "0.8000030517578125\n",
            "0.600006103515625\n",
            "0.20001220703125\n",
            "0.4000244140625\n",
            "0.800048828125\n",
            "0.60009765625\n",
            "0.2001953125\n",
            "0.400390625\n",
            "0.80078125\n",
            "0.6015625\n",
            "0.203125\n",
            "0.40625\n",
            "0.8125\n",
            "0.625\n",
            "0.25\n",
            "0.5\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70M71sZYkT_b",
        "colab_type": "text"
      },
      "source": [
        "**What went wrong?**\n",
        "\n",
        "It's like a pattern here 0.1 0.2 0.4 0.8 0.6 repeatedly. But the problem here , it starts getting a litte bit of small decimal (0.60000000000000**01**) and it grows bigger and bigger. Eventually, it causes problem to us when we run long enough. It will converge to 1. This is one of problems about using computers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mOZan_lMLm",
        "colab_type": "text"
      },
      "source": [
        "## Problem: math is continuous & infinite, but computers are discrete & finite\n",
        "\n",
        "Two Limitations of computer representations of numbers:\n",
        "\n",
        "1. they can't be arbitrarily large or small\n",
        "2. there must be gaps between them\n",
        "\n",
        "The reason we need to care about accuracy, is because computers can't store infinitely accurate numbers. It's possible to create calculations that give very wrong answers (particularly when repeating an operation many times, since each operation could multiply the error).\n",
        "\n",
        "How computers store numbers:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/fpa.png)\n",
        "\n",
        "The mantissa can also be referred to as the significand.\n",
        "\n",
        "IEEE Double precision arithmetic:\n",
        "\n",
        "- Numbers can be as large as $1.79 \\times 10^{308}$ and as small as $2.23 \\times 10^{-308}$.\n",
        "- The interval $[1,2]$ is represented by discrete subset: $$1, \\: 1+2^{-52}, \\: 1+2 \\times 2^{-52},\\: 1+3 \\times 2^{-52},\\: \\ldots, 2$$\n",
        "\n",
        "- The interval $[2,4]$ is represented: $$2, \\: 2+2^{-51}, \\: 2+2 \\times 2^{-51},\\: 2+3 \\times 2^{-51},\\: \\ldots, 4$$\n",
        "\n",
        "Floats and doubles are not equidistant:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/fltscale-wh.png)\n",
        "\n",
        "Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)\n",
        "\n",
        "**Machine Epsilon**\n",
        "\n",
        "Half the distance between 1 and the next larger number. This can vary by computer. IEEE standards for double precision specify \n",
        "\n",
        "$$ \\varepsilon_{machine} = 2^{-53} \\approx 1.11 \\times 10^{-16}$$\n",
        "\n",
        "**Two important properties of Floating Point Arithmetic:**\n",
        "\n",
        "The difference between a real number $x$ and its closest floating point approximation $fl(x)$ is always smaller than $\\varepsilon_{machine}$ in relative terms. For some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon_{machine}$, $$fl(x)=x \\cdot (1 + \\varepsilon)$$\n",
        "\n",
        "Where is any operation ($+, -, \\times, \\div$), and $\\circledast$ is its floating point analogue, $$ x \\circledast y = (x &lt;/em&gt; y)(1 + \\varepsilon)$$ for some $\\varepsilon$, where $\\lvert \\varepsilon \\rvert \\leq \\varepsilon&lt;em&gt;{machine}$ That is, every operation of floating point arithmetic is exact up to a relative error of size at most $\\varepsilon&lt;/em&gt;{machine}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgMpkCn3nvoi",
        "colab_type": "text"
      },
      "source": [
        "##Speed of different types of memory\n",
        "\n",
        "This course is 90% NLP and 10% things I want to make sure you see before the end of your MSDS.\n",
        "\n",
        "Here are some numbers everyone should know (from the legendary Jeff Dean):\n",
        "\n",
        "- L1 cache reference 0.5 ns\n",
        "- L2 cache reference 7 ns\n",
        "- Main memory reference/RAM 100 ns\n",
        "- Send 2K bytes over 1 Gbps network 20,000 ns\n",
        "- Read 1 MB sequentially from memory 250,000 ns\n",
        "- Round trip within same datacenter 500,000 ns\n",
        "- Disk seek 10,000,000 ns\n",
        "- Read 1 MB sequentially from network 10,000,000 ns\n",
        "- Read 1 MB sequentially from disk 30,000,000 ns\n",
        "- Send packet CA->Netherlands->CA 150,000,000 ns\n",
        "- And here is an updated, interactive version, which includes a timeline of how these numbers have changed.\n",
        "\n",
        "Key take-away: Each successive memory type is (at least) an order of magnitude worse than the one before it. Disk seeks are very slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SABUdDsuceR",
        "colab_type": "text"
      },
      "source": [
        "##Revisiting Naive Bayes in an Excel Spreadsheet\n",
        "\n",
        "Let's calculate naive bayes in a spreadsheet to get a more visual picture of what is going on. Here's how I processed the data for this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkd7rF0Hugz_",
        "colab_type": "text"
      },
      "source": [
        "### Loading our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDzof5ERnSo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai import *\n",
        "from fastai.text import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVdoQWTkkO4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhd7V0Wpuvs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "movie_reviews = (TextList.from_csv(path,'texts.csv',cols='text')\n",
        "                        .split_from_df(col=2)\n",
        "                        .label_from_df(cols=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzC7wHO4vEig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_term_doc_matrix(label_list, vocab_len):\n",
        "    j_indices = []\n",
        "    indptr = []\n",
        "    values = []\n",
        "    indptr.append(0)\n",
        "\n",
        "    for i, doc in enumerate(label_list):\n",
        "        feature_counter = Counter(doc.data)\n",
        "        j_indices.extend(feature_counter.keys())\n",
        "        values.extend(feature_counter.values())\n",
        "        indptr.append(len(j_indices))\n",
        "        \n",
        "#     return (values, j_indices, indptr)\n",
        "\n",
        "    return scipy.sparse.csr_matrix((values, j_indices, indptr),\n",
        "                                   shape=(len(indptr) - 1, vocab_len),\n",
        "                                   dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LRyiZ5RvMxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_term_doc = get_term_doc_matrix(movie_reviews.train.x,len(movie_reviews.vocab.itos))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En6CuDP7vlor",
        "colab_type": "text"
      },
      "source": [
        "### Getting data for our spreadsheet\n",
        "\n",
        "To keep our spreadsheet manageable, we will just get the 40 shortest reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSt0E-7JvjFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inds = np.argpartition(np.count_nonzero(trn_term_doc.todense(),1),40,axis=0)[:40]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5plecadQv55y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "2570a5f9-3494-4357-868c-ef5f434f334f"
      },
      "source": [
        "inds"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[597],\n",
              "        [170],\n",
              "        [320],\n",
              "        [119],\n",
              "        [730],\n",
              "        [386],\n",
              "        [428],\n",
              "        [245],\n",
              "        [467],\n",
              "        [102],\n",
              "        [477],\n",
              "        [210],\n",
              "        [ 53],\n",
              "        [212],\n",
              "        [ 42],\n",
              "        [336],\n",
              "        [395],\n",
              "        [275],\n",
              "        [533],\n",
              "        [276],\n",
              "        [499],\n",
              "        [630],\n",
              "        [264],\n",
              "        [697],\n",
              "        [461],\n",
              "        [441],\n",
              "        [122],\n",
              "        [704],\n",
              "        [790],\n",
              "        [712],\n",
              "        [756],\n",
              "        [254],\n",
              "        [369],\n",
              "        [483],\n",
              "        [617],\n",
              "        [607],\n",
              "        [699],\n",
              "        [646],\n",
              "        [ 46],\n",
              "        [328]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkntuPpv9va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inds = np.squeeze(np.asarray(inds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-1l9p6RwFHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11a5cc44-9f60-4601-f497-b7f125cd7e91"
      },
      "source": [
        "inds"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([597, 170, 320, 119, 730, 386, 428, 245, 467, 102, 477, 210,  53, 212,  42, 336, 395, 275, 533, 276, 499, 630,\n",
              "       264, 697, 461, 441, 122, 704, 790, 712, 756, 254, 369, 483, 617, 607, 699, 646,  46, 328])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaPaXeLlwTTs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's get the text from these 40 shortest reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mroJytVawF4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_text = [movie_reviews.train.x[i] for i in inds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6WMfO-7wfNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fb7e74c-5dea-4dc5-a1f8-06b76946d012"
      },
      "source": [
        "list_text"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text xxbos xxmaj this movie is so bad , i knew how it ends right after this little girl killed the first person . xxmaj very bad acting very bad plot very bad movie \n",
              "  \n",
              "   do yourself a xxunk and xxup don't watch it 1 / 10,\n",
              " Text xxbos i found this film to funny from the start . xxmaj john xxmaj waters use of characters reminded of some of the down to earth characters from xxmaj xxunk films . xxmaj christina xxmaj xxunk has once again xxunk her abilities in this film . xxmaj if you are looking for a fun movie without xxunk , i recommend this film .,\n",
              " Text xxbos xxmaj if you ever see a stand up comedy movie this is the one . xxmaj you will laugh xxunk if you have any sense of humor at all . xxmaj this is a once in a lifetime performance from a once in a lifetime performer . xxmaj this is a stand up standard .,\n",
              " Text xxbos xxmaj another movie to suffer without an adventure to run , no xxunk to solve . xxmaj just an xxunk man , acting like an animal . xxmaj no a good reason to take this journey . xxmaj pitt and xxmaj lewis are great actors ; magnificent xxmaj michelle xxmaj xxunk but a weak xxmaj david xxmaj duchovny performance ...,\n",
              " Text xxbos xxmaj this is an hilarious movie . xxmaj one of the very best things about it is the quality of the performance by each actor . xxmaj from the xxunk role to the xxunk , each character is vivid , xxunk and so understandable . xxmaj it can also make you laugh so hard your health will improve .,\n",
              " Text xxbos xxmaj this is by far one of the worst movies i have ever seen , the poor special effects along with the poor acting are just a few of the things wrong with this film . i am fan of the first two major xxunk but this one is lame !,\n",
              " Text xxbos xxmaj the direction struck me as poor man 's xxmaj xxunk xxmaj bergman . xxmaj the xxunk dialogue was annoying . xxmaj the xxunk xxunk that all characters except xxmaj xxunk ' showed made me think they were drugged . i think the director ruined it for me .,\n",
              " Text xxbos xxmaj omar xxmaj xxunk is an outstanding actor . i really think he gets into his character xxunk . xxmaj when xxunk gets shot he shows true emotion . xxmaj he also shows true emotion when xxunk puts the gun to him in the room . xxmaj omar is a very talented actor ! !,\n",
              " Text xxbos xxmaj the greatest xxunk in life is being dull , and this movie is xxunk boring . its funny , its left out of his \" a life in film \" documentary . xxmaj he goes from a long piece on \" xxmaj xxunk xxmaj memories \" and then fast forwards to \" xxmaj xxunk \" . xxmaj this little piece of xxunk xxunk just is n't worth the effort .,\n",
              " Text xxbos xxmaj the way the story is developed , keeps the audience wondering what is the xxunk 's dark past . xxmaj we get some clues during the series , but enough to keep us interested in the mini - series . xxmaj the characters are all believable and i personally felt xxunk and xxunk by the story .,\n",
              " Text xxbos xxmaj this movie is one of the masterpieces from xxmaj mr. xxmaj xxunk . xxmaj it is about youth , xxunk , happiness , xxunk , xxunk , honor , corruption . xxmaj and it is like everything else from great xxmaj italian director xxunk art . \n",
              "  \n",
              "  ,\n",
              " Text xxbos xxmaj it 's terrific when a funny movie does n't make smile you . xxmaj what a pity ! ! xxmaj this film is very boring and so long . xxmaj it 's simply xxunk . xxmaj the story is staggering without goal and no fun . \n",
              "  \n",
              "   xxmaj you feel better when it 's finished .,\n",
              " Text xxbos \" xxmaj foxes \" is a great film . xxmaj the four young actresses xxmaj xxunk xxmaj foster , xxmaj xxunk xxmaj xxunk , xxmaj marilyn xxmaj xxunk and xxmaj xxunk xxmaj xxunk are wonderful . xxmaj the song \" xxmaj on the radio \" by xxmaj donna xxmaj summer is lovely . a great film . xxrep 5 *,\n",
              " Text xxbos xxmaj this movie features xxmaj charlie xxmaj xxunk dancing in a strip club . xxmaj beyond that , it features a truly bad script with dull , unrealistic dialogue . xxmaj that it got as many positive xxunk suggests some people may be joking .,\n",
              " Text xxbos xxmaj this film is brilliant ! xxmaj it touches everyone who sees it in an extraordinary way . xxmaj it really takes you back to your youth and puts a new perspective on how you view your childhood memories . xxmaj there are so many layers to this film . xxmaj it is innovative and absolutely fabulous !,\n",
              " Text xxbos i understand there was some conflict between xxmaj xxunk and the great xxmaj xxunk xxmaj smith during the filming . xxmaj understandable when you put one of the world 's greatest actresses of all time ( xxmaj smith , of course ) with one whose performances seem to get worse with each subsequent film .,\n",
              " Text xxbos ... okay , maybe not all of it . xxmaj xxunk by the false promise of bikini - xxunk women on the movie 's cover ... but the xxup horror ... xxup the xxup horror ... ... whatever you do , do xxup not watch this movie . xxmaj xxunk out your eyes , repeatedly xxunk your xxunk in ... do what it takes . xxmaj never again -- never forget ! \n",
              "  \n",
              "  ,\n",
              " Text xxbos a bit of xxmaj trivia b / c i ca n't figure out how to submit xxmaj trivia : xxmaj in the backdrop of this performance , one of the images is \n",
              "  \n",
              "   xxmaj george xxmaj xxunk 's \" a xxmaj sunday xxmaj afternoon on the xxmaj island of xxmaj la xxmaj grande xxmaj xxunk \" painting ( seen best in chapter 18 ) , this painting is the subject of a xxmaj xxunk musical xxmaj sunday in the xxmaj park with xxmaj george . \n",
              "  \n",
              "   a bit of xxmaj trivia b / c i ca n't figure out how to submit xxmaj trivia : xxmaj in the backdrop of this performance , one of the images is \n",
              "  \n",
              "   xxmaj george xxmaj xxunk 's \" a xxmaj sunday xxmaj afternoon on the xxmaj island of xxmaj la xxmaj grande xxmaj xxunk \" painting ( seen best in chapter 18 ) , this painting is the subject of a xxmaj xxunk musical xxmaj sunday in the xxmaj park with xxmaj george .,\n",
              " Text xxbos xxmaj it 's a good movie if you plan to watch lots of landscapes and animals , like an animal documentary . xxmaj and making xxmaj pierce xxmaj xxunk an indian make you wonder ' xxmaj does all those people do n't recognize if someone is n't indian at plain sight ? ',\n",
              " Text xxbos xxmaj very nice action with an xxunk story which actually does n't suck . xxmaj interesting enough to merit watching instead of skipping past to get to the good parts . xxmaj having xxmaj jenna xxmaj xxunk and xxmaj asia xxmaj xxunk helps xxunk it up , too . xxmaj jenna in that xxunk and those xxunk is just xxunk ! xxmaj worth picking up just to see her !,\n",
              " Text xxbos a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies . a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies . a very realistic portrait of a broken family and the effect it has on the kid caught in between . xxmaj as a child of divorced parents i was totally relating to events in the film . xxmaj also - a really cool zombie twist which i thought was xxup very xxup original . i 'm tired of the same old stuff in movies .,\n",
              " Text xxbos xxmaj the wit and pace and three show stopping xxmaj xxunk xxmaj xxunk numbers put this ahead of the over - rated xxunk xxmaj street . xxmaj this is the xxunk 30 's musical with a knockout xxunk performance from xxmaj jimmy xxmaj cagney . xxmaj one of the last xxunk before the xxmaj motion xxmaj picture xxmaj production xxmaj code was strictly xxunk . a must see .,\n",
              " Text xxbos xxmaj this film is scary because you can find yourself relating to ideas they have and can recall other people saying and having xxunk ideas make this a haunting well done movie xxrep 4 . the xxunk style is not xxunk to point it xxunk you out of film like blair witch it only adds to the raw \" real \" feeling of the film that makes it .,\n",
              " Text xxbos i did enjoy this film , i thought it ended up being an old fashioned love story with a few twists . i expected him to get the girl , i wo n't tell you if he does or not you will need to watch the movie to find out . xxmaj overall if you are looking to watch a love story this one will suffice .,\n",
              " Text xxbos xxmaj this movie was so bad that my xxunk . went down about 40 points after seeing it . xxmaj it made me wonder who could sit through the weeks it took to make it and think that it was worth it . xxmaj it must of been some kind of personal favor to xxmaj van xxmaj damme .,\n",
              " Text xxbos i can not believe how unknown this movie is , it was absolutely incredible . xxmaj the ending alone has stuck with me for almost thirty years . xxmaj the road sign through the xxunk mirror blew me away . xxmaj if you liked \" xxup race xxup with xxup the xxup devil \" you will love this movie,\n",
              " Text xxbos xxmaj this was a horrible film ! i gave it 2 xxmaj points , one for xxmaj xxunk xxmaj jolie and a second one for the beautiful xxmaj xxunk in the beginning ... xxmaj other than that the story just plain sucked and cars xxunk through cities was n't so new in 1970 . xxmaj the xxmaj xxunk was probably what annoyed me the most , xxunk seen anything so constructed !,\n",
              " Text xxbos xxmaj good actors and good performances ca n't mask a pointless script , bad dialogue , and patterns of behavior xxunk into nothing you 'd care about . xxmaj the most interesting character is xxmaj david xxmaj xxunk . xxmaj no character development - no xxunk , no interest , just some suffering for no particular reason , teaching us nothing and not even xxunk to entertain .,\n",
              " Text xxbos xxmaj yes , i 'm sentimental & xxunk ! ! xxmaj but this movie ( and it 's theme song ) remain one of my all time xxunk ! ! xxmaj robert xxmaj xxunk xxmaj jr. does such justice to the role of \" xxmaj louis xxmaj xxunk \" xxunk and the storyline ( although far - xxunk ) is romantic & makes one believe in happy xxunk ! !,\n",
              " Text xxbos i thoroughly enjoyed this film for its humor and xxunk . i especially like the way the characters welcomed xxmaj gina 's various suitors . xxmaj with friends ( and family ) like these anyone would feel xxunk and loved . i found the writing witty and natural and the actors made the material come alive .,\n",
              " Text xxbos i love this movie . i mean the story may not be the best , but the dancing most certainly makes up for it . xxmaj you get to know a little bit about each character the way xxup they want you to learn about them . i just think that you wo n't like this movie unless you 're into xxmaj broadway ...,\n",
              " Text xxbos i and a friend rented this movie . xxmaj we both found the movie soundtrack and production techniques to be xxunk . xxmaj the movie 's plot appeared to drag on throughout with little surprise in the ending . xxmaj we both xxunk that the movie could have been xxunk into roughly an hour giving it more suspense and moving plot .,\n",
              " Text xxbos xxmaj this movie was amazingly bad . i do n't think i 've ever seen a movie where every attempt at humor failed as miserably . xxmaj let 's see ... the acting was pathetic , the \" special effects \" where horrible , the plot non - xxunk ... that pretty much xxunk up this movie . \n",
              "  \n",
              "  ,\n",
              " Text xxbos xxmaj this is a worthless sequel to a great action movie . xxmaj cheap looking , and worst of all , xxup boring xxup action xxup scenes ! xxmaj the only decent thing about the movie is the last fight sequence . xxmaj only xxunk minutes , but it feels like it goes on forever ! xxmaj even die - hard xxmaj van xxmaj damme xxunk myself ) should avoid this one !,\n",
              " Text xxbos xxmaj beautiful and touching movie . xxmaj rich colors , great settings , good acting and one of the most charming movies i have seen in a while . i never saw such an interesting setting when i was in xxmaj china . xxmaj my wife liked it so much she asked me to xxunk on and rate it so other would enjoy too .,\n",
              " Text xxbos xxmaj this film is the worst film i have ever seen . xxmaj the story line is weak - i could n't even follow it . xxmaj the acting is high - xxunk . xxmaj the sound track is irritating . xxmaj the attempts at humor are not . xxmaj the editing is horrible . xxmaj the credits are even slow - i would be embarrassed to have my name associated with this waste of film . xxmaj do n't waste your time even thinking about this attempt at acting .,\n",
              " Text xxbos i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . \n",
              "  \n",
              "   xxmaj well directed , well acted and a good story .,\n",
              " Text xxbos i caught this movie about 8 years ago , and have never had it of my mind . surely someone out there will release it on xxmaj video , or hey why not xxup dvd ! xxmaj the ford xxunk is the star xxrep 7 . if you have any head for cars xxup watch xxup this and be blown away .,\n",
              " Text xxbos all i have to say is if you do n't like it then there is something wrong with you . plus xxmaj jessica is just all kinds of hot xxrep 5 ! the only reason you may not like it is because it is set in the future where xxmaj xxunk has gone to hell . that and you my not like it cause the future they show could very well happen .,\n",
              " Text xxbos xxmaj this tale set in xxmaj xxunk , xxmaj new xxmaj zealand xxunk ( xxmaj xxunk xxunk of the xxunk xxmaj xxunk xxmaj college ) is xxunk 's first feature . \n",
              "  \n",
              "   xxmaj with a contemporary xxmaj new xxmaj zealand xxunk xxmaj via xxmaj xxunk xxunk with absolutely hilarious situations which develop in the ( adult ) family context . xxmaj at the same time it manages to xxunk intense emotions of sadness and despair . \n",
              "  \n",
              "   xxmaj one of the most moving and xxunk movies of the year - not to be missed !]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aHUzZ5jwido",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_used = defaultdict(int)\n",
        "\n",
        "for i in inds:\n",
        "    for val in movie_reviews.train.x[i].data:\n",
        "        vocab_used[val] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUjykE-Jx18k",
        "colab_type": "text"
      },
      "source": [
        "Let's choose the words that are used at least 6 times (so not too rare), but less than 30 (so not too common). You could try experimenting with different cut-off points on your own:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86sZMg4twuFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interesting_inds = [key for key,val in vocab_used.items() if val<30 and val>6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOUat4RgyEcW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53a08a8f-7df2-4bf2-bac3-b7d961aa67e9"
      },
      "source": [
        "len(interesting_inds)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck5nhyu5yMR4",
        "colab_type": "text"
      },
      "source": [
        "I copied the vocab and text of the movie reviews directly from here to paste into the spreadsheet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BX9U8SyGhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "outputId": "39a019d4-86b8-43cd-f4c2-9c3f8fa272ba"
      },
      "source": [
        "[movie_reviews.vocab.itos[i] for i in interesting_inds]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['so',\n",
              " 'bad',\n",
              " 'very',\n",
              " 'acting',\n",
              " '\\n \\n ',\n",
              " 'do',\n",
              " 'xxup',\n",
              " 'film',\n",
              " 'from',\n",
              " 'some',\n",
              " 'if',\n",
              " 'are',\n",
              " 'for',\n",
              " 'up',\n",
              " 'one',\n",
              " 'have',\n",
              " 'all',\n",
              " 'an',\n",
              " 'no',\n",
              " 'just',\n",
              " 'like',\n",
              " 'good',\n",
              " 'great',\n",
              " 'but',\n",
              " '...',\n",
              " 'about',\n",
              " 'movies',\n",
              " 'seen',\n",
              " 'with',\n",
              " '!',\n",
              " 'me',\n",
              " 'as',\n",
              " \"'s\",\n",
              " 'was',\n",
              " 'that',\n",
              " 'out',\n",
              " '\"',\n",
              " 'on',\n",
              " \"n't\",\n",
              " 'story',\n",
              " '-',\n",
              " '(',\n",
              " ')',\n",
              " 'not']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeN0ZqEFyj2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = trn_term_doc[inds,:]\n",
        "y = movie_reviews.train.y[inds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoS2AeYayzBM",
        "colab_type": "text"
      },
      "source": [
        "### Export to CSV\n",
        "\n",
        "\n",
        "Let's export the term-document matrix and the labels to CSVs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie703e4ayx1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import FileLink, FileLinks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhvHZbaZzCX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c656ad91-d5f8-4b5e-b62f-c30a4414ddf8"
      },
      "source": [
        "np.savetxt(\"x.csv\", x.todense()[:,interesting_inds], delimiter=\",\", fmt='%.14f')\n",
        "FileLink('x.csv')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href='x.csv' target='_blank'>x.csv</a><br>"
            ],
            "text/plain": [
              "/content/x.csv"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVSGV7pzDh5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5177ce23-9db1-4092-c4dd-e20470710954"
      },
      "source": [
        "np.savetxt(\"y.csv\", y, delimiter=\",\", fmt=\"%i\")\n",
        "FileLink('y.csv')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href='y.csv' target='_blank'>y.csv</a><br>"
            ],
            "text/plain": [
              "/content/y.csv"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK68fssZzFV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}