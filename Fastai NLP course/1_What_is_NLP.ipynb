{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_What_is_NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/Fastai%20NLP%20course/1_What_is_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpB_PSY9giwB",
        "colab_type": "text"
      },
      "source": [
        "These notebooks are found in the repo: https://github.com/fastai/course-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0yP5UCgkr4",
        "colab_type": "text"
      },
      "source": [
        "# 1. What is NLP?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMr9trmPgphO",
        "colab_type": "text"
      },
      "source": [
        "##What can you do with NLP?\n",
        "\n",
        "NLP is a broad field, encompassing a variety of tasks, including:\n",
        "\n",
        "- Part-of-speech tagging: identify if each word is a noun, verb, adjective, etc.)\n",
        "- Named entity recognition NER): identify person names, organizations, locations, medical codes, time expressions, quantities, monetary values, etc)\n",
        "- Question answering\n",
        "- Speech recognition\n",
        "- Text-to-speech and Speech-to-text\n",
        "- Topic modeling\n",
        "- Sentiment classification\n",
        "- Language modeling\n",
        "- Translation\n",
        "\n",
        "Many techniques from NLP are useful in a variety of places, for instance, you may have text within your tabular data.\n",
        "\n",
        "There are also interesting techniques that let you go between text and images:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/tench-net.png)\n",
        "\n",
        "from [Lesson 9](http://course18.fast.ai/lessons/lesson11.html) of Practical Deep Learning for Coders 2018.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m35ppm5FiF9x",
        "colab_type": "text"
      },
      "source": [
        "##This course\n",
        "\n",
        "In this course, we will cover these applications:\n",
        "\n",
        "Topic modeling\n",
        "Sentiment classification\n",
        "Language modeling\n",
        "Translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8oZ8HxSijsF",
        "colab_type": "text"
      },
      "source": [
        "##Top-down teaching approach\n",
        "\n",
        "I'll be using a top-down teaching method, which is different from how most CS/math courses operate. Typically, in a bottom-up approach, you first learn all the separate components you will be using, and then you gradually build them up into more complex structures. The problems with this are that students often lose motivation, don't have a sense of the \"big picture\", and don't know what they'll need.\n",
        "\n",
        "If you took the fast.ai deep learning course, that is what we used. You can hear more about my teaching philosophy in this blog post or in this talk.\n",
        "\n",
        "Harvard Professor David Perkins has a book, Making Learning Whole in which he uses baseball as an analogy. We don't require kids to memorize all the rules of baseball and understand all the technical details before we let them play the game. Rather, they start playing with a just general sense of it, and then gradually learn more rules/details as time goes on.\n",
        "\n",
        "All that to say, don't worry if you don't understand everything at first! You're not supposed to. We will start using some \"black boxes\" that haven't yet been explained, and then we'll dig into the lower level details later. The goal is to get experience working with interesting applications, which will motivate you to learn more about the underlying structures as time goes on.\n",
        "\n",
        "To start, focus on what things DO, not what they ARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqI829AMjBHN",
        "colab_type": "text"
      },
      "source": [
        "##A changing field\n",
        "\n",
        "Historically, NLP originally relied on hard-coded rules about a language. In the 1990s, there was a change towards using statistical & machine learning approaches, but the complexity of natural language meant that simple statistical approaches were often not state-of-the-art. We are now currently in the midst of a major change in the move towards neural networks. Because deep learning allows for much greater complexity, it is now achieving state-of-the-art for many things.\n",
        "\n",
        "This doesn't have to be binary: there is room to combine deep learning with rules-based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pYaxSnfjEgf",
        "colab_type": "text"
      },
      "source": [
        "## Case study: spell checkers\n",
        "\n",
        "This example comes from Peter Norvig:\n",
        "\n",
        "Historically, spell checkers required thousands of lines of hard-coded rules:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/spellchecker1.png)\n",
        "\n",
        "A version that uses historical data and probabilities can be written in far fewer lines:\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/spellchecker2.png)\n",
        "[\n",
        "Read more here](http://norvig.com/spell-correct.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTJkF7UYk_yH",
        "colab_type": "text"
      },
      "source": [
        "## A field in flux\n",
        "\n",
        "The field is still very much in a state of flux, with best practices changing.\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/skomoroch.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCG9aAAllMb-",
        "colab_type": "text"
      },
      "source": [
        "## Norvig vs. Chomsky\n",
        "\n",
        "This \"debate\" captures the tension between two approaches:\n",
        "\n",
        "- modeling the underlying mechanism of a phenomena\n",
        "- using machine learning to predict outputs (without necessarily understanding the mechanisms that create them)\n",
        "\n",
        "This tension is still very much present in NLP (and in many fields in which machine learning is being adopted, as well as in approachs to \"artificial intelligence\" in general).\n",
        "\n",
        "**Background**: Noam Chomsky is an MIT emeritus professor, the father of modern linguistics, one of the founders of cognitive science, has written >100 books. Peter Norvig is Director of Research at Google.\n",
        "\n",
        "**From MIT Tech Review coverage of a panel at MIT in 2011**:\n",
        "\n",
        "\"Chomsky derided researchers in machine learning who use purely statistical methods to produce behavior that mimics something in the world, but who don’t try to understand the meaning of that behavior. Chomsky compared such researchers to scientists who might study the dance made by a bee returning to the hive, and who could produce a statistically based simulation of such a dance without attempting to understand why the bee behaved that way. “That’s a notion of scientific success that’s very novel. I don’t know of anything like it in the history of science,” said Chomsky.\"\n",
        "\n",
        "**From Norvig's response On Chomsky and the Two Cultures of Statistical Learning**:\n",
        "\n",
        "\"Breiman is inviting us to give up on the idea that we can uniquely model the true underlying form of nature's function from inputs to outputs. Instead he asks us to be satisfied with a function that accounts for the observed data well, and generalizes to new, previously unseen data well, but may be expressed in a complex mathematical form that may bear no relation to the \"true\" function's form (if such a true function even exists). Chomsky takes the opposite approach: he prefers to keep a simple, elegant model, and give up on the idea that the model will represent the data well.\"\n",
        "\n",
        "- [Noam Chomsky on Where Artificial Intelligence Went Wrong: An extended conversation with the legendary linguist](https://www.theatlantic.com/technology/archive/2012/11/noam-chomsky-on-where-artificial-intelligence-went-wrong/261637/)\n",
        "- [Norvig vs. Chomsky and the Fight for the Future of AI](https://www.tor.com/2011/06/21/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLzLYtm7mQx3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Yann LeCun vs. Chris Manning\n",
        "\n",
        "Another interesting discussion along the topic of how much linguistic structure to incorporate into NLP models is between Yann LeCun and Chris Manning:\n",
        "\n",
        "[Deep Learning, Structure and Innate Priors: A Discussion between Yann LeCun and Christopher Manning](http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html):\n",
        "\n",
        "On one side, Manning is a prominent advocate for incorporating more linguistic structure into deep learning systems. On the other, LeCun is a leading proponent for the ability of simple but powerful neural architectures to perform sophisticated tasks without extensive task-specific feature engineering. For this reason, anticipation for disagreement between the two was high, with one Twitter commentator describing the event as “the AI equivalent of Batman vs Superman”.\n",
        "\n",
        "...\n",
        "\n",
        "Manning described structure as a “necessary good” (9:14), arguing that we should have a positive attitude towards structure as a good design decision. In particular, structure allows us to design systems that can learn more from less data, and at a higher level of abstraction, compared to those without structure.\n",
        "\n",
        "Conversely, LeCun described structure as a “necessary evil” (2:44), and warned that imposing structure requires us to make certain assumptions, which are invariably wrong for at least some portion of the data, and may become obsolete within the near future. As an example, he hypothesized that ConvNets may be obsolete in 10 years (29:57)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUoEwLWkmkZj",
        "colab_type": "text"
      },
      "source": [
        "#Resources\n",
        "\n",
        "**Books**\n",
        "\n",
        "We won't be using a text book, although here are a few helpful references:\n",
        "\n",
        "- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), by Dan Jurafsky and James H. Martin (free PDF)\n",
        "\n",
        "- [Introduction to Information Retrieval by By Christopher D. Manning](https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html), Prabhakar Raghavan, and Hinrich Schütze (free online)\n",
        "\n",
        "- [Natural Language Processing with PyTorch](https://learning.oreilly.com/library/view/natural-language-processing/9781491978221/) by Brian McMahan and Delip Rao (need to purchase or have O'Reilly Safari account)\n",
        "\n",
        "**Blogs**\n",
        "\n",
        "Good NLP-related blogs:\n",
        "\n",
        "- [Sebastian Ruder](http://ruder.io/)\n",
        "\n",
        "- [Joyce Xu](https://medium.com/@joycex99)\n",
        "\n",
        "- [Jay Alammar](https://jalammar.github.io/)\n",
        "\n",
        "- [Stephen Merity](https://smerity.com/articles/articles.html)\n",
        "\n",
        "- [Rachael Tatman](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQrLrXwcnNwZ",
        "colab_type": "text"
      },
      "source": [
        "## NLP Tools\n",
        "\n",
        "- Regex (example: find all phone numbers with different format like: 123-456-7890, (123) 456-7890, etc.)\n",
        "- Tokenization: splitting your text into meaningful units (has a different meaning in security) which are used for word embeddings\n",
        "- Word embeddings\n",
        "- Linear algebra/matrix decomposition\n",
        "- Neural nets\n",
        "- Hidden Markov Models\n",
        "- Parse trees\n",
        "\n",
        "Example from http://damir.cavar.me/charty/python/ : \"She killed the man with the tie.\"\n",
        "\n",
        "**Was the man wearing the tie?**\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/parse2.png)\n",
        "\n",
        "**Or was the tie the murder weapon?**\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/parse1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y37y9uSpCRN",
        "colab_type": "text"
      },
      "source": [
        "##Python Libraries\n",
        "\n",
        "- [nltk](https://www.nltk.org/): first released in 2001, very broad NLP library\n",
        "- [spaCy](https://spacy.io/): creates parse trees, excellent tokenizer, opinionated\n",
        "- [gensim](https://radimrehurek.com/gensim/): topic modeling and similarity detection\n",
        "\n",
        "specialized tools:\n",
        "\n",
        "- [PyText](https://pytext-pytext.readthedocs-hosted.com/en/latest/)\n",
        "- [fastText](https://fasttext.cc/) has library of embeddings\n",
        "\n",
        "general ML/DL libraries with text features:\n",
        "\n",
        "- sklearn: general purpose Python ML library\n",
        "- fastai: fast & accurate neural nets using modern best practices, on top of PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYWww6EV1S6G",
        "colab_type": "text"
      },
      "source": [
        "##A few NLP applications from fast.ai students\n",
        "\n",
        "Some things you can do with NLP:\n",
        "\n",
        "- [How Quid uses deep learning with small data](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data): Quid has a database of company descriptions, and needed to identify company descriptions that are low quality (too much generic marketing language)\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/quid1.png)\n",
        "\n",
        "- Legal Text Classifier with Universal Language Model Fine-Tuning: A law student in Singapore classified legal documents by category (civil, criminal, contract, family, tort,...)\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/lim-how-kang-1.png)\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/lim-how-kang-2.png)\n",
        "\n",
        "- [Democrats ‘went low’ on Twitter leading up to 2018](https://www.rollcall.com/news/campaigns/lead-midterms-twitter-republicans-went-high-democrats-went-low): Journalism grad students analyzed twitter sentiment of politicians\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/floris-wu.png)\n",
        "\n",
        "- [Introducing Metadata Enhanced ULMFiT](https://www.novetta.com/2019/03/introducing_me_ulmfit/), classifying quotes from articles. Uses metadata (such as publication, country, and source) together with the text of the quote to improve accuracy of the classifier.\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/novetta1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtisiZqq4Uek",
        "colab_type": "text"
      },
      "source": [
        "# Ethics issues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogp5gkUV1jWa",
        "colab_type": "text"
      },
      "source": [
        "## Bias\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/google-translate.png)\n",
        "\n",
        "- [How Vector Space Mathematics Reveals the Hidden Sexism in Language](https://www.technologyreview.com/s/602025/how-vector-space-mathematics-reveals-the-hidden-sexism-in-language/)\n",
        "    - Man Is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\n",
        "    \n",
        "    - Back in 2013, a handful of researchers at Google set loose a neural network on a corpus of three million words taken from Google News texts. The neural net’s goal was to look for patterns in the way words appear next to each other.\n",
        "\n",
        "    - What it found was complex but the Google team discovered it could represent these patterns using vectors in a vector space with some 300 dimensions.\n",
        "    \n",
        "    - It turned out that words with similar meanings occupied similar parts of this vector space. And the relationships between words could be captured by simple vector algebra. For example, “man is to king as woman is to queen” or, using the common notation, “man : king :: woman : queen.” Other relationships quickly emerged too such as  “sister : woman :: brother : man,” and so on. These relationships are known as word embeddings.\n",
        "    \n",
        "    - And they offer plenty of evidence to back up the claim. This comes from querying the vector space to find word embeddings. For example, it is possible to pose the question: “Paris : France :: Tokyo : x” and it will give you the answer x = Japan.\n",
        "\n",
        "    - But ask the database “father : doctor :: mother : x” and it will say x = nurse. And the query “man : computer programmer :: woman : x” gives x = homemaker.\n",
        "\n",
        "- [Semantics derived automatically from language corpora contain human-like biases](https://arxiv.org/abs/1608.07187)\n",
        "- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/abs/1903.03862)\n",
        "- [Word Embeddings, Bias in ML, Why You Don't Like Math, & Why AI Needs You](https://www.youtube.com/watch?v=25nC0n9ERq4&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=9)\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/rigler-tweet.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0X6wow5wPX",
        "colab_type": "text"
      },
      "source": [
        "## Fakery\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/gpt2-howard.png)\n",
        "\n",
        "[OpenAI's New Multitalented AI writes, translates, and slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2)\n",
        "\n",
        "[He Predicted The 2016 Fake News Crisis. Now He's Worried About An Information Apocalypse](https://www.buzzfeednews.com/article/charliewarzel/the-terrifying-future-of-fake-news). (interview with Avi Ovadya)\n",
        "\n",
        "- Generate an audio or video clip of a world leader declaring war. “It doesn’t have to be perfect — just good enough to make the enemy think something happened that it provokes a knee-jerk and reckless response of retaliation.”\n",
        "\n",
        "- A combination of political botnets and astroturfing, where political movements are manipulated by fake grassroots campaigns to effectively compete with real humans for legislator and regulator attention because it will be too difficult to tell the difference.\n",
        "\n",
        "- Imagine if every bit of spam you receive looked identical to emails from real people you knew (with appropriate context, tone, etc).\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/etzioni-fraud.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduqzkgugOF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}