{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translation with RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hduongck/AI-ML-Learning/blob/master/Fastai%20NLP%20course/9_Translation_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPEHTXlkrRxB",
        "colab_type": "text"
      },
      "source": [
        "[Video 12](https://youtu.be/IfsjMg4fLWQ)\n",
        "\n",
        "**Summary of my results**\n",
        " \n",
        "    \n",
        "  |model            | train_loss | valid_loss | seq2seq_acc | bleu|\n",
        "    |-------------------|----------|----------|----------|----------|\n",
        "    |seq2seq            | 3.355085 | 4.272877 | 0.382089 | 0.291899|\n",
        "    |+ teacher forcing | 3.154585 |4.022432 | 0.407792 | 0.310715|\n",
        "    |+ attention       | 1.452292 | 3.420485 | 0.498205 | 0.413232|\n",
        "    |transformer        | 1.913152 | 2.349686 | 0.781749 | 0.612880|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eafgi0h_QQbm",
        "colab_type": "text"
      },
      "source": [
        "**Material:**\n",
        "\n",
        "https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/\n",
        "\n",
        "https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QTsXYHtuRsd",
        "colab_type": "text"
      },
      "source": [
        "# Translation with an RNN\n",
        "\n",
        "This notebook is modified from[ this one](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/translation.ipynb) created by Sylvain Gugger.\n",
        "\n",
        "Today we will be tackling the task of translation. We will be translating from French to English, and to keep our task a manageable size, we will limit ourselves to translating questions.\n",
        "\n",
        "This task is an example of sequence to sequence (seq2seq). Seq2seq can be more challenging than classification, since the output is of variable length (and typically different from the length of the input.\n",
        "\n",
        "French/English parallel texts from http://www.statmt.org/wmt15/translation-task.html . It was created by Chris Callison-Burch, who crawled millions of web pages and then used a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing \"fr\" with \"en\" and about 40 other hand-written rules), and assume that these documents are translations of each other.\n",
        "\n",
        "Translation is much tougher in straight PyTorch: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chHeN_Y4kgZT",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\n",
        "\n",
        "A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items. A trained model would work like this:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPGx8dOckxmv",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "401c418b-1fec-4e34-cb86-3f814a0de31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_1.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_1.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WsDjrollmT9",
        "colab_type": "text"
      },
      "source": [
        "In neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFjEBjYmlviH",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "6bc47741-e9a9-4e10-fba6-2a0f49063f9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U6c1ho5mOeL",
        "colab_type": "text"
      },
      "source": [
        "### Looking under the hood\n",
        "\n",
        "Under the hood, the model is composed of an encoder and a decoder.\n",
        "\n",
        "The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder send the context over to the decoder, which begins producing the output sequence item by item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agkQ8Uq7lq_B",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "9e26347b-5985-48d8-a770-b12e260ac6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_3.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay loop; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_3.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay loop; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTbYZfB4sz2v",
        "colab_type": "text"
      },
      "source": [
        "The same applies in the case of machine translation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65IizT_bmfUt",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "b03d472e-147c-476e-8079-54c2ba337c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_4.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"170\" src=\"https://jalammar.github.io/images/seq2seq_4.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPf6LVTinDi8",
        "colab_type": "text"
      },
      "source": [
        "The **context** is a vector (an array of numbers, basically) in the case of machine translation. The **encoder and decoder** tend to both be recurrent neural networks (Be sure to check out [Luis Serrano’s A friendly introduction to Recurrent Neural Networks for an intro to RNNs](https://www.youtube.com/watch?v=UNmqTiOnRfg)).\n",
        "\n",
        "![alt text](https://jalammar.github.io/images/context.png)\n",
        "\n",
        "    The context is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values.\n",
        "    \n",
        "You can set **the size of the context vector** when you set up your model. It is **basically the number of hidden units** in the encoder RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024.\n",
        "\n",
        "By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “[word embedding](https://machinelearningmastery.com/what-are-word-embeddings/)” algorithms. These turn words into vector spaces that capture a lot of the meaning/semantic information of the words (e.g. [king - man + woman = queen](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)).\n",
        "\n",
        "![alt text](https://jalammar.github.io/images/embedding.png)\n",
        "\n",
        "\n",
        "```\n",
        "We need to turn the input words into vectors before processing them. That transformation is done using a word embedding algorithm. We can use pre-trained embeddings or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, we're showing a vector of size four for simplicity.\n",
        "```\n",
        "\n",
        "Now that we’ve introduced our main vectors/tensors, let’s recap the mechanics of an RNN and establish a visual language to describe these models:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qVEaB6Ko5ZC",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "b293c753-6f0b-4c6e-a6c2-e782d2444bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"350\" src=\"https://jalammar.github.io/images/RNN_1.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"350\" src=\"https://jalammar.github.io/images/RNN_1.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcV9S4_3pscm",
        "colab_type": "text"
      },
      "source": [
        "The next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, we’ll use an animation like this to describe the vectors inside a neural machine translation model.\n",
        "\n",
        "\n",
        "\n",
        "In the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.\n",
        "\n",
        "Let’s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_oG9S8WmhqY",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "a616bee0-d69d-407c-c728-372e1be1a73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"570\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_5.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"570\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_5.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmf7GENHq7Dk",
        "colab_type": "text"
      },
      "source": [
        "The decoder also maintains a hidden states that it passes from one time step to the next. We just didn’t visualize it in this graphic because we’re concerned with the major parts of the model for now.\n",
        "\n",
        "Let’s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an “unrolled” view where instead of showing the one decoder, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNF6nnNmkhg",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "32e166fd-8c50-469f-f676-fe036cc608ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_6.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5imwx3trLFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEU6gIokurSS",
        "colab_type": "text"
      },
      "source": [
        "##Download and preprocess our data\n",
        "\n",
        "We will start by reducing the original dataset to questions. You only need to execute this once, uncomment to run. The dataset can be downloaded [here](https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dASDjg4gujog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = Config().data_path()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVBXE7-Puzhl",
        "colab_type": "code",
        "outputId": "a81e4d64-1cb4-494c-cdab-a6934e572083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "! wget https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz -P {path}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-18 00:37:16--  https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.160.205\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.160.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598183296 (2.4G) [application/x-tar]\n",
            "Saving to: ‘/root/.fastai/data/giga-fren.tgz’\n",
            "\n",
            "giga-fren.tgz       100%[===================>]   2.42G  54.6MB/s    in 51s     \n",
            "\n",
            "2019-08-18 00:38:08 (48.2 MB/s) - ‘/root/.fastai/data/giga-fren.tgz’ saved [2598183296/2598183296]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zcmCyxcu1_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! tar xf {path}/giga-fren.tgz -C {path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_Feez7u3-9",
        "colab_type": "code",
        "outputId": "2fd4246c-aa52-463f-ba67-b42b91afca4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path = Config().data_path()/'giga-fren'\n",
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/giga-fren/giga-fren.release2.fixed.en'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/giga-fren.release2.fixed.fr')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHpZx44Fu-14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with open(path/'giga-fren.release2.fixed.fr') as f: fr = f.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8dwWqF0vSUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with open(path/'giga-fren.release2.fixed.en') as f: en = f.read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-DyDsdjvW-w",
        "colab_type": "text"
      },
      "source": [
        "We will use regex to pick out questions by finding the strings in the English dataset that start with \"Wh\" and end with a question mark. You only need to run these lines once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLmwAHBVvT_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
        "re_fq = re.compile('^([^?.!]+\\?)')\n",
        "en_fname = path/'giga-fren.release2.fixed.en'\n",
        "fr_fname = path/'giga-fren.release2.fixed.fr'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4l0fjMvarp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = ((re_eq.search(eq), re_fq.search(fq)) \n",
        "         for eq, fq in zip(open(en_fname, encoding='utf-8'), open(fr_fname, encoding='utf-8')))\n",
        "qs = [(e.group(), f.group()) for e,f in lines if e and f]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql1s0zlPv51m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qs = [(q1,q2) for q1,q2 in qs]\n",
        "df = pd.DataFrame({'fr': [q[1] for q in qs], 'en': [q[0] for q in qs]}, columns = ['en', 'fr'])\n",
        "df.to_csv(path/'questions_easy.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdLJpGsTybyy",
        "colab_type": "text"
      },
      "source": [
        "## Load our data into a DataBunch\n",
        "\n",
        "Our questions look like this now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugqEv5BAv0e8",
        "colab_type": "code",
        "outputId": "3e767f1a-6771-43cc-ed67-9f489150f657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is light ?</td>\n",
              "      <td>Qu’est-ce que la lumière?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who are we?</td>\n",
              "      <td>Où sommes-nous?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Where did we come from?</td>\n",
              "      <td>D'où venons-nous?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What would we do without it?</td>\n",
              "      <td>Que ferions-nous sans elle ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the absolute location (latitude and lo...</td>\n",
              "      <td>Quelle sont les coordonnées (latitude et longi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 fr\n",
              "0                                    What is light ?                          Qu’est-ce que la lumière?\n",
              "1                                        Who are we?                                    Où sommes-nous?\n",
              "2                            Where did we come from?                                  D'où venons-nous?\n",
              "3                       What would we do without it?                       Que ferions-nous sans elle ?\n",
              "4  What is the absolute location (latitude and lo...  Quelle sont les coordonnées (latitude et longi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHMpfLgfzAtI",
        "colab_type": "text"
      },
      "source": [
        "To make it simple, we lowercase everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CwqoPdyv87Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['en'] = df['en'].apply(lambda x : x.lower())\n",
        "df['fr'] = df['fr'].apply(lambda x : x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebmCNk4ZzDpB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The first thing is that we will need to collate inputs and targets in a batch because they have different lengths so we need to add padding to make the sequence length the same. By adding zeros to fill out the length. \n",
        "\n",
        "Later on we are going to throw away sentences that are too long and again to make this simpler and have kind of an easy batch that we can put on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcrkC8FuTzz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_collate(samples, pad_idx=1, pad_first=True, backwards=False):\n",
        "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
        "    samples = to_data(samples)\n",
        "    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n",
        "    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n",
        "    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n",
        "    if backwards: pad_first = not pad_first\n",
        "    for i,s in enumerate(samples):\n",
        "        if pad_first: \n",
        "            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
        "        else:         \n",
        "            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
        "    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n",
        "    return res_x,res_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFEL_6KkXyH6",
        "colab_type": "text"
      },
      "source": [
        "- doc(Dataset): Dataset class is an abtract class that represent a dataset. Abtract class is kind of defining these things that we want a dataset to have. In this case, we are saying any data specific type of dataset you create, it's going to have a length (`__len__`) as well as `__getitem__` which is basically a way to index into it. This is what gives you the ability to index into something and we'll subclass it which means we are making a class we are inherting from it\n",
        "\n",
        "- doc(DataLoader): the DataLoader is going to give us a way to iterate over a dataset. So we want something that we can call and it will return a batch of the data. In detail, it has batch size , shuffle your batches to change the order. It is usefull with neural network since we're looking at a batch of data at a time, you need a way to get out a batch of data \n",
        "\n",
        "\n",
        "```\n",
        "class DataLoader[test]\n",
        "        DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn='default_collate', pin_memory=True, drop_last=False, timeout=0, worker_init_fn=None)\n",
        "```\n",
        "\n",
        "\n",
        "- doc(DataBunch) : The DataBunch is built on DataLoader. This gives you 2 or 3 DataLoader,  including training dataloader, validation dataloader and optionally test dataloader. This is kind of a way to store your training set, validation set and test set all together in one place. You'll be able to return batches of any of those from this DataBunch. This is usefull object for kind of keeing pieces that you need together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XFLiB5kU0KM",
        "colab_type": "text"
      },
      "source": [
        "Then we create a special DataBunch that uses this collate function. This seq2seqDataBunch  create a type of text databunch that suitable for training a RNN. We do that by adding @classmethod that how to create a seq2seqDataBunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umNM0lYgU4uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqDataBunch(TextDataBunch):\n",
        "    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n",
        "    @classmethod\n",
        "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n",
        "               dl_tfms=None, pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n",
        "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
        "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
        "        val_bs = ifnone(val_bs, bs)\n",
        "        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
        "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
        "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
        "        dataloaders = [train_dl]\n",
        "        for ds in datasets[1:]:\n",
        "            lengths = [len(t) for t in ds.x.items]\n",
        "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
        "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
        "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28LUjX9dZWR",
        "colab_type": "text"
      },
      "source": [
        "**SortishSampler??**:  this is from Fastai lib. Go through the text data by order of length with a bit of randomness\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "class SortishSampler(Sampler):\n",
        "    \"Go through the text data by order of length with a bit of randomness.\"\n",
        "\n",
        "    def __init__(self, data_source:NPArrayList, key:KeyFunc, bs:int):\n",
        "        self.data_source,self.key,self.bs = data_source,key,bs\n",
        "\n",
        "    def __len__(self) -> int: return len(self.data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        idxs = np.random.permutation(len(self.data_source))\n",
        "        sz = self.bs*50\n",
        "        ck_idx = [idxs[i:i+sz] for i in range(0, len(idxs), sz)]\n",
        "        sort_idx = np.concatenate([sorted(s, key=self.key, reverse=True) for s in ck_idx])\n",
        "        sz = self.bs\n",
        "        ck_idx = [sort_idx[i:i+sz] for i in range(0, len(sort_idx), sz)]\n",
        "        max_ck = np.argmax([self.key(ck[0]) for ck in ck_idx])  # find the chunk with the largest key,\n",
        "        ck_idx[0],ck_idx[max_ck] = ck_idx[max_ck],ck_idx[0]     # then make sure it goes first.\n",
        "        sort_idx = np.concatenate(np.random.permutation(ck_idx[1:])) if len(ck_idx) > 1 else np.array([],dtype=np.int)\n",
        "        sort_idx = np.concatenate((ck_idx[0], sort_idx))\n",
        "        return iter(sort_idx)\n",
        "File:           /usr/local/lib/python3.6/dist-packages/fastai/text/data.py\n",
        "Type:           type\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EewJ9Ijwem3n",
        "colab_type": "text"
      },
      "source": [
        "And a subclass of TextList that will use this DataBunch class in the call .databunch and will use TextList to label (since our targets are other texts)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrE-eNuOdHgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqTextList(TextList):\n",
        "    _bunch = Seq2SeqDataBunch\n",
        "    _label_cls = TextList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLhVFOZ0e4oB",
        "colab_type": "text"
      },
      "source": [
        "Thats all we need to use the data block API!. \n",
        "\n",
        "Before we process our French and English questions, put those into dataframe (df). Now we create seq2seq list for a TextList from that "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdz6gItXeqPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = Seq2SeqTextList.from_df(df,path=path,cols='fr').split_by_rand_pct(seed=42).label_from_df(cols='en',label_cls=TextList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBgiuz37fjQK",
        "colab_type": "text"
      },
      "source": [
        "Now we want to look at 90th percentile. We use numpy method, basically we want to see how long are these items and we get 28 is the 90th percentile for x and 24 for y\n",
        "\n",
        "- x is French\n",
        "- y is English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjOWc3ZVfT47",
        "colab_type": "code",
        "outputId": "de52e535-1301-4d9b-e777-b22413261350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.percentile([len(o) for o in src.train.x.items] + [len(o) for o in src.valid.x.items],90)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4_wnQQogt0f",
        "colab_type": "code",
        "outputId": "2f7652d2-3e01-4bf8-8c8d-7066d68c060b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.percentile([len(o) for o in src.train.y.items] + [len(o) for o in src.valid.x.items],90)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UESKA0GRhIDp",
        "colab_type": "text"
      },
      "source": [
        "We remove the items where one of the target is more than 30 tokens long. Since not many items fall in that it's less than 10% and this will kind of make it simpler for what we are working with. And we're not throwing much data by doing this. \n",
        "\n",
        "It's interesting to note that French seem to be longer than English on the whole (28 vs 24)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17GkUKIRhDEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = src.filter_by_func(lambda x,y: len(x) > 30 or len(y) >30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCPwmqAPiDRI",
        "colab_type": "code",
        "outputId": "efcae58c-1b34-41e3-f48e-05da0cca8e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(src.train) + len(src.valid)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48350"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2e4fTWuiMSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = src.databunch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sODK6G5YiSPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTlBtkJqiTRP",
        "colab_type": "code",
        "outputId": "14acdb26-2f65-45ee-af5a-0da0efb466dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqDataBunch;\n",
              "\n",
              "Train: LabelList (38704 items)\n",
              "x: Seq2SeqTextList\n",
              "xxbos qu’est - ce que la lumière ?,xxbos où sommes - nous ?,xxbos d'où venons - nous ?,xxbos que ferions - nous sans elle ?,xxbos quel est le groupe autochtone principal sur l’île de vancouver ?\n",
              "y: TextList\n",
              "xxbos what is light ?,xxbos who are we ?,xxbos where did we come from ?,xxbos what would we do without it ?,xxbos what is the major aboriginal group on vancouver island ?\n",
              "Path: /root/.fastai/data/giga-fren;\n",
              "\n",
              "Valid: LabelList (9646 items)\n",
              "x: Seq2SeqTextList\n",
              "xxbos quels pourraient être les effets sur l’instrument de xxunk et sur l’aide humanitaire qui ne sont pas co - xxunk ?,xxbos quand la source primaire a - t - elle été créée ?,xxbos pourquoi tant de soldats ont - ils fait xxunk de ne pas voir ce qui s'est passé le 4 et le 16 mars ?,xxbos quels sont les taux d'impôt sur le revenu au canada pour 2007 ?,xxbos pourquoi le programme devrait - il intéresser les employeurs et les fournisseurs de services ?\n",
              "y: TextList\n",
              "xxbos what would be the resulting effects on the pre - accession instrument and humanitarian aid that are not co - decided ?,xxbos when was the primary source created ?,xxbos why did so many soldiers look the other way in relation to the incidents of march 4th and march xxunk ?,xxbos what are the income tax rates in canada for 2007 ?,xxbos why is the program good for employers and service providers ?\n",
              "Path: /root/.fastai/data/giga-fren;\n",
              "\n",
              "Test: None"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8PL3bPibfK",
        "colab_type": "code",
        "outputId": "9597406d-bc2a-404e-dbde-54ff0ef716f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.fastai/data/giga-fren')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-ZOdnYjCzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_data(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yGXSTqejImT",
        "colab_type": "code",
        "outputId": "1de007ce-7cc2-4f13-fb48-468769f8f24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "data.show_batch()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos quels services offrez - vous aux résidents , par l'entremise de votre propre organisation , par l'entremise de liens / partenariats avec des organismes communautaires ou les deux ?</td>\n",
              "      <td>xxbos which of the following services do you offer to residents either through your own organization or through linkages and / or partnerships with community agencies or both ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos quelles initiatives ont été prises , conformément aux recommandations précédentes du comité , pour sensibiliser tant les hommes que les femmes à la contraception et aux méthodes xxunk ?</td>\n",
              "      <td>xxbos what steps have been taken , in line with the committee ’s previous recommendations , to raise awareness about contraception and xxunk methods among both men and women ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos qu’en est - il d’un leader qui ne possède que deux de ces caractéristiques ou de celui qui ne manifeste que de temps en temps certains comportements xxunk ?</td>\n",
              "      <td>xxbos what if a leader xxunk only two of the factors ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos quels mécanismes d'évaluation , de surveillance ou de responsabilisation sont en place pour l'étude de cas , tant dans le cadre de l'initiative qu'à l'intérieur de votre organisation ?</td>\n",
              "      <td>xxbos what evaluation , monitoring or other accountability mechanisms are in place for this work , within the initiative and within your organization ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxbos quelles sont les hypothèses , selon la définition actuelle du développement , do nt les pays en développement sont censés s'inspirer pour réformer leurs systèmes de politique sociale ?</td>\n",
              "      <td>xxbos what are the assumptions in the current understanding of development that developing countries are expected to accommodate in reforming their social - policy systems ?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1YwmxTpjnD3",
        "colab_type": "text"
      },
      "source": [
        "## Create our Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4X0NMFjjwCu",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained embeddings\n",
        "\n",
        "You will need to download the word embeddings (crawl vectors) from the fastText docs. FastText has [pre-trained word vectors](https://fasttext.cc/docs/en/crawl-vectors.html) for 157 languages, trained on Common Crawl and Wikipedia. These models were trained using CBOW.\n",
        "\n",
        "If you need a refresher on word embeddings, you can check out my gentle intro in this [word embedding workshop](https://www.youtube.com/watch?v=25nC0n9ERq4&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=10&t=0s) with accompanying [github repo](https://github.com/fastai/word-embeddings-workshop).\n",
        "\n",
        "More reading on CBOW (Continuous Bag of Words vs. Skip-grams):\n",
        "\n",
        "- [fastText tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html#advanced-readers-skipgram-versus-cbow)\n",
        "- [StackOverflow](https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words)\n",
        "\n",
        "\n",
        "To install fastText:\n",
        "\n",
        "\n",
        "```\n",
        "$ git clone https://github.com/facebookresearch/fastText.git\n",
        "$ cd fastText\n",
        "$ pip install .\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNRPDWlLlnnO",
        "colab_type": "code",
        "outputId": "6dd4ce99-3ebb-41b3-c46e-4e11d67a8435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install fastText"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastText\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fastText) (2.3.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fastText) (41.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastText) (1.16.4)\n",
            "Building wheels for collected packages: fastText\n",
            "  Building wheel for fastText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastText: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2382768 sha256=8e8cbbc2bef0e951f95b173f98bb19148073ff8a916c63c33a286776d22be4f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fastText\n",
            "Installing collected packages: fastText\n",
            "Successfully installed fastText-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIz2FiaojJ9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fasttext as ft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUCMiygAlj1x",
        "colab_type": "code",
        "outputId": "cecf4d9a-5c57-4718-8638-59d92918d140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz -P {path}\n",
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz -P {path}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-18 00:43:36--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘/root/.fastai/data/giga-fren/cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  28.1MB/s    in 2m 34s  \n",
            "\n",
            "2019-08-18 00:46:11 (27.9 MB/s) - ‘/root/.fastai/data/giga-fren/cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n",
            "--2019-08-18 00:46:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:6a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4496886212 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘/root/.fastai/data/giga-fren/cc.fr.300.bin.gz’\n",
            "\n",
            "cc.fr.300.bin.gz    100%[===================>]   4.19G  28.0MB/s    in 2m 45s  \n",
            "\n",
            "2019-08-18 00:48:58 (26.0 MB/s) - ‘/root/.fastai/data/giga-fren/cc.fr.300.bin.gz’ saved [4496886212/4496886212]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2r1WHPbp1Ug",
        "colab_type": "code",
        "outputId": "f3a4abbd-4a71-45b2-a009-d59bf098b317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "path.ls()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/giga-fren/cc.en.300.bin.gz'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/giga-fren.release2.fixed.en'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/questions_easy.csv'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/giga-fren.release2.fixed.fr'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/data_save.pkl'),\n",
              " PosixPath('/root/.fastai/data/giga-fren/cc.fr.300.bin.gz')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKvgfwM-l3ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip {path}/cc.en.300.bin.gz\n",
        "!gunzip {path}/cc.fr.300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmsgL42Fn99P",
        "colab_type": "code",
        "outputId": "c65b69fd-0c08-4460-aba7-7704d43f65ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fr_vecs = ft.load_model(str((path/'cc.fr.300.bin')))\n",
        "en_vecs = ft.load_model(str((path/'cc.en.300.bin')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0s6iKsVoSSy",
        "colab_type": "text"
      },
      "source": [
        "We create an embedding module with the pretrained vectors and random data for the missing parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_X8r45yoUTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_emb(vecs,itos,em_sz = 300,mult=1.):\n",
        "    emb = nn.Embedding(len(itos),em_sz,padding_idx=1)\n",
        "    wgts =emb.weight.data\n",
        "    vec_dic = {w:vecs.get_word_vector(w) for w in vecs.get_words()}\n",
        "    miss = []\n",
        "    for i,w in enumerate(itos):\n",
        "        try : wgts[i]:tensor(vec_dic[w])\n",
        "        except : miss.append(w)\n",
        "    return emb\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUwJvLrMr1ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_enc=create_emb(fr_vecs,data.x.vocab.itos)\n",
        "emb_dec=create_emb(en_vecs,data.y.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab8tA9_v3hiE",
        "colab_type": "code",
        "outputId": "2826ef2f-cea1-4441-eaff-b3e58dbfad43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "emb_enc.weight.size(), emb_dec.weight.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([11336, 300]), torch.Size([8144, 300]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWT96yxQ3iO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs('/root/.fastai/models/',exist_ok=True)\n",
        "model_path = '/root/.fastai/models'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-C96Xolub04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(emb_enc, f'{model_path}/fr_emb.pth')\n",
        "torch.save(emb_dec, f'{model_path}/en_emb.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvMaNuu25mnW",
        "colab_type": "code",
        "outputId": "66167fa8-ec11-4f56-85d7-7f073c24650f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#shutil.copy(f'{model_path}/fr_emb.pth','/content')\n",
        "#shutil.copy(f'{model_path}/en_emb.pth','/content')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/en_emb.pth'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1fAYu1EwEOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_enc = torch.load(model_path/'fr_emb.pth')\n",
        "emb_dec = torch.load(model_path/'en_emb.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93w0noZqwLUC",
        "colab_type": "text"
      },
      "source": [
        "### Our Model\n",
        "\n",
        "**Review Question:** What are the two types of numbers in deep learning?\n",
        "\n",
        "**Encoders & Decoders**\n",
        "\n",
        "The model in itself consists in an encoder and a decoder\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/seq2seq.png?raw=True)\n",
        "\n",
        "The encoder is a recurrent neural net and we feed it our input sentence, producing an output (that we discard for now) and a hidden state. **A hidden state** is the activations that come out of an RNN.\n",
        "\n",
        "That hidden state is then given to the decoder (an other RNN) which uses it in conjunction with the outputs it predicts to get produce the translation. We loop until the decoder produces a padding token (or at 30 iterations to make sure it's not an infinite loop at the beginning of training).\n",
        "\n",
        "We will use a GRU for our encoder and a separate GRU for our decoder. Other options are to use LSTMs or QRNNs (see here). GRUs, LSTMs, and QRNNs all solve the problem of how RNNs can lack long-term memory.\n",
        "\n",
        "Links:\n",
        "\n",
        "- Illustrated Guide to LSTM’s and GRU’s: A step by step explanation\n",
        "- fast.ai implementation of seq2seq with QRNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcFKRY9S0TC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, emb_enc, emb_dec, \n",
        "                    nh, out_sl, \n",
        "                    nl=2, bos_idx=0, pad_idx=1):\n",
        "        super().__init__()\n",
        "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
        "        self.bos_idx,self.pad_idx = bos_idx,pad_idx\n",
        "        self.em_sz_enc = emb_enc.embedding_dim\n",
        "        self.em_sz_dec = emb_dec.embedding_dim\n",
        "        self.voc_sz_dec = emb_dec.num_embeddings\n",
        "                 \n",
        "        self.emb_enc = emb_enc\n",
        "        self.emb_enc_drop = nn.Dropout(0.15)\n",
        "        self.gru_enc = nn.GRU(self.em_sz_enc, nh, num_layers=nl,\n",
        "                              dropout=0.25, batch_first=True)\n",
        "        self.out_enc = nn.Linear(nh, self.em_sz_dec, bias=False)\n",
        "        \n",
        "        self.emb_dec = emb_dec\n",
        "        self.gru_dec = nn.GRU(self.em_sz_dec, self.em_sz_dec, num_layers=nl,\n",
        "                              dropout=0.1, batch_first=True)\n",
        "        self.out_drop = nn.Dropout(0.35)\n",
        "        self.out = nn.Linear(self.em_sz_dec, self.voc_sz_dec)\n",
        "        self.out.weight.data = self.emb_dec.weight.data\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.initHidden(bs)\n",
        "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
        "        _, h = self.gru_enc(emb, h)\n",
        "        h = self.out_enc(h)\n",
        "        return h\n",
        "    \n",
        "    def decoder(self, dec_inp, h):\n",
        "        emb = self.emb_dec(dec_inp).unsqueeze(1)\n",
        "        outp, h = self.gru_dec(emb, h)\n",
        "        outp = self.out(self.out_drop(outp[:,0]))\n",
        "        return h, outp\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        bs, sl = inp.size()\n",
        "        h = self.encoder(bs, inp)\n",
        "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
        "        \n",
        "        res = []\n",
        "        for i in range(self.out_sl):\n",
        "            h, outp = self.decoder(dec_inp, h)\n",
        "            dec_inp = outp.max(1)[1]\n",
        "            res.append(outp)\n",
        "            if (dec_inp==self.pad_idx).all(): break\n",
        "        return torch.stack(res, dim=1)\n",
        "    \n",
        "    def initHidden(self, bs): return one_param(self).new_zeros(self.nl, bs, self.nh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr_9oT_Vw1x7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "```\n",
        "def forward(self, inp):\n",
        "        bs, sl = inp.size()\n",
        "        h = self.encoder(bs, inp)\n",
        "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
        "        \n",
        "        res = []\n",
        "        for i in range(self.out_sl):\n",
        "            h, outp = self.decoder(dec_inp, h)\n",
        "            dec_inp = outp.max(1)[1]\n",
        "            res.append(outp)\n",
        "            if (dec_inp==self.pad_idx).all(): break\n",
        "        return torch.stack(res, dim=1)\n",
        "```\n",
        "\n",
        "The forward step is going to put the batch size and input into an encoder and get out a hidden state. Our decode input need to be initialized to right size and to start with beginning of string. \n",
        "\n",
        "And then we'll go through an for loop where in each step that we are putting our decode input (**dec_inp** ) and our hidden state (**h**) we got from an encoder.  We put all these into a decoder _ **self.decoder(dec_inp,h)** , then we get out the hidden state and an output as well.\n",
        "\n",
        "**dec_inp = out.max(1)[1]** : take the max of that to get decoder input \n",
        "\n",
        "**res.append(outp)**: we want to hold on to what our output was from the decoder. \n",
        "\n",
        "**if (dec_inp==self.pad_idx).all(): break** : Then if we have a padding token we'll break out of this \n",
        "\n",
        "**return torch.stack(res,dim=1)** : at the end, we'll return our results \n",
        "\n",
        "=> in summary, you are putting your input into an encoder and then in a for loop you re getting something out of the decoder each time "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_YCcdp30Te0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " def encoder(self, bs, inp):\n",
        "        h = self.initHidden(bs)\n",
        "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
        "        _, h = self.gru_enc(emb, h)\n",
        "        h = self.out_enc(h)\n",
        "        return h\n",
        "```\n",
        "\n",
        "it is taking the embedding that we had prepared for the encoder and we put the input into embedding - **`self.emb_enc(inp)`** - . That basically is taking the French words and picking out what French are the vectors word embedding for those . Then we apply the dropout - **`self.emb_enc_drop(self.emb_enc(inp))`** which do regularization \n",
        "\n",
        "**`_,h = self.gru_enc(emb,h)`** : put that into our first GRU since this uses two different GRUs. This one is learning for the encoder . It including embedding and hidden state of what came before \n",
        "\n",
        "**`h = self.out_enc(h)`** : hidden_state will be put into a Linear layer - self.out_enc -> return a final hidden state \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt59mfv16ryI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "def decoder(self, dec_inp, h):\n",
        "        emb = self.emb_dec(dec_inp).unsqueeze(1)\n",
        "        outp, h = self.gru_dec(emb, h)\n",
        "        outp = self.out(self.out_drop(outp[:,0]))\n",
        "        return h, outp\n",
        "```\n",
        "we get the decoder input (dec_inp) . \n",
        "\n",
        "**`emb = self.emb_dec(dec_inp).unsqueeze(1)`** :Then we put into embedding and add a dimension (unsqueeze(1)) \n",
        "\n",
        "**`outp, h = self.gru_dec(emb, h)`** : we put our emb and h through GRU decoder -> this is our second GRU \n",
        "\n",
        "**`outp = self.out(self.out_drop(outp[:,0]))`**: Then apply some dropout then apply self.out which is a Linear layer ,\n",
        "\n",
        "**`return h, outp`** : finally we get output from there "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFNSfdw68J7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb,yb = next(iter(data.valid_dl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnahJx1NVqgq",
        "colab_type": "code",
        "outputId": "b0ae6335-c83e-4307-c290-1811cfd0dc4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xb.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb2K-tfWVsLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = Seq2SeqRNN(emb_enc,emb_dec,256,30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HKlLc9SWpDN",
        "colab_type": "text"
      },
      "source": [
        "- emb_enc=English embedding\n",
        "- emb_dec = French embedding \n",
        "- 256 : is a number of hidden layers\n",
        "- 30 : is a sequence length "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhMF0tKXWdXi",
        "colab_type": "code",
        "outputId": "04683d59-3b07-4c15-c8f4-b4b8e6337e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "rnn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqRNN(\n",
              "  (emb_enc): Embedding(11336, 300, padding_idx=1)\n",
              "  (emb_enc_drop): Dropout(p=0.15)\n",
              "  (gru_enc): GRU(300, 256, num_layers=2, batch_first=True, dropout=0.25)\n",
              "  (out_enc): Linear(in_features=256, out_features=300, bias=False)\n",
              "  (emb_dec): Embedding(8144, 300, padding_idx=1)\n",
              "  (gru_dec): GRU(300, 300, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (out_drop): Dropout(p=0.35)\n",
              "  (out): Linear(in_features=300, out_features=8144, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD_hh4t9Wd1W",
        "colab_type": "code",
        "outputId": "98a0e029-f337-493e-80ad-3b900ebb1355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(xb[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SJgpdrRXGep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h = rnn.encoder(64,xb.cpu())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PFP7LDWXLr8",
        "colab_type": "code",
        "outputId": "68c89292-b44a-450a-fc73-89b13fb8917e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "h.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prPjOT83XN-y",
        "colab_type": "text"
      },
      "source": [
        "hidden state size is 2x64x300 :\n",
        "\n",
        "- 2 : number of hidden layers inside GRU\n",
        "- 64 : is batch size\n",
        "- 300 : is the dimension of the embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kR1sta-X_sX",
        "colab_type": "text"
      },
      "source": [
        "The loss pads output and target so that they are of the same size before using the usual flattened version of cross entropy. We do the same for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thge3XJ8XMiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_loss(out, targ, pad_idx=1):\n",
        "    bs,targ_len = targ.size()\n",
        "    _,out_len,vs = out.size()\n",
        "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
        "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
        "    return CrossEntropyFlat()(out, targ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnPXnE_3YNO7",
        "colab_type": "text"
      },
      "source": [
        "### Train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM292UEAYLL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data,rnn,loss_func=seq2seq_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3tVDmGrYY1J",
        "colab_type": "code",
        "outputId": "786d6b16-952c-4ded-cf3a-4b6e5509b6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3---Ho9YaMy",
        "colab_type": "code",
        "outputId": "fc6d561e-e1b9-4efb-cc09-38af5d2bda17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKDBERCQrCgwREcnK/wcImNSRjFnJ3AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtmH8QjTYfzE",
        "colab_type": "code",
        "outputId": "bbc55c01-90b1-4a62-86c4-83672ab26249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "learn.fit_one_cycle(4, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.198815</td>\n",
              "      <td>5.412505</td>\n",
              "      <td>01:17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.929246</td>\n",
              "      <td>4.815796</td>\n",
              "      <td>01:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.721554</td>\n",
              "      <td>4.963487</td>\n",
              "      <td>01:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.504862</td>\n",
              "      <td>5.290407</td>\n",
              "      <td>01:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8aXI6peY0OU",
        "colab_type": "text"
      },
      "source": [
        "let's free up some RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCfqEyEJYyoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del fr_vecs\n",
        "del en_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dge52fyDZCWa",
        "colab_type": "text"
      },
      "source": [
        "As loss is not very interpretable, it's nice to see they are both decreasing but in terms of human interpretability , we don't really know how good is our model based on these losses , we dont' have the meaning for that\n",
        "\n",
        "Let's also look at the accuracy. Again, we will add padding so that the output and target are of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C27QVZDRYiOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_acc(out, targ, pad_idx=1):\n",
        "    bs,targ_len = targ.size()\n",
        "    _,out_len,vs = out.size()\n",
        "    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n",
        "    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n",
        "    out = out.argmax(2)\n",
        "    return (out==targ).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj76B0okZ-Th",
        "colab_type": "text"
      },
      "source": [
        "#### BLEU\n",
        "\n",
        "[Video 12 - 27:29](https://youtu.be/IfsjMg4fLWQ?t=1649)\n",
        "\n",
        "The problem, although accuracy is technical definition but measuring how good a translation model is a little bit trickier.\n",
        "\n",
        "One of the metric is deal with this called **BLEU**\n",
        "\n",
        "\n",
        "In translation, the metric usually used is BLEU.\n",
        "\n",
        "A great post by Rachael Tatman: [Evaluating Text Output in NLP: BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pl2jCNPeTxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGram():\n",
        "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
        "    def __eq__(self, other):\n",
        "        if len(self.ngram) != len(other.ngram): return False\n",
        "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
        "    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJVLqMHeiP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_grams(x, n, max_n=5000):\n",
        "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x1WilIWei3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
        "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
        "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
        "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39f_jVXse8EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CorpusBLEU(Callback):\n",
        "    def __init__(self, vocab_sz):\n",
        "        self.vocab_sz = vocab_sz\n",
        "        self.name = 'bleu'\n",
        "    \n",
        "    def on_epoch_begin(self, **kwargs):\n",
        "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
        "    \n",
        "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
        "        last_output = last_output.argmax(dim=-1)\n",
        "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
        "            self.pred_len += len(pred)\n",
        "            self.targ_len += len(targ)\n",
        "            for i in range(4):\n",
        "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
        "                self.corrects[i] += c\n",
        "                self.counts[i]   += t\n",
        "    \n",
        "    def on_epoch_end(self, last_metrics, **kwargs):\n",
        "        precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
        "        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
        "        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n",
        "        return add_metrics(last_metrics, bleu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICBv82EjfEbZ",
        "colab_type": "text"
      },
      "source": [
        "**`bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)`** : we are taking the average of the 4 precisions and these corresponding to unigram, bi-gram,tri-gram, 4-gram. We capture  averaging ** 0.25 and then mutiplying by brevity penalty (len_penalty).\n",
        "\n",
        "To remind why we need brevity penalty : the answer is in case the length is too short , it's left out information and we want to penalize that . That's how BLEU deals with that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdnvLX7JaWxJ",
        "colab_type": "text"
      },
      "source": [
        "#### Training with metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frCsKoApfFO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data,rnn,loss_func=seq2seq_loss,metrics=[seq2seq_acc,CorpusBLEU(len(data.y.vocab.itos))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZXGcJBpfX34",
        "colab_type": "code",
        "outputId": "93a00b59-6bb6-4566-d3d9-5838eb12ab6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdpzUkLNfdkW",
        "colab_type": "code",
        "outputId": "a069fb9d-329e-4ca5-a5bc-7aefc97d0293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "learn.fit_one_cycle(4,1e-2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>seq2seq_acc</th>\n",
              "      <th>bleu</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.482030</td>\n",
              "      <td>5.682566</td>\n",
              "      <td>0.245021</td>\n",
              "      <td>0.219095</td>\n",
              "      <td>01:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.530885</td>\n",
              "      <td>5.019033</td>\n",
              "      <td>0.312583</td>\n",
              "      <td>0.253177</td>\n",
              "      <td>01:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.300954</td>\n",
              "      <td>5.511343</td>\n",
              "      <td>0.253818</td>\n",
              "      <td>0.226397</td>\n",
              "      <td>01:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.150765</td>\n",
              "      <td>4.866162</td>\n",
              "      <td>0.321122</td>\n",
              "      <td>0.250603</td>\n",
              "      <td>01:26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZcIMML3fh-z",
        "colab_type": "code",
        "outputId": "fb690825-159c-4fec-cd7f-be96c6d177e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "learn.fit_one_cycle(4, 1e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>seq2seq_acc</th>\n",
              "      <th>bleu</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.145194</td>\n",
              "      <td>5.134167</td>\n",
              "      <td>0.294418</td>\n",
              "      <td>0.233603</td>\n",
              "      <td>01:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.939709</td>\n",
              "      <td>4.613554</td>\n",
              "      <td>0.347170</td>\n",
              "      <td>0.274131</td>\n",
              "      <td>01:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.781650</td>\n",
              "      <td>4.792542</td>\n",
              "      <td>0.329084</td>\n",
              "      <td>0.261027</td>\n",
              "      <td>01:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.806191</td>\n",
              "      <td>4.636693</td>\n",
              "      <td>0.345565</td>\n",
              "      <td>0.270017</td>\n",
              "      <td>01:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvEjebp-iKWd",
        "colab_type": "text"
      },
      "source": [
        "So how good is our model? Let's see a few predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAugzxnqfk-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(learn, ds_type=DatasetType.Valid):\n",
        "    learn.model.eval()\n",
        "    inputs, targets, outputs = [],[],[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in progress_bar(learn.dl(ds_type)):\n",
        "            out = learn.model(xb)\n",
        "            for x,y,z in zip(xb,yb,out):\n",
        "                inputs.append(learn.data.train_ds.x.reconstruct(x))\n",
        "                targets.append(learn.data.train_ds.y.reconstruct(y))\n",
        "                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))\n",
        "    return inputs, targets, outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGkNIWW-iMx7",
        "colab_type": "code",
        "outputId": "123edbff-802f-4d14-b0a0-63c312f51756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "inputs, targets, outputs = get_predictions(learn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='151' class='' max='151', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [151/151 00:39<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6yY7qqwiTeo",
        "colab_type": "code",
        "outputId": "06563137-0e63-47d2-f8ce-1ba0e2f000e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[700], targets[700], outputs[700]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos qui a le pouvoir de modifier le règlement sur les poids et mesures et le règlement sur l'inspection de l'électricité et du gaz ?,\n",
              " Text xxbos who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations ?,\n",
              " Text xxbos who has the the the the the and and and and and)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S0I56M0iV4x",
        "colab_type": "code",
        "outputId": "f3dd3046-d36d-4819-8b47-0e1f2fafcec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[702], targets[702], outputs[702]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos qui a le pouvoir de modifier la loi sur les poids et mesures et la loi sur l'inspection de l'électricité et du gaz ?,\n",
              " Text xxbos who has the authority to change the electricity and gas inspection act or the weights and measures act ?,\n",
              " Text xxbos who has the the the the the the and and and the the the)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMltiM5DiZSg",
        "colab_type": "code",
        "outputId": "defa9d6c-5e58-484d-babb-a1b31d7f1cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[2513], targets[2513], outputs[2513]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos quelles sont les deux tendances qui ont nuit à la pêche au saumon dans cette province ?,\n",
              " Text xxbos what two trends negatively affected the province ’s salmon fishery ?,\n",
              " Text xxbos what are the two two the the the the the the two two two ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb2kAjA0ibMP",
        "colab_type": "code",
        "outputId": "c1baaf6e-f4b6-4962-c076-f11c28b1da0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[4000], targets[4000], outputs[4000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos où les aires marines nationales de conservation du canada seront - elles situées ?,\n",
              " Text xxbos where will national marine conservation areas of canada be located ?,\n",
              " Text xxbos where is the canada of the the the ? ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPaPGni8jGCX",
        "colab_type": "text"
      },
      "source": [
        "It's usually beginning well, but falls into repeated words at the end of the question.\n",
        "\n",
        "To address the issue of starting but then end up with repeated words. These issues are :\n",
        "\n",
        "- RNN doesn't have a chance to get back on track because the input is the previous words. \" where are the the the the the the ?\" its own incorrect output becomes input for next stage which is the problem \n",
        "\n",
        "The technique to deal with it is **Teaching forcing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31iKzK7NkGVJ",
        "colab_type": "text"
      },
      "source": [
        "### + Teaching forcing : technique to address the issue of previous model [45:41](https://youtu.be/IfsjMg4fLWQ?t=2711)\n",
        "\n",
        "One way to help training is to help the decoder by feeding it the real targets instead of its predictions (if it starts with wrong words, it's very unlikely to give us the right translation). We do that all the time at the beginning, then progressively reduce the amount of teacher forcing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhqeElYYiccG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TeacherForcing(LearnerCallback):\n",
        "    \n",
        "    def __init__(self, learn, end_epoch):\n",
        "        super().__init__(learn)\n",
        "        self.end_epoch = end_epoch\n",
        "    \n",
        "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
        "        if train: return {'last_input': [last_input, last_target]}\n",
        "    \n",
        "    def on_epoch_begin(self, epoch, **kwargs):\n",
        "        self.learn.model.pr_force = 1 - epoch/self.end_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuRqoj-9x7LN",
        "colab_type": "text"
      },
      "source": [
        "The one thing about teaching forcing it's slightly incentivize the model to take more risk because it gets something wrong, it's still gonna be fine the next time since you're going to show it  the answer before it's next prediction. So basically we gradually reduce the  amount of teaching forcing. But at the beginning, we want to do a lot because the model is not very good like we're just starting to train it. Of course, it's gonna get stuff wrong and we don't want that  to just screw it up so that it can't learn.\n",
        "\n",
        "**`self.learn.model.pr_force = 1 - epoch/self.end_epoch`** : pr-force is probability of forcing , that will start high, start at 1 and gradually decrease until the end of the epoch . We want the probability of forcing to be zero. In the real world, we don't have the answer everytime "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcEEkYSpzZOv",
        "colab_type": "text"
      },
      "source": [
        "We will add the following code to our forward method:\n",
        "\n",
        "   \n",
        "```\n",
        " if (targ is not None) and (random.random()<self.pr_force):\n",
        "        if i>=targ.shape[1]: break\n",
        "        dec_inp = targ[:,i]\n",
        "```\n",
        "\n",
        "\n",
        "Additionally, forward will take an additional argument of target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrVReSLakYhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqRNN_tf(nn.Module):\n",
        "    def __init__(self, emb_enc, emb_dec, nh, out_sl, nl=2, bos_idx=0, pad_idx=1):\n",
        "        super().__init__()\n",
        "        self.nl,self.nh,self.out_sl = nl,nh,out_sl\n",
        "        self.bos_idx,self.pad_idx = bos_idx,pad_idx\n",
        "        self.em_sz_enc = emb_enc.embedding_dim\n",
        "        self.em_sz_dec = emb_dec.embedding_dim\n",
        "        self.voc_sz_dec = emb_dec.num_embeddings\n",
        "                 \n",
        "        self.emb_enc = emb_enc\n",
        "        self.emb_enc_drop = nn.Dropout(0.15)\n",
        "        self.gru_enc = nn.GRU(self.em_sz_enc, nh, num_layers=nl,\n",
        "                              dropout=0.25, batch_first=True)\n",
        "        self.out_enc = nn.Linear(nh, self.em_sz_dec, bias=False)\n",
        "        \n",
        "        self.emb_dec = emb_dec\n",
        "        self.gru_dec = nn.GRU(self.em_sz_dec, self.em_sz_dec, num_layers=nl,\n",
        "                              dropout=0.1, batch_first=True)\n",
        "        self.out_drop = nn.Dropout(0.35)\n",
        "        self.out = nn.Linear(self.em_sz_dec, self.voc_sz_dec)\n",
        "        self.out.weight.data = self.emb_dec.weight.data\n",
        "        self.pr_force = 0.\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.initHidden(bs)\n",
        "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
        "        _, h = self.gru_enc(emb, h)\n",
        "        h = self.out_enc(h)\n",
        "        return h\n",
        "    \n",
        "    def decoder(self, dec_inp, h):\n",
        "        emb = self.emb_dec(dec_inp).unsqueeze(1)\n",
        "        outp, h = self.gru_dec(emb, h)\n",
        "        outp = self.out(self.out_drop(outp[:,0]))\n",
        "        return h, outp\n",
        "            \n",
        "    def forward(self, inp, targ=None):\n",
        "        bs, sl = inp.size()\n",
        "        h = self.encoder(bs, inp)\n",
        "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
        "        \n",
        "        res = []\n",
        "        for i in range(self.out_sl):\n",
        "            h, outp = self.decoder(dec_inp, h)\n",
        "            res.append(outp)\n",
        "            dec_inp = outp.max(1)[1]\n",
        "            if (dec_inp==self.pad_idx).all(): break\n",
        "                #---------add teacher forcing----------\n",
        "            if (targ is not None) and (random.random()<self.pr_force):\n",
        "                if i>=targ.shape[1]: continue\n",
        "                dec_inp = targ[:,i]\n",
        "                #-------------------------------------\n",
        "        return torch.stack(res, dim=1)\n",
        "\n",
        "    def initHidden(self, bs): return one_param(self).new_zeros(self.nl, bs, self.nh)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ema4Owgkzqts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_enc = torch.load(f'{model_path}/fr_emb.pth')\n",
        "emb_dec = torch.load(f'{model_path}/en_emb.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcgItB81zrCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_tf = Seq2SeqRNN_tf(emb_enc,emb_dec,256,30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rr59Zd1z8M_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data,rnn_tf, loss_func=seq2seq_loss, metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))], \n",
        "               callback_fns = partial(TeacherForcing, end_epoch=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m0GkL310Qoj",
        "colab_type": "code",
        "outputId": "ae7ac3e6-13b3-455b-d07b-6aa6721aec64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRNrc/wd2SPuq/JLoCwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZO0mmSz0bvY",
        "colab_type": "code",
        "outputId": "d1c6e403-0ba9-4a3b-c8fe-1e301877acf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "learn.fit_one_cycle(6, 3e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>seq2seq_acc</th>\n",
              "      <th>bleu</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.618804</td>\n",
              "      <td>8.096622</td>\n",
              "      <td>0.355594</td>\n",
              "      <td>0.299144</td>\n",
              "      <td>01:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.049555</td>\n",
              "      <td>5.257868</td>\n",
              "      <td>0.366460</td>\n",
              "      <td>0.328175</td>\n",
              "      <td>01:38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.589221</td>\n",
              "      <td>5.363857</td>\n",
              "      <td>0.287679</td>\n",
              "      <td>0.269377</td>\n",
              "      <td>01:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.515140</td>\n",
              "      <td>5.295720</td>\n",
              "      <td>0.283943</td>\n",
              "      <td>0.211130</td>\n",
              "      <td>01:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.176762</td>\n",
              "      <td>5.049106</td>\n",
              "      <td>0.306556</td>\n",
              "      <td>0.248506</td>\n",
              "      <td>01:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.075339</td>\n",
              "      <td>5.044560</td>\n",
              "      <td>0.307807</td>\n",
              "      <td>0.242560</td>\n",
              "      <td>01:25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEaKd2gS0eeU",
        "colab_type": "code",
        "outputId": "7339419c-a89d-426d-caa8-99e71e6138ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "inputs, targets, outputs = get_predictions(learn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='151' class='' max='151', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [151/151 00:38<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g02QCb_W1Cto",
        "colab_type": "code",
        "outputId": "1e8f54f9-4c35-4e90-aa6c-7e6ee7f0a5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "inputs[700],targets[700],outputs[700]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos qui a le pouvoir de modifier le règlement sur les poids et mesures et le règlement sur l'inspection de l'électricité et du gaz ?,\n",
              " Text xxbos who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations ?,\n",
              " Text xxbos who has the the and the and and and and and)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dELfhkn1E5v",
        "colab_type": "code",
        "outputId": "ee5d0a27-9a5e-44b8-dc01-4d1c067c10bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[2513], targets[2513], outputs[2513]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos quelles sont les deux tendances qui ont nuit à la pêche au saumon dans cette province ?,\n",
              " Text xxbos what two trends negatively affected the province ’s salmon fishery ?,\n",
              " Text xxbos what two the two two the the the the)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sghPki6t1GGv",
        "colab_type": "code",
        "outputId": "760d96d8-1c7d-47f9-e0e4-9669cef1f34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[4000], targets[4000], outputs[4000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos où les aires marines nationales de conservation du canada seront - elles situées ?,\n",
              " Text xxbos where will national marine conservation areas of canada be located ?,\n",
              " Text xxbos where will the the of the the be ? ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhJFvviS07Jv",
        "colab_type": "text"
      },
      "source": [
        "We can the see improvement of prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTpOXy3Y3daK",
        "colab_type": "text"
      },
      "source": [
        "### + Attention [50:30](https://youtu.be/IfsjMg4fLWQ?t=3038)\n",
        "\n",
        "Attention is a technique that uses the output of our encoder: instead of discarding it entirely, we use it with our hidden state to pay attention to specific words in the input sentence for the predictions in the output sentence. Specifically, we compute attention weights, then add to the input of the decoder the linear combination of the output of the encoder, with those attention weights.\n",
        "\n",
        "A nice illustration of attention comes from this [blog post](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar (visualization originally from [Tensor2Tensor notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)):\n",
        "\n",
        "From video : instead of only keeping the final hidden state from the encoder, now we want to keep the output at every state of the encoder. We also want to know which to focus on.\n",
        "\n",
        "![alt text](https://github.com/fastai/course-nlp/raw/85e505295efeed88ce61dc0ff5e424bde9741a15/images/alammar-attention.png)\n",
        "\n",
        "We have \" The animal didn't cross the street because it was too tired\". When you are working with \"it\" , it's really important to know \"it\" related to \"animal\", we wouldn't want to think it is the \"street\". As human, when read the sentence, we know what \"it\" refer to \"animal\". So the idea is in your output , you want to know what to focus on from the input . And so we are going to use **weights** or rather we're going to learn weights to learn that information of what we should be focusing on . So basically, we're going to end up taking a average of the output from the encoder at each step to know what were the key steps of the encoder for the particular word I'm on in the output. This will be different for each word of the output what you want to focus on from the input.\n",
        "\n",
        "\n",
        "A second things that might help is to use **a bidirectional model for the encoder**. We set the bidrectional parameter to True for our GRU encoder, and double the number of inputs to the linear output layer of the encoder. Basically, we are doing one version of going forward and one version of going backward to get double information. That's shown up below\n",
        "\n",
        "Also, we now need to set our hidden state:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "hid = hid.view(2,self.n_layers, bs, self.n_hid).permute(1,2,0,3).contiguous()\n",
        "hid = self.out_enc(self.hid_dp(hid).view(self.n_layers, bs, 2*self.n_hid))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzkcd4shrJjw",
        "colab_type": "text"
      },
      "source": [
        "#### Let’s Pay Attention Now\n",
        "\n",
        "The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in [Bahdanau et al., 2014](https://arxiv.org/abs/1409.0473) and [Luong et al., 2015](https://arxiv.org/abs/1508.04025). These papers introduced and refined a technique called “Attention”, which highly improved the quality of machine translation systems. **Attention allows the model to focus on the relevant parts of the input sequence as needed.**\n",
        "\n",
        "![alt text](https://jalammar.github.io/images/attention.png)\n",
        "\n",
        "\n",
        "```\n",
        "At time step 7, the attention mechanism enables the decoder to focus on the word \"étudiant\" (\"student\" in french) before it generates the English translation. This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention.\n",
        "```\n",
        "\n",
        "Let’s continue looking at attention models at this high level of abstraction. An attention model differs from a classic sequence-to-sequence model in two main ways:\n",
        "\n",
        "**First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder:**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VROt7kWpsq11",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "faefbf22-565f-47e3-9c24-20bf7bd6fe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"250\" src=\"https://jalammar.github.io/images/seq2seq_7.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXRg296ktg4C",
        "colab_type": "text"
      },
      "source": [
        "**Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:**\n",
        "\n",
        "1. Look at the set of encoder hidden states it received – each encoder hidden states is most associated with a certain word in the input sentence\n",
        "2. Give each hidden states a score (let’s ignore how the scoring is done for now)\n",
        "3. Multiply each hidden states by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WdFNLsUtIhA",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "0f042747-12ff-4dd2-c34c-d9129dc1e6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"570\" height=\"310\" src=\"https://jalammar.github.io/images/attention_process.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"570\" height=\"310\" src=\"https://jalammar.github.io/images/attention_process.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFbzg_Xju3YA",
        "colab_type": "text"
      },
      "source": [
        "This scoring exercise is done at each time step on the decoder side.\n",
        "\n",
        "Let us now bring the whole thing together in the following visualization and look at how the attention process works:\n",
        "\n",
        "1. The attention decoder RNN takes in the embedding of the `<END>` token, and an initial decoder hidden state.\n",
        "2. The RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded.\n",
        "3. **Attention Step**: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.\n",
        "4. We concatenate h4 and C4 into one vector.\n",
        "5. We pass this vector through a feedforward neural network (one trained jointly with the model).\n",
        "6.The output of the feedforward neural networks indicates the output word of this time step.\n",
        "7. Repeat for the next time steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EddaHPojuM2r",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "17e92fd8-1c97-477e-9a95-4425fd58c135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"630\" height=\"350\" src=\"https://jalammar.github.io/images/attention_tensor_dance.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"630\" height=\"350\" src=\"https://jalammar.github.io/images/attention_tensor_dance.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrqENsCxvzid",
        "colab_type": "text"
      },
      "source": [
        "This is another way to look at which part of the input sentence we’re paying attention to at each decoding step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "outputId": "f51e6287-59f7-4798-a77b-f2df49a84e9a",
        "id": "zSmDunWIv5ey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "#@title \n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"630\" height=\"280\" src=\"https://jalammar.github.io/images/seq2seq_9.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"630\" height=\"280\" src=\"https://jalammar.github.io/images/seq2seq_9.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBo9MaD1wNnl",
        "colab_type": "text"
      },
      "source": [
        "Note that the model isn’t just mindless aligning the first word at the output with the first word from the input. It actually learned from the training phase how to align words in that language pair (French and English in our example). An example for how precise this mechanism can be comes from the attention papers listed above:\n",
        "\n",
        "![alt text](https://jalammar.github.io/images/attention_sentence.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8mjLNJxMha",
        "colab_type": "text"
      },
      "source": [
        "#### Self-Attention at a High Level\n",
        "\n",
        "Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.\n",
        "\n",
        "Say the following sentence is an input sentence we want to translate:\n",
        "\n",
        "”The animal didn't cross the street because it was too tired”\n",
        "\n",
        "What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\n",
        "\n",
        "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\n",
        "\n",
        "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "\n",
        "If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeLhYQr_yTUb",
        "colab_type": "text"
      },
      "source": [
        "#### Self-Attention in Detail\n",
        "Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.\n",
        "\n",
        "The **first step** in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So **for each word, we create a Query vector, a Key vector, and a Value vector**. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
        "\n",
        "Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Multiplying $X_1$ by the $W^Q$ weight matrix produces $q_1$, the \"query\" vector associated with that word. We end up creating a \"query\", a \"key\", and a \"value\" projection of each word in the input sentence.\n",
        "\n",
        "**What are the “query”, “key”, and “value” vectors? **\n",
        "\n",
        "They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\n",
        "\n",
        "The **second step** in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
        "\n",
        "The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of $q_1$ and $k_1$. The second score would be the dot product of $q_1$ and $k_2$.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_self_attention_score.png)\n",
        "\n",
        "The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\n",
        "\n",
        "![](http://jalammar.github.io/images/t/self-attention_softmax.png)\n",
        "\n",
        "    This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\n",
        "\n",
        "\n",
        "\n",
        "The **fifth step** is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
        "\n",
        "The **sixth step** is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
        "\n",
        "![](http://jalammar.github.io/images/t/self-attention-output.png)\n",
        "\n",
        "That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14Y0y6Azcoi",
        "colab_type": "text"
      },
      "source": [
        "##### Matrix Calculation of Self-Attention\n",
        "\n",
        "The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained ($W^Q$, $W^K$, $W^V$).\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n",
        "\n",
        "    Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)\n",
        "    \n",
        "**Finally**, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Vhqu_k6OsV",
        "colab_type": "text"
      },
      "source": [
        "##### More explaination from Jeremy to what is Q, K and V? [Video 17](https://youtu.be/AFkGPmU16QA?t=1024)\n",
        "\n",
        "- Q is going to learnt for the word I am currently translating. I am looking at word 1, Q is going to be for word 1. Then I look at all the other words in the sentence and say how much attention I give to each one when translating word 1.\n",
        "\n",
        "- K is something that I learned for how can caculate for all the other words in the sentence, all of them including word 1\n",
        "\n",
        "-> Q and K are going to get combined together to overall give a sense how relevant is the word represented by K to translating the word represented by Q. When they are combined together, this is forming a scalar.\n",
        "\n",
        "- V is applied to same word that K is applied to. So you figured out how much attention you want to give to the thing for K given that we are looking up with the thing for Q. Then what we return is V. \n",
        "\n",
        "The intuition here that if we did not do K and Q , V is just weight matrix being fed into feed-forward neural net each time. But we don't just want the weight matrix for the current word. We want to have a mix of weight matrices based on how much attention we want to give them.\n",
        "\n",
        "In the end, V is the weight matrix in neural network , Q and K tell how much to mix up all the different weight matrices in the different words \n",
        "\n",
        "![alt text](https://i2.wp.com/mlexplained.com/wp-content/uploads/2017/12/scaled_dot_product_attention.png?w=840)\n",
        "\n",
        "![alt text](https://s0.wp.com/latex.php?latex=%5Ctextrm%7BAttention%7D%28Q%2C+K%2C+V%29+%3D+%5Ctextrm%7Bsoftmax%7D%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V+&bg=ffffff&fg=000&s=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZN9OcQyojvM",
        "colab_type": "text"
      },
      "source": [
        "#### Implement attention ( added only to decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siTSX7x5UkvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2SeqRNN_attn(nn.Module):\n",
        "    def __init__(self, emb_enc, emb_dec, nh, out_sl, nl=2, bos_idx=0, pad_idx=1):\n",
        "        super().__init__()\n",
        "        self.nl,self.nh,self.out_sl,self.pr_force = nl,nh,out_sl,1\n",
        "        self.bos_idx,self.pad_idx = bos_idx,pad_idx\n",
        "        self.emb_enc,self.emb_dec = emb_enc,emb_dec\n",
        "        self.emb_sz_enc,self.emb_sz_dec = emb_enc.embedding_dim,emb_dec.embedding_dim\n",
        "        self.voc_sz_dec = emb_dec.num_embeddings\n",
        "                 \n",
        "        self.emb_enc_drop = nn.Dropout(0.15)\n",
        "        self.gru_enc = nn.GRU(self.emb_sz_enc, nh, num_layers=nl, dropout=0.25, \n",
        "                              batch_first=True, bidirectional=True)\n",
        "        self.out_enc = nn.Linear(2*nh, self.emb_sz_dec, bias=False)\n",
        "        \n",
        "        self.gru_dec = nn.GRU(self.emb_sz_dec + 2*nh, self.emb_sz_dec, num_layers=nl,\n",
        "                              dropout=0.1, batch_first=True)\n",
        "        self.out_drop = nn.Dropout(0.35)\n",
        "        self.out = nn.Linear(self.emb_sz_dec, self.voc_sz_dec)\n",
        "        self.out.weight.data = self.emb_dec.weight.data\n",
        "        \n",
        "        self.enc_att = nn.Linear(2*nh, self.emb_sz_dec, bias=False)\n",
        "        self.hid_att = nn.Linear(self.emb_sz_dec, self.emb_sz_dec)\n",
        "        self.V =  self.init_param(self.emb_sz_dec)\n",
        "        \n",
        "    def encoder(self, bs, inp):\n",
        "        h = self.initHidden(bs)\n",
        "        emb = self.emb_enc_drop(self.emb_enc(inp))\n",
        "        # we have two times of hidden state since we have bi-directional component\n",
        "        enc_out, hid = self.gru_enc(emb, 2*h) \n",
        "        # this change below only is related to be bi-directional, not attention\n",
        "        pre_hid = hid.view(2, self.nl, bs, self.nh).permute(1,2,0,3).contiguous()\n",
        "        pre_hid = pre_hid.view(self.nl, bs, 2*self.nh)\n",
        "        hid = self.out_enc(pre_hid)\n",
        "        return hid,enc_out\n",
        "    \n",
        "    # we will deal with attention in the decoder\n",
        "    def decoder(self, dec_inp, hid, enc_att, enc_out):\n",
        "        hid_att = self.hid_att(hid[-1])\n",
        "        \n",
        "        # we have put enc_out and hid through linear layers.\n",
        "        # Here we want to combine the attention on the encoder and attention on\n",
        "        # the hidden layer\n",
        "        u = torch.tanh(enc_att + hid_att[:,None])\n",
        "        \n",
        "        # we want to learn the importance of each time step\n",
        "        attn_wgts = F.softmax(u @ self.V, 1)\n",
        "        \n",
        "        # ctx stands for context. We are multiplying the attention weights\n",
        "        # by the output from the encoder to get the context. That's the \n",
        "        # weighted average of enc_out (which is the output at every time step)\n",
        "        # Back to example \" the animal don't cross the street because it was too\n",
        "        # too tired\". We want to know \"it\" refered to \"animal\", we would put more\n",
        "        # hopefully the network would learn to put more weight on those steps.\n",
        "        ctx = (attn_wgts[...,None] * enc_out).sum(1)\n",
        "        emb = self.emb_dec(dec_inp)\n",
        "        # concatenate decoder embedding with context (we could have just\n",
        "        # used the hidden state that came out of the decoder, if we weren't\n",
        "        # using attention)\n",
        "        outp, hid = self.gru_dec(torch.cat([emb, ctx], 1)[:,None], hid)\n",
        "        outp = self.out(self.out_drop(outp[:,0]))\n",
        "        return hid, outp\n",
        "        \n",
        "    def show(self, nm,v):\n",
        "        if False: print(f\"{nm}={v[nm].shape}\")\n",
        "        \n",
        "    def forward(self, inp, targ=None):\n",
        "        bs, sl = inp.size()\n",
        "        hid,enc_out = self.encoder(bs, inp)\n",
        "       # self.show(\"hid\",vars())\n",
        "\n",
        "        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n",
        "        \n",
        "        # Before we do have to intialize this encoder attention which is just\n",
        "        # the linear layer\n",
        "        enc_att = self.enc_att(enc_out)\n",
        "        \n",
        "        res = []\n",
        "        for i in range(self.out_sl):\n",
        "            # Inside the for loop we are still calling decoder the same way we\n",
        "            # did before\n",
        "            hid, outp = self.decoder(dec_inp, hid, enc_att, enc_out)\n",
        "            # tracking the output\n",
        "            res.append(outp)\n",
        "            dec_inp = outp.max(1)[1]\n",
        "            if (dec_inp==self.pad_idx).all(): break\n",
        "            # we are still using teacherforcing\n",
        "           \n",
        "            if (targ is not None) and (random.random()<self.pr_force):\n",
        "                if i>=targ.shape[1]: continue\n",
        "                dec_inp = targ[:,i]\n",
        "        return torch.stack(res, dim=1)\n",
        "\n",
        "    def initHidden(self, bs): return one_param(self).new_zeros(2*self.nl, bs, self.nh)\n",
        "    def init_param(self, *sz): return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tZfrv5-t9OB",
        "colab_type": "text"
      },
      "source": [
        "We are stil incrementally improving the model by adding more (teacherforcing and attention) to seq2seq model\n",
        "\n",
        "hid=torch.Size([2, 64, 300])\n",
        "\n",
        "dec_inp=torch.Size([64])\n",
        "\n",
        "enc_att=torch.Size([64, 30, 300]): 64 is the batch size, 30 is the length of our sequence. This is key dimension that you  want to know which of 30 is the most important as you go through it. 300 is the size of embeddings.\n",
        "\n",
        "hid_att=torch.Size([64, 300])\n",
        "\n",
        "u=torch.Size([64, 30, 300])\n",
        "\n",
        "attn_wgts=torch.Size([64, 30]) : we are taking the weights at difference of 30 time steps\n",
        "\n",
        "enc_out=torch.Size([64, 30, 512])\n",
        "\n",
        "ctx=torch.Size([64, 512])\n",
        "\n",
        "emb=torch.Size([64, 300])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xivtsrHMt8mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2SeqRNN_attn(emb_enc,emb_dec,256,30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSeNtrkMuyjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data,model,loss_func=seq2seq_loss,metrics=[seq2seq_acc, CorpusBLEU(len(data.y.vocab.itos))],\n",
        "                callback_fns=partial(TeacherForcing, end_epoch=30))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5hrXVkOvHUv",
        "colab_type": "code",
        "outputId": "17ca1a03-aac3-4b6f-8412-d43fd964f3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "learn.lr_find()\n",
        "learn.recorder.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDDMzq8n/B5QNAfcPpCcvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT-HtaTHvKfp",
        "colab_type": "code",
        "outputId": "621df70d-f500-4152-a79f-e1405e681aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "learn.fit_one_cycle(15,3e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>seq2seq_acc</th>\n",
              "      <th>bleu</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.211560</td>\n",
              "      <td>3.973259</td>\n",
              "      <td>0.533767</td>\n",
              "      <td>0.296959</td>\n",
              "      <td>02:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.288683</td>\n",
              "      <td>4.115649</td>\n",
              "      <td>0.507748</td>\n",
              "      <td>0.372423</td>\n",
              "      <td>02:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.237403</td>\n",
              "      <td>4.340171</td>\n",
              "      <td>0.480048</td>\n",
              "      <td>0.389193</td>\n",
              "      <td>02:19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.180545</td>\n",
              "      <td>4.544856</td>\n",
              "      <td>0.465811</td>\n",
              "      <td>0.385388</td>\n",
              "      <td>02:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.061826</td>\n",
              "      <td>3.297514</td>\n",
              "      <td>0.538278</td>\n",
              "      <td>0.356199</td>\n",
              "      <td>02:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.956458</td>\n",
              "      <td>3.642374</td>\n",
              "      <td>0.505945</td>\n",
              "      <td>0.395031</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.927408</td>\n",
              "      <td>4.178248</td>\n",
              "      <td>0.449730</td>\n",
              "      <td>0.391585</td>\n",
              "      <td>02:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.889459</td>\n",
              "      <td>3.743475</td>\n",
              "      <td>0.478988</td>\n",
              "      <td>0.407258</td>\n",
              "      <td>02:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.755543</td>\n",
              "      <td>4.114755</td>\n",
              "      <td>0.433610</td>\n",
              "      <td>0.385047</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.831728</td>\n",
              "      <td>3.985462</td>\n",
              "      <td>0.449586</td>\n",
              "      <td>0.392925</td>\n",
              "      <td>02:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.744323</td>\n",
              "      <td>4.069306</td>\n",
              "      <td>0.443683</td>\n",
              "      <td>0.395139</td>\n",
              "      <td>02:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.611336</td>\n",
              "      <td>4.037031</td>\n",
              "      <td>0.440051</td>\n",
              "      <td>0.394864</td>\n",
              "      <td>02:15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.708314</td>\n",
              "      <td>3.996373</td>\n",
              "      <td>0.440225</td>\n",
              "      <td>0.389908</td>\n",
              "      <td>02:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.618841</td>\n",
              "      <td>4.070520</td>\n",
              "      <td>0.432829</td>\n",
              "      <td>0.386996</td>\n",
              "      <td>02:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.738750</td>\n",
              "      <td>4.082696</td>\n",
              "      <td>0.429424</td>\n",
              "      <td>0.383554</td>\n",
              "      <td>02:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8VeUfpxvXSu",
        "colab_type": "code",
        "outputId": "ae6f882c-8fd5-4edb-d0f1-8147b897dfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "inputs, targets, outputs = get_predictions(learn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='151' class='' max='151', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [151/151 00:41<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9486kmr303KG",
        "colab_type": "code",
        "outputId": "89f7cf9b-0f38-4871-8e29-3bc5969e150d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "inputs[700], targets[700], outputs[700]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos qui a le pouvoir de modifier le règlement sur les poids et mesures et le règlement sur l'inspection de l'électricité et du gaz ?,\n",
              " Text xxbos who has the authority to change the electricity and gas inspection regulations and the weights and measures regulations ?,\n",
              " Text xxbos who has the changes to the regulations and regulations and regulations ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe06n_b507iz",
        "colab_type": "code",
        "outputId": "10b01221-7f77-4cba-a3eb-b0163a969811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "inputs[701], targets[701], outputs[701]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos ´ ` ou sont xxunk leurs grandes convictions en ce qui a trait a la ` ` ´ transparence et a la responsabilite ?,\n",
              " Text xxbos what happened to their great xxunk about transparency and accountability ?,\n",
              " Text xxbos what are the main reasons for the and and the ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kPsKGjP09Qo",
        "colab_type": "code",
        "outputId": "2a0cddf6-65df-41a5-baca-7635a0278b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "inputs[4002], targets[4002], outputs[4002]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Text xxbos quelles ressources votre communauté possède - t - elle qui favoriseraient la guérison ?,\n",
              " Text xxbos what resources exist in your community that would promote recovery ?,\n",
              " Text xxbos what resources does your community have to the ?)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAss55PqgsFS",
        "colab_type": "text"
      },
      "source": [
        "The prediction now improved better and better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXuVnSqFij7N",
        "colab_type": "text"
      },
      "source": [
        "# Translation with Transformer : different architecture with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMH2EStatmX-",
        "colab_type": "text"
      },
      "source": [
        "## Mutlti-head attention : The Beasts with many heads\n",
        "\n",
        "The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:\n",
        "\n",
        "1. It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n",
        "\n",
        "2. It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)\n",
        "\n",
        "    With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.\n",
        "\n",
        "If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_attention_heads_z.png)\n",
        "\n",
        "This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.\n",
        "\n",
        "How do we do that? We concat the matrices then multiple them by an additional weights matrix $W^O$.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)\n",
        "\n",
        "That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
        "\n",
        "Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)\n",
        "\n",
        "    As we encode the word \"it\", one attention head is focusing most on \"the animal\", while another is focusing on \"tired\" -- in a sense, the model's representation of the word \"it\" bakes in some of the representation of both \"animal\" and \"tired\".\n",
        "    \n",
        "If we add all the attention heads to the picture, however, things can be harder to interpret:\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyF88LpXxxzf",
        "colab_type": "text"
      },
      "source": [
        "## Representing The Order of The Sequence Using Positional Encoding\n",
        "\n",
        "One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.\n",
        "\n",
        "To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png)\n",
        "\n",
        "    To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.\n",
        "    \n",
        "If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_positional_encoding_example.png)\n",
        "\n",
        "What might this pattern look like?\n",
        "\n",
        "In the following figure, each row corresponds the a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png)\n",
        "\n",
        "\n",
        "    A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\n",
        "    \n",
        "The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyuFfwyYHswc",
        "colab_type": "text"
      },
      "source": [
        "## The Residuals\n",
        "\n",
        "One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)\n",
        "\n",
        "If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n",
        "\n",
        "This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP7_4lsjIyv4",
        "colab_type": "text"
      },
      "source": [
        "## The Decoder Side\n",
        "Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\n",
        "\n",
        "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
        "\n",
        "    After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).\n",
        "    \n",
        "\n",
        "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
        "\n",
        "**The self attention layers in the decoder operate in a slightly different way than the one in the encoder:**\n",
        "\n",
        "- In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
        "\n",
        "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxjJuTkRJ9QY",
        "colab_type": "text"
      },
      "source": [
        "##The Final Linear and Softmax Layer\n",
        "\n",
        "The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
        "\n",
        "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n",
        "\n",
        "Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n",
        "\n",
        "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)\n",
        "\n",
        "    This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqgHfzx7LHw4",
        "colab_type": "text"
      },
      "source": [
        "## Recap Of Training\n",
        "Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\n",
        "\n",
        "During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\n",
        "\n",
        "To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and `“<eos>”` (short for ‘end of sentence’)).\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/vocabulary.png)\n",
        "\n",
        "    The output vocabulary of our model is created in the preprocessing phase before we even begin training.\n",
        "    \n",
        "Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/one-hot-vocabulary-example.png)\n",
        "\n",
        "Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQj1oPz0LwLY",
        "colab_type": "text"
      },
      "source": [
        "## The Loss Function\n",
        "\n",
        "Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\n",
        "\n",
        "What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_logits_output_and_label.png)\n",
        "\n",
        "    Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\n",
        "    \n",
        "How do you compare two probability distributions? We simply subtract one from the other. For more details, look at [cross-entropy](https://colah.github.io/posts/2015-09-Visual-Information/) and [Kullback–Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained).\n",
        "\n",
        "But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\n",
        "\n",
        "- Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 3,000 or 10,000)\n",
        "- The first probability distribution has the highest probability at the cell associated with the word “i”\n",
        "- The second probability distribution has the highest probability at the cell associated with the word “am”\n",
        "- And so on, until the fifth output distribution indicates `‘<end of sentence>’` symbol, which also has a cell associated with it from the 10,000 element vocabulary.\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/output_target_probability_distributions.png)\n",
        "\n",
        "    The targeted probability distributions we'll train our model against in the training example for one sample sentence.\n",
        "\n",
        "After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\n",
        "\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/output_trained_model_probability_distributions.png)\n",
        "\n",
        "    Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.\n",
        "\n",
        "Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘me’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (because we compared the results after calculating the beams for positions #1 and #2), and top_beams is also two (since we kept two words). These are both hyperparameters that you can experiment with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd7ZDMZ-0_Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}